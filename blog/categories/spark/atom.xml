<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: spark | 研究研究]]></title>
  <link href="http://www.yanjiuyanjiu.com/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://www.yanjiuyanjiu.com/"/>
  <updated>2013-11-11T10:29:01+08:00</updated>
  <id>http://www.yanjiuyanjiu.com/</id>
  <author>
    <name><![CDATA[soulmachine]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[使用docker打造spark集群]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20131027"/>
    <updated>2013-10-27T20:30:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/build-spark-cluster-with-docker</id>
    <content type="html"><![CDATA[<p><strong>前提条件：</strong>安装好了docker，见我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20131025">Docker安装</a></p>

<p>有两种方式，</p>

<ul>
  <li><a href="https://github.com/apache/incubator-spark">Spark官方repo</a>里，docker文件夹下的脚本。官方的这个脚本封装很薄，尽可能把必要的信息展示出来。</li>
  <li><a href="https://github.com/amplab/docker-scripts">AMPLab开源的这个独立小项目</a>，来打造一个spark集群。这个脚本封装很深，自带了一个DNS服务器，还有hadoop，非常自动化，缺点是很多信息看不到了。</li>
</ul>

<h1 id="section">1. 第1种方式</h1>

<h2 id="git-clone-">git clone 源码</h2>
<p>首先要把官方repo的代码下载下来</p>

<pre><code>git clone git@github.com:apache/incubator-spark.git
</code></pre>

<h2 id="apt">（可选）修改apt源</h2>
<p>在国内，将apt源修改国内源，例如163的源，速度会快很多。将<code>base/Dockerfile</code>里的</p>

<pre><code>RUN echo "deb http://archive.ubuntu.com/ubuntu precise main universe" &gt; /etc/apt/sources.list
</code></pre>

<p>替换为</p>

<pre><code>RUN echo "deb http://mirrors.163.com/ubuntu/ precise main restricted universe multiverse" &gt; /etc/apt/sources.list
RUN echo "deb http://mirrors.163.com/ubuntu/ precise-security main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb http://mirrors.163.com/ubuntu/ precise-updates main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb http://mirrors.163.com/ubuntu/ precise-proposed main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb http://mirrors.163.com/ubuntu/ precise-backports main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise-security main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise-updates main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise-proposed main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise-backports main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
</code></pre>

<h2 id="build">build镜像</h2>
<p>将<code>build</code>和<code>spark-test/build</code>里的<code>docker</code>命令前，添加<code>sudo</code>，然后执行<code>docker</code>下的<code>build</code></p>

<pre><code>cd docker
./build
</code></pre>

<h2 id="master">启动master</h2>

<pre><code>sudo docker run -v $SPARK_HOME:/opt/spark spark-test-master
</code></pre>

<h2 id="worker">启动worker</h2>
<p>新开一个终端窗口（强烈推荐tmux），启动一个worker</p>

<pre><code>sudo docker run -v $SPARK_HOME:/opt/spark spark-test-worker &lt;master_ip&gt;
</code></pre>

<p>可以在master终端窗口看到worker注册上来了。</p>

<p>可以再开多个终端窗口，启动多个worker。</p>

<h1 id="section-1">2. 第2种方式</h1>

<h2 id="wget">升级wget</h2>
<p>如果发现wget不识别<code>--no-proxy</code>选项，需要升级wget。</p>

<h2 id="section-2">下载镜像</h2>
<p>为了让脚本第一次执行的时候更快，还是手动下载所有的镜像吧，amplab在index.docker.io上有一个官方账号，把这个账号有关spark的repo都pull下来。</p>

<pre><code>sudo docker pull amplab/apache-hadoop-hdfs-precise
sudo docker pull amplab/dnsmasq-precise
sudo docker pull amplab/spark-worker
sudo docker pull amplab/spark-master
sudo docker pull amplab/spark-shell
</code></pre>

<h2 id="git-clone--1">git clone 脚本</h2>

<pre><code>git@github.com:amplab/docker-scripts.git
</code></pre>

<p>这个脚本可以一键启动集群，爽啊哈哈哈！</p>

<h2 id="spark">一键启动spark集群</h2>

<pre><code>sudo ./deploy/deploy.sh -i amplab/spark:0.8.0 -w 3 
</code></pre>

<h2 id="spark-shell">启动 Spark shell</h2>
<p>启动一个交互式shell吧，IP为上一步输出的Master的IP</p>

<pre><code>sudo docker run -i -t -dns 172.17.0.90 amplab/spark-shell:0.8.0
</code></pre>

<h2 id="section-3">运行一个简单的的例子</h2>

<pre><code>scala&gt; val textFile = sc.textFile("hdfs://master:9000/user/hdfs/test.txt")
scala&gt; textFile.count()
scala&gt; textFile.map({line =&gt; line}).collect()
</code></pre>

<h2 id="section-4">关闭集群</h2>

<pre><code>$ sudo ./deploy/kill_all.sh spark
$ sudo ./deploy/kill_all.sh nameserver
</code></pre>

<h2 id="section-5">更多详情请参考项目主页的文档</h2>

<p><a href="https://github.com/amplab/docker-scripts">https://github.com/amplab/docker-scripts</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安装Spark 0.8 集群(在CentOS上)]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20131017"/>
    <updated>2013-10-17T11:58:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/installing-spark-on-centos-cn</id>
    <content type="html"><![CDATA[<p><strong>环境</strong>:CentOS 6.4, Hadoop 1.1.2, JDK 1.7, Spark 0.8.0, Scala 2.9.3</p>

<p>Spark 0.7.2 的安装请看之前的一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20130617/">安装Spark集群(在CentOS上)</a> 。</p>

<p>Spark的安装很简单，总结起来一句话：下载，解压，然后拷贝到所有机器，完毕，无需任何配置。</p>

<h1 id="jdk-17">1. 安装 JDK 1.7</h1>
<pre><code>yum search openjdk-devel
sudo yum install java-1.7.0-openjdk-devel.x86_64
/usr/sbin/alternatives --config java
/usr/sbin/alternatives --config javac
sudo vim /etc/profile
# add the following lines at the end
export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.19.x86_64
export JRE_HOME=$JAVA_HOME/jre
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
# test
$ java -version
</code></pre>

<p>参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120423/">安装和配置CentOS服务器的详细步骤</a>。</p>

<h1 id="scala-293">2. 安装 Scala 2.9.3</h1>
<p>Spark 0.8.0 依赖 Scala 2.9.3, 我们必须要安装Scala 2.9.3.</p>

<p>下载 <a href="http://www.scala-lang.org/downloads/distrib/files/scala-2.9.3.tgz">scala-2.9.3.tgz</a> 并 保存到home目录.</p>

<pre><code>$ tar -zxf scala-2.9.3.tgz
$ sudo mv scala-2.9.3 /usr/lib
$ sudo vim /etc/profile
# add the following lines at the end
export SCALA_HOME=/usr/lib/scala-2.9.3
export PATH=$PATH:$SCALA_HOME/bin
# save and exit vim
#make the bash profile take effect immediately
source /etc/profile
# test
$ scala -version
</code></pre>

<h1 id="spark">3. 下载预编译好的Spark</h1>
<p>下载预编译好的Spark, <a href="http://spark-project.org/download/spark-0.8.0-incubating-bin-hadoop1.tgz">spark-0.8.0-incubating-bin-hadoop1.tgz</a>. </p>

<p>如果你想从零开始编译，则下载源码包，但是我不建议你这么做，因为有一个Maven仓库，twitter4j.org, 被墙了，导致编译时需要翻墙，非常麻烦。如果你有DIY精神，并能顺利翻墙，则可以试试这种方式。</p>

<h1 id="local">4. Local模式</h1>

<h2 id="section">4.1 解压</h2>

<pre><code>$ tar -zxf spark-0.8.0-incubating-bin-hadoop1.tgz
</code></pre>

<h2 id="sparkhome">4.2 （可选）设置 SPARK_HOME环境变量</h2>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.8.0
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="sparkpi">4.3 现在可以运行SparkPi了</h2>

<pre><code>$ cd $SPARK_HOME
$ ./run-example org.apache.spark.examples.SparkPi local
</code></pre>

<h1 id="cluster">5. Cluster模式</h1>

<!-- more -->

<h2 id="hadoop">5.1 安装Hadoop</h2>
<p>用VMware Workstation 创建三台CentOS 虚拟机，hostname分别设置为 master, slave01, slave02，设置SSH无密码登陆，安装hadoop，然后启动hadoop集群。参考我的这篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20130612">在CentOS上安装Hadoop</a>. </p>

<h2 id="scala">5.2 Scala</h2>
<p>在三台机器上都要安装 Scala 2.9.3 , 按照第2节的步骤。JDK在安装Hadoop时已经安装了。</p>

<h2 id="masterspark">5.3 在master上安装并配置Spark</h2>
<p>解压</p>

<pre><code>$ tar -zxf spark-0.8.0-incubating-bin-hadoop1.tgz.tgz
</code></pre>

<p>在 in <code>conf/spark-env.sh</code> 中设置<code>SCALA_HOME</code></p>

<pre><code>$ cd ~/spark-0.8.0/conf
$ mv spark-env.sh.template spark-env.sh
$ vim spark-env.sh
# add the following line
export SCALA_HOME=/usr/lib/scala-2.9.3
# save and exit
</code></pre>

<p>在<code>conf/slaves</code>, 添加Spark worker的hostname, 一行一个。</p>

<pre><code>$ vim slaves
slave01
slave02
# save and exit
</code></pre>

<p>（可选）设置 SPARK_HOME环境变量，并将SPARK_HOME/bin加入PATH</p>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.8.0
export PATH=$PATH:$SPARK_HOME/bin
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="workerspark">5.4 在所有worker上安装并配置Spark</h2>
<p>既然master上的这个文件件已经配置好了，把它拷贝到所有的worker即可。<strong>注意，三台机器spark所在目录必须一致，因为master会登陆到worker上执行命令，master认为worker的spark路径与自己一样。</strong></p>

<pre><code>$ cd
$ scp -r spark-0.8.0 dev@slave01:~
$ scp -r spark-0.8.0 dev@slave02:~
</code></pre>

<h2 id="spark-">5.5 启动 Spark 集群</h2>
<p>在master上执行</p>

<pre><code>$ cd ~/spark-0.8.0
$ bin/start-all.sh
</code></pre>

<p>检测进程是否启动</p>

<pre><code>$ jps
11055 Jps
2313 SecondaryNameNode
2409 JobTracker
2152 NameNode
4822 Master
</code></pre>

<p>浏览master的web UI(默认<a href="http://localhost:8080">http://localhost:8080</a>). 这是你应该可以看到所有的word节点，以及他们的CPU个数和内存等信息。</p>

<h2 id="spark-1">5.6 运行Spark自带的例子</h2>

<p>运行SparkPi</p>

<pre><code>$ cd ~/spark-0.8.0
$ ./run-example org.apache.spark.examples.SparkPi spark://master:7077
</code></pre>

<p>运行 SparkLR</p>

<pre><code>#Logistic Regression
#./run-example org.apache.spark.examples.SparkLR spark://master:7077
</code></pre>

<p>运行 SparkKMeans</p>

<pre><code>#kmeans
$ ./run-example org.apache.spark.examples.SparkKMeans spark://master:7077 ./kmeans_data.txt 2 1
</code></pre>

<h2 id="hdfswordcount">5.7 从HDFS读取文件并运行WordCount</h2>

<pre><code>$ cd ~/spark-0.8.0
$ hadoop fs -put README.md .
$ MASTER=spark://master:7077 ./spark-shell
scala&gt; val file = sc.textFile("hdfs://master:9000/user/dev/README.md")
scala&gt; val count = file.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey(_+_)
scala&gt; count.collect()
</code></pre>

<h2 id="spark--1">5.8 停止 Spark 集群</h2>

<pre><code>$ cd ~/spark-0.8.0
$ bin/stop-all.sh
</code></pre>

<h1 id="section-1">参考资料</h1>
<ol>
  <li><a href="http://spark-project.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li>
  <li><a href="https://github.com/mesos/spark/wiki/Running-A-Spark-Standalone-Cluster">Running A Spark Standalone Cluster</a></li>
  <li><a href="http://sprism.blogspot.com/2012/11/lightning-fast-wordcount-using-spark.html">Lightning-Fast WordCount using Spark Alongside Hadoop</a></li>
</ol>

<p>以下博客都已经过时了：</p>

<ol>
  <li><a href="http://chapeau.freevariable.com/2013/04/installing-spark-on-fedora-18.html">Installing Spark on Fedora 18</a></li>
  <li><a href="http://rdc.taobao.com/team/jm/archives/1823">Spark随谈（二）—— 安装攻略</a></li>
  <li><a href="http://www.cnblogs.com/jerrylead/archive/2012/08/13/2636115.html">Spark安装与学习</a></li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安装Spark集群(在CentOS上)]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20130617"/>
    <updated>2013-06-17T22:16:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/installing-spark-on-centos-cn</id>
    <content type="html"><![CDATA[<p><strong>环境</strong>:CentOS 6.4, Hadoop 1.1.2, JDK 1.7, Spark 0.7.2, Scala 2.9.3</p>

<p>折腾了几天，终于把Spark 集群安装成功了，其实比hadoop要简单很多，由于网上搜索到的博客大部分都还停留在需要依赖mesos的版本，走了不少弯路。</p>

<h1 id="jdk-17">1. 安装 JDK 1.7</h1>
<pre><code>yum search openjdk-devel
sudo yum install java-1.7.0-openjdk-devel.x86_64
/usr/sbin/alternatives --config java
/usr/sbin/alternatives --config javac
sudo vim /etc/profile
# add the following lines at the end
export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.19.x86_64
export JRE_HOME=$JAVA_HOME/jre
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
# test
$ java -version
</code></pre>

<p>参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120423/">安装和配置CentOS服务器的详细步骤</a>。</p>

<h1 id="scala-293">2. 安装 Scala 2.9.3</h1>
<p>Spark 0.7.2 依赖 Scala 2.9.3, 我们必须要安装Scala 2.9.3.</p>

<p>下载 <a href="http://www.scala-lang.org/downloads/distrib/files/scala-2.9.3.tgz">scala-2.9.3.tgz</a> 并 保存到home目录.</p>

<pre><code>$ tar -zxf scala-2.9.3.tgz
$ sudo mv scala-2.9.3 /usr/lib
$ sudo vim /etc/profile
# add the following lines at the end
export SCALA_HOME=/usr/lib/scala-2.9.3
export PATH=$PATH:$SCALA_HOME/bin
# save and exit vim
#make the bash profile take effect immediately
source /etc/profile
# test
$ scala -version
</code></pre>

<h1 id="spark">3. 下载预编译好的Spark</h1>
<p>下载预编译好的Spark, <a href="http://www.spark-project.org/download-spark-0.7.2-prebuilt-hadoop1">spark-0.7.2-prebuilt-hadoop1.tgz</a>. </p>

<p>如果你想从零开始编译，则下载源码包，但是我不建议你这么做，因为有一个Maven仓库，twitter4j.org, 被墙了，导致编译时需要翻墙，非常麻烦。如果你有DIY精神，并能顺利翻墙，则可以试试这种方式。</p>

<h1 id="section">4. 本地模式</h1>

<h2 id="section-1">4.1 解压</h2>

<pre><code>$ tar -zxf spark-0.7.2-prebuilt-hadoop1.tgz
</code></pre>

<h2 id="sparkexamplesjar-">4.2 设置SPARK_EXAMPLES_JAR 环境变量</h2>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_EXAMPLES_JAR=$HOME/spark-0.7.2/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.2.jar
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<p>这一步其实最关键，很不幸的是，官方文档和网上的博客，都没有提及这一点。我是偶然看到了这两篇帖子，<a href="https://groups.google.com/forum/?fromgroups#!topic/spark-users/nQ6wB2lcFN8">Running SparkPi</a>, <a href="https://groups.google.com/forum/#!msg/spark-users/x5UczgI-Xm8/wzMm3Mb77-oJ">Null pointer exception when running ./run spark.examples.SparkPi local</a>，才补上了这一步，之前死活都无法运行SparkPi。</p>

<h2 id="sparkhomesparkhomebinpath">4.3 （可选）设置 SPARK_HOME环境变量，并将SPARK_HOME/bin加入PATH</h2>
<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.7.2
export PATH=$PATH:$SPARK_HOME/bin
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="sparkpi">4.4 现在可以运行SparkPi了</h2>

<pre><code>$ cd ~/spark-0.7.2
$ ./run spark.examples.SparkPi local 
</code></pre>

<h1 id="section-2">5. 集群模式</h1>

<!-- more -->

<h2 id="hadoop">5.1 安装Hadoop</h2>
<p>用VMware Workstation 创建三台CentOS 虚拟机，hostname分别设置为 master, slave01, slave02，设置SSH无密码登陆，安装hadoop，然后启动hadoop集群。参考我的这篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20130612">在CentOS上安装Hadoop</a>. </p>

<h2 id="scala">5.2 Scala</h2>
<p>在三台机器上都要安装 Scala 2.9.3 , 按照第2节的步骤。JDK在安装Hadoop时已经安装了。</p>

<h2 id="masterspark">5.3 在master上安装并配置Spark</h2>
<p>解压</p>

<pre><code>$ tar -zxf spark-0.7.2-prebuilt-hadoop1.tgz
</code></pre>

<p>设置SPARK_EXAMPLES_JAR 环境变量</p>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_EXAMPLES_JAR=$HOME/spark-0.7.2/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.2.jar
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<p>在 in <code>conf/spark-env.sh</code> 中设置<code>SCALA_HOME</code></p>

<pre><code>$ cd ~/spark-0.7.2/conf
$ mv spark-env.sh.template spark-env.sh
$ vim spark-env.sh
# add the following line
export SCALA_HOME=/usr/lib/scala-2.9.3
# save and exit
</code></pre>

<p>在<code>conf/slaves</code>, 添加Spark worker的hostname, 一行一个。</p>

<pre><code>$ vim slaves
slave01
slave02
# save and exit
</code></pre>

<p>（可选）设置 SPARK_HOME环境变量，并将SPARK_HOME/bin加入PATH</p>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.7.2
export PATH=$PATH:$SPARK_HOME/bin
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="workerspark">5.4 在所有worker上安装并配置Spark</h2>
<p>既然master上的这个文件件已经配置好了，把它拷贝到所有的worker。<strong>注意，三台机器spark所在目录必须一致，因为master会登陆到worker上执行命令，master认为worker的spark路径与自己一样。</strong></p>

<pre><code>$ cd
$ scp -r spark-0.7.2 dev@slave01:~
$ scp -r spark-0.7.2 dev@slave02:~
</code></pre>

<p>按照第5.3节设置<code>SPARK_EXAMPLES_JAR</code>环境变量，配置文件不用配置了，因为是直接从master复制过来的，已经配置好了。</p>

<h2 id="spark-">5.5 启动 Spark 集群</h2>
<p>在master上执行</p>

<pre><code>$ cd ~/spark-0.7.2
$ bin/start-all.sh
</code></pre>

<p>检测进程是否启动</p>

<pre><code>$ jps
11055 Jps
2313 SecondaryNameNode
2409 JobTracker
2152 NameNode
4822 Master
</code></pre>

<p>浏览master的web UI(默认<a href="http://localhost:8080">http://localhost:8080</a>). 这是你应该可以看到所有的word节点，以及他们的CPU个数和内存等信息。
##5.6 运行SparkPi例子</p>

<pre><code>$ cd ~/spark-0.7.2
$ ./run spark.examples.SparkPi spark://master:7077
</code></pre>

<p>（可选）运行自带的例子，SparkLR 和 SparkKMeans.</p>

<pre><code>#Logistic Regression
#./run spark.examples.SparkLR spark://master:7077
#kmeans
$ ./run spark.examples.SparkKMeans spark://master:7077 ./kmeans_data.txt 2 1
</code></pre>

<h2 id="hdfswordcount">5.7 从HDFS读取文件并运行WordCount</h2>

<pre><code>$ cd ~/spark-0.7.2
$ hadoop fs -put README.md .
$ MASTER=spark://master:7077 ./spark-shell
scala&gt; val file = sc.textFile("hdfs://master:9000/user/dev/README.md")
scala&gt; val count = file.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey(_+_)
scala&gt; count.collect()
</code></pre>

<h2 id="spark--1">5.8 停止 Spark 集群</h2>

<pre><code>$ cd ~/spark-0.7.2
$ bin/stop-all.sh
</code></pre>

<h1 id="section-3">参考资料</h1>
<ol>
  <li><a href="http://spark-project.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li>
  <li><a href="https://github.com/mesos/spark/wiki/Running-A-Spark-Standalone-Cluster">Running A Spark Standalone Cluster</a></li>
  <li><a href="http://sprism.blogspot.com/2012/11/lightning-fast-wordcount-using-spark.html">Lightning-Fast WordCount using Spark Alongside Hadoop</a></li>
</ol>

<p>以下博客都已经过时了：</p>

<ol>
  <li><a href="http://chapeau.freevariable.com/2013/04/installing-spark-on-fedora-18.html">Installing Spark on Fedora 18</a></li>
  <li><a href="http://rdc.taobao.com/team/jm/archives/1823">Spark随谈（二）—— 安装攻略</a></li>
  <li><a href="http://www.cnblogs.com/jerrylead/archive/2012/08/13/2636115.html">Spark安装与学习</a></li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing Spark on CentOS]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20130614"/>
    <updated>2013-06-14T19:06:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/installing-spark-on-centos</id>
    <content type="html"><![CDATA[<p><strong>Environment</strong>:CentOS 6.4, Hadoop 1.1.2, JDK 1.7, Spark 0.7.2, Scala 2.9.3</p>

<p>After a few days hacking , I have found that installing a Spark cluster is exteremely easy :)</p>

<h1 id="install-jdk-17">1. Install JDK 1.7</h1>
<pre><code>yum search openjdk-devel
sudo yum install java-1.7.0-openjdk-devel.x86_64
/usr/sbin/alternatives --config java
/usr/sbin/alternatives --config javac
sudo vim /etc/profile
# add the following lines at the end
export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.19.x86_64
export JRE_HOME=$JAVA_HOME/jre
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
# test
$ java -version
</code></pre>

<h1 id="install-scala-293">2. Install Scala 2.9.3</h1>
<p>Spark 0.7.2 depends on Scala 2.9.3, So we must install Scala of version 2.9.3.</p>

<p>Download <a href="http://www.scala-lang.org/downloads/distrib/files/scala-2.9.3.tgz">scala-2.9.3.tgz</a> and save it to home directory.</p>

<pre><code>$ tar -zxf scala-2.9.3.tgz
$ sudo mv scala-2.9.3 /usr/lib
$ sudo vim /etc/profile
# add the following lines at the end
export SCALA_HOME=/usr/lib/scala-2.9.3
export PATH=$PATH:$SCALA_HOME/bin
# save and exit vim
# make the bash profile take effect immediately
source /etc/profile
# test
$ scala -version
</code></pre>

<h1 id="download-prebuilt-packages">3. Download prebuilt packages</h1>
<p>Download prebuilt packages, <a href="http://www.spark-project.org/download-spark-0.7.2-prebuilt-hadoop1">spark-0.7.2-prebuilt-hadoop1.tgz</a>. </p>

<p>If you want to compile it from scratch, download the source package, but I don’t recommend this way, because in Chinese Mainland the GFW has blocked one of maven repositories, twitter4j.org, which makes the compilation an impossible mission unless you can conquer GFW.</p>

<h1 id="local-mode">4. Local Mode</h1>

<h2 id="untar-the-tarball">4.1 Untar the tarball</h2>

<pre><code>$ tar -zxf spark-0.7.2-prebuilt-hadoop1.tgz
</code></pre>

<h2 id="set-the-sparkexamplesjar-environment-variable">4.2 Set the SPARK_EXAMPLES_JAR environment variable</h2>
<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_EXAMPLES_JAR=$HOME/spark-0.7.2/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.2.jar
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<p>This is the most important step that must be done , but unfortunately the official docs and most web blogs haven’t mentioned this. I found this step when I bumped into these posts, <a href="https://groups.google.com/forum/?fromgroups#!topic/spark-users/nQ6wB2lcFN8">Running SparkPi</a>, <a href="https://groups.google.com/forum/#!msg/spark-users/x5UczgI-Xm8/wzMm3Mb77-oJ">Null pointer exception when running ./run spark.examples.SparkPi local</a>.</p>

<h2 id="optionalset-sparkhome-and-add-sparkhomebin-to-path">4.3 (Optional)Set SPARK_HOME and add SPARK_HOME/bin to PATH</h2>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.7.2
export PATH=$PATH:$SPARK_HOME/bin
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="now-you-can-run-sparkpi">4.4 Now you can run SparkPi.</h2>

<pre><code>$ cd ~/spark-0.7.2
$ ./run spark.examples.SparkPi local 
</code></pre>

<h1 id="cluster-mode">5. Cluster Mode</h1>

<!-- more -->

<h2 id="install-hadoop">5.1 Install hadoop</h2>
<p>Use VMware Workstation to create three CentOS virtual machines, which’s hostnames are master, slave01, slave02, setup password-less ssh to the slaves, install hadoop on the three machines and start up the hadoop cluster. For more details please read another blog of mine, <a href="http://www.yanjiuyanjiu.com/blog/20130612">在CentOS上安装Hadoop</a>.</p>

<h2 id="install-jdk-and-scala">5.2 Install JDK and Scala</h2>
<p>Install JDK 1.7 and Scala 2.9.3 on the three machines, according to section 1 and section 2.</p>

<h2 id="install-and-configure-spark-on-master">5.3 Install and configure Spark on master</h2>
<p>Untar</p>

<pre><code>$ tar -zxf spark-0.7.2-prebuilt-hadoop1.tgz
</code></pre>

<p>Set the SPARK_EXAMPLES_JAR environment variable</p>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_EXAMPLES_JAR=$HOME/spark-0.7.2/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.2.jar
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<p>Set <code>SCALA_HOME</code> in <code>conf/spark-env.sh</code></p>

<pre><code>$ cd ~/spark-0.7.2/conf
$ mv spark-env.sh.template spark-env.sh
$ vim spark-env.sh
# add the following line
export SCALA_HOME=/usr/lib/scala-2.9.3
# save and exit
</code></pre>

<p>In<code>conf/slaves</code>, add hostnames of Spark workers, one per line.</p>

<pre><code>$ vim slaves
slave01
slave02
# save and exit
</code></pre>

<p>(Optional)Set SPARK_HOME and add SPARK_HOME/bin to PATH</p>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.7.2
export PATH=$PATH:$SPARK_HOME/bin
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="install-and-configure-spark-on-workers">5.4 Install and configure Spark on workers</h2>
<p>Copy the spark directory to all slaves. <strong>Remark，the spark directories must locat at the the same path on all machines，because the master will login to work to execute spark commands, it assumes that workers have the same path as itself</strong></p>

<pre><code>$ cd
$ scp -r spark-0.7.2 dev@slave01:~
$ scp -r spark-0.7.2 dev@slave02:~
</code></pre>

<p>Set <code>SPARK_EXAMPLES_JAR</code>  on all slaves as section 5.3. There is no need to edit configuration files because they are copied from master, which are already well configured.</p>

<h2 id="start-spark-cluster">5.5 Start Spark cluster</h2>
<p>On master</p>

<pre><code>$ cd ~/spark-0.7.2
$ bin/start-all.sh
</code></pre>

<p>Check whether the processes have been started.</p>

<pre><code>$ jps
11055 Jps
2313 SecondaryNameNode
2409 JobTracker
2152 NameNode
4822 Master
</code></pre>

<p>Look at the master’s web UI (<a href="http://localhost:8080">http://localhost:8080</a> by default). You should see the new node listed there, along with its number of CPUs and memory (minus one gigabyte left for the OS).</p>

<h2 id="run-the-sparkpi-example-in-cluster-mode">5.6 run the SparkPi example in cluster mode</h2>

<pre><code>$ cd ~/spark-0.7.2
$ ./run spark.examples.SparkPi spark://master:7077
</code></pre>

<p>(Optional)Run built-in examples, SparkLR and SparkKMeans.</p>

<pre><code>#Logistic Regression
#./run spark.examples.SparkLR spark://master:7077
#kmeans
$ ./run spark.examples.SparkKMeans spark://master:7077 ./kmeans_data.txt 2 1
</code></pre>

<h2 id="read-files-from-hdfs-and-run-wordcount">5.7 read files from HDFS and run WordCount</h2>

<pre><code>$ cd ~/spark-0.7.2
$ hadoop fs -put README.md .
$ MASTER=spark://master:7077 ./spark-shell
scala&gt; val file = sc.textFile("hdfs://master:9000/user/dev/README.md")
scala&gt; val count = file.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey(_+_)
scala&gt; count.collect()
</code></pre>

<h2 id="stop-spark-cluster">5.8 Stop Spark cluster</h2>

<pre><code>$ cd ~/spark-0.7.2
$ bin/stop-all.sh
</code></pre>

<h1 id="references">References</h1>
<ol>
  <li><a href="http://spark-project.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li>
  <li><a href="https://github.com/mesos/spark/wiki/Running-A-Spark-Standalone-Cluster">Running A Spark Standalone Cluster</a></li>
  <li><a href="http://sprism.blogspot.com/2012/11/lightning-fast-wordcount-using-spark.html">Lightning-Fast WordCount using Spark Alongside Hadoop</a></li>
</ol>

<p>The following posts are outdated.</p>

<ol>
  <li><a href="http://chapeau.freevariable.com/2013/04/installing-spark-on-fedora-18.html">Installing Spark on Fedora 18</a></li>
  <li><a href="http://rdc.taobao.com/team/jm/archives/1823">Spark随谈（二）—— 安装攻略</a></li>
  <li><a href="http://www.cnblogs.com/jerrylead/archive/2012/08/13/2636115.html">Spark安装与学习</a></li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Scala IDE 阅读spark源码 -- 将sbt项目转化为eclipse项目]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20130611"/>
    <updated>2013-06-11T11:48:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/read-spark-source-code-using-scala-ide</id>
    <content type="html"><![CDATA[<p>阅读Spark源代码，最简单的方式是下载源码包，解压后用纯文本方式来阅读源码。这样效率不高，可以用sbteclipse这个插件，将sbt项目文件转化为eclipse项目文件，然后导入到Scala IDE，用eclipse来阅读源码，效率大大提高。</p>

<p><strong>环境</strong>：Windows 7, JDK 1.6</p>

<ol>
  <li>安装 scala。去官网 <a href="http://www.scala-lang.org/">http://www.scala-lang.org/</a> ，下载MSI，安装，按默认设置即可。</li>
  <li>
    <p>安装 sbt。去官网 <a href="http://www.scala-sbt.org/">http://www.scala-sbt.org/</a> ， 下载MSI，安装，按默认设置即可。
Linux下可以省略以上两步，spark源码自带了一个sbt，且启动sbt时它会自动下载对应的scala编译器。</p>
  </li>
  <li>
    <p>安装 Scala IDE。 去官网 <a href="http://scala-ide.org/">http://scala-ide.org/</a>，点击”Get the SDK”绿色按钮，下载。这个IDE的好处是，自带了scala编译器，解压即可使用。</p>
  </li>
  <li>
    <p>下载spark源码。 去官网 <a href="http://spark-project.org/">http://spark-project.org/</a> 下载源码，当前版本是 0.7.2, source package 大约4M左右。解压源码，例如 解压到 d:spark-0.7.0\</p>
  </li>
  <li>
    <p>添加 sbteclipse 插件依赖。spark已经添加了依赖，这一步什么也不需要做。</p>

    <p>这个插件的作用，就是能够读取sbt的配置文件，生成一个eclipse的工程文件。有了eclipse工程文件，就可以导入到eclipse了。</p>

    <p>spark已经添加了依赖，见 d:\spark-0.7.2\project\plugins.sbt，有一行</p>

    <p>addSbtPlugin(“com.typesafe.sbteclipse” % “sbteclipse-plugin” % “2.1.1”)</p>
  </li>
  <li>
    <p>启动cmd，启动sbt。</p>

    <blockquote>
      <p>cd  d:\spark-0.7.0<br />
 sbt</p>
    </blockquote>

    <p>Linux下则是</p>

    <blockquote>
      <p>cd  d:\spark-0.7.0<br />
 sbt/sbt</p>
    </blockquote>

    <p>开始下载各种依赖包，需要等待很长时间。</p>
  </li>
  <li>
    <p><strong>翻@_@墙。见本文最后一段。</strong></p>

    <p><!--more--></p>
  </li>
  <li>
    <p>等sbt提升符&gt;出现后，输入eclipse命令，开始生成eclipse工程文件，需要耐心等待一段时间</p>

    <blockquote>
      <p>&gt;eclipse<br />
 [info] About to create Eclipse project files for your project(s).<br />
 [info] Successfully created Eclipse project files for project(s):<br />
 [info] spark-examples  <br />
 [info] spark-streaming <br />
 [info] spark-repl<br />
 [info] spark-bagel<br />
 [info] spark-core  </p>
    </blockquote>

    <p>这样就生成成功了，去core, bagel, streaming, repl, examples 五个文件夹下，可以看到有一个.project和.classpath文件。从这里也可以看出，spark源码由五个项目组成。</p>
  </li>
  <li>用 Scala IDE 导入这5个工程，选择 d:\spark-0.7.2 文件夹，可以一次性导入5个项目。
项目图标上有红色感叹号，是因为jar包的路径不对，右击某个项目，选择 “Build Path -&gt; Configure Build Path” ，删除所有的jar，点击 “Add external jars”，浏览到 d:\spark-0.7.2\lib_managed\jars，添加所有的jar，这是红色感叹号就消失了。每个项目都如此操作一番。</li>
</ol>

<p><strong>注意：第8步在国内是无法成功的，因为一些maven仓库被墙，例如 twitter4j.org这个仓库就被墙了。因此需要翻@_@墙。</strong></p>

<p>我平时用goagent翻@_@墙，不过goagent只能让浏览器翻@_@墙，如何让goagent变成全局代理呢？即所有http协议都经过goagent。可以用 Proxifier，它可以把goagent变成操作系统全局的http代理。</p>

<p>不过 spark 在访问maven仓库时，用的是https网址，即https协议，虽然goagent可以用来访问https页面，但 goagent 和 Proxifier 使用时，https协议总是链接不通（参考 <a href="https://code.google.com/p/goagent/issues/detail?id=5210">https://code.google.com/p/goagent/issues/detail?id=5210</a>）。</p>

<p>于是我又想到了另一个方法，用SSH翻@_@墙。去网上找一个免费的ssh，安装 Bitvise SSH Client，然后在”Services”标签页面，勾选”SOCKS/HTTP Proxy Forwarding”，这样来翻@_@墙，Proxifier  使用  Bitvise SSH Client 提供的代理。</p>

<p>翻@_@墙成功后，再输入 eclipse，当到达 twitter4j.org 时，会发现 SUCCESS了。耐心等待，最后会成功生成.project文件。</p>

<p>用Scala IDE 导入项目，就可以开始阅读spark 源码了 :)</p>

<p>如果不想折腾，可以下载我已经生成好的项目, <a href="http://pan.baidu.com/share/link?shareid=534521368&amp;uk=2466605404">spark-0.7.2.zip</a>。解压，启动Scala IDE，选择菜单<code>File-&gt;Import-&gt;General-&gt;Existing projects into workspace</code>，浏览到 spark-0.7.2目录，批量导入5个项目。导入后项目图标有红色感叹号，这是因为你的电脑上路径和我的路径不一样，找不到引用的jar了。右击项目，选择<code>Build Path-&gt;Configure Build Path</code>，选择<code>Libraries</code>标签，这时可以看到所有jar都有红叉叉，全选，删除，然后点击<code>Add External Jars</code>，浏览到<code>spark-0.7.2\lib_managed\jars</code>，把所有jar都导入，导入后红色感叹号就消失了。对每个项目都执行上述操作。</p>

<p><strong>2013-07-27 更新</strong>：eclipse项目上有红色小叉叉图标，之前一直没解决，今天解决了，主要原因是，<strong>Scala IDE 版本不对！</strong> scala-ide.org 官网最新的的3.0.1只支持scala 2.10，不再支持2.9.3。由于Spark目前使用scala 2.9.3写的，所以我们要下载支持 scala 2.9.3 版，scala ide 3.0.0是支持 2.9.3的，不过首选要下载 eclipse JUNO，不要使用新版的eclipse，例如eclipse Indigo, Kepler都不行。</p>

<p>因此，正确的做法是，先下载 eclipse juno，然后下载 3.0.0 的zip包，解压，然后启动eclipse，点击菜单”help-&gt;Install New Software”，浏览到刚刚解压的<code>site</code>文件夹，就可以安装scala ide插件了。</p>
]]></content>
  </entry>
  
</feed>
