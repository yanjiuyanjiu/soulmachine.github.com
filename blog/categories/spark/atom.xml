<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Spark | 研究研究]]></title>
  <link href="http://www.yanjiuyanjiu.com/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://www.yanjiuyanjiu.com/"/>
  <updated>2014-03-19T23:23:41+08:00</updated>
  <id>http://www.yanjiuyanjiu.com/</id>
  <author>
    <name><![CDATA[soulmachine]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Spark开发环境的配置]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140130"/>
    <updated>2014-01-30T16:36:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/spark-development-environment</id>
    <content type="html"><![CDATA[<p><strong>软件版本</strong>：Spark 0.9</p>

<p>配置Spark开发环境，其实分为三个层次，一种是针对运维人员，把Spark安装部署到集群；一种是针对普通开发者，引入Spark的jar包，调用Spark提供的接口，编写分布式程序，写好后编译成jar，就可以提交到Spark集群去运行了；第三种是针对Spark开发者，为了给Spark贡献代码，需要git clone Spark的代码，然后导入IDE，为Spark开发代码。</p>

<h2 id="spark">1 部署Spark集群</h2>
<p>这种是运维人员在生产环境下，搭建起一个Spark集群。</p>

<h3 id="spark-1">（可选）创建新用户 Spark</h3>
<p>一般我倾向于把需要启动daemon进程，对外提供服务的程序，即服务器类的程序，安装在单独的用户下面。这样可以做到隔离，运维方面，安全性也提高了。</p>

<p>创建一个新的group,</p>

<pre><code>$ sudo groupadd spark
</code></pre>

<p>创建一个新的用户，并加入group,</p>

<pre><code>$ sudo useradd -g spark spark
</code></pre>

<p>给新用户设置密码，</p>

<pre><code>$ sudo passwd spark
</code></pre>

<p>在每台机器上创建 spark 新用户，并配置好SSH无密码，参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120102/">SSH无密码登录的配置</a></p>

<p>假设有三台机器，hostname分别是 master, worker01, worker02。</p>

<h3 id="spark-">1.1 下载 Spark 预编译好的二进制包</h3>
<p>如果你需要用到HDFS，则要针对Hadoop 1.x 和Hadoop 2.x 选择不同的版本。这里我选择 Hadoop 2.x 版。</p>

<pre><code>spark@master $ wget http://d3kbcqa49mib13.cloudfront.net/spark-0.9.0-incubating-bin-hadoop1.tgz
spark@master $ tar zxf spark-0.9.0-incubating-bin-hadoop1.tgz -C ~/local/opt
</code></pre>

<h3 id="tgzscp">1.2 将tgz压缩包scp到所有机器，解压到相同的路径</h3>

<pre><code>spark@master $ scp spark-0.9.0-incubating-bin-hadoop1.tgz spark@worker01:~
spark@master $ ssh worker01
spark@worker01 $ tar zxf spark-0.9.0-incubating-bin-hadoop1.tgz -C ~/local/opt
spark@worker01 $ exit
spark@master $ scp spark-0.9.0-incubating-bin-hadoop1.tgz spark@worker02:~
spark@master $ ssh worker02
spark@worker02 $ tar zxf spark-0.9.0-incubating-bin-hadoop1.tgz -C ~/local/opt
spark@worker02 $ exit
</code></pre>

<h3 id="section">1.3 修改配置文件</h3>
<p>Spark 0.9 以后，配置文件简单多了，只有一个必须要配置，就是 <code>conf/slaves</code> 这个文件。在这个文件里添加slave的hostname。</p>

<h3 id="slave">1.4 拷贝配置文件到所有slave</h3>

<pre><code>spark@master $ spark@master $ scp ./conf/slaves spark@worker01:~/local/opt/spark-0.9.0-incubating-bin-hadoop1/conf
spark@master $ spark@master $ scp ./conf/slaves spark@worker02:~/local/opt/spark-0.9.0-incubating-bin-hadoop1/conf
</code></pre>

<h3 id="spark-2">1.5 启动Spark集群</h3>

<pre><code>spark@master $ ./sbin/start-all.sh
</code></pre>

<!-- more -->

<p>也可以一台一台启动，先启动 master</p>

<pre><code>spark@master $ ./sbin/start-master.sh
</code></pre>

<p>启动两台 slave，</p>

<pre><code>spark@worker01 $ ./sbin/start-slave.sh 1 spark://master:7077
spark@worker02 $ ./sbin/start-slave.sh 2 spark://master:7077
</code></pre>

<p>其中，<code>1</code>, <code>2</code> 是 worker的编号，可以是任意数字，只要不重复即可，<code>spark://master:7077</code> 是 master 的地址。以后向集群提交作业的时候，也需要这个地址。</p>

<h3 id="section-1">1.6 测试一下，向集群提交一个作业</h3>

<pre><code>spark@master $ ./bin/run-example org.apache.spark.examples.SparkPi spark://master:7077
</code></pre>

<h2 id="section-2">2 配置普通开发环境</h2>
<p>TODO</p>

<h2 id="spark-3">3 配置Spark开发环境</h2>
<p>当你需要修改Spark的代码，或给Spark添加代码，就需要阅读本节了。</p>

<h3 id="git-clone-">3.1 git clone 代码</h3>

<pre><code>git clone git@github.com:apache/incubator-spark.git
</code></pre>

<h3 id="section-3">3.2 编译</h3>
<p>Spark脚本会自动下载对应版本的sbt和scala编译器，因此机器事先不需要安装sbt和scala</p>

<p>按照 github 官方repo首页的文档，输入如下一行命令即可开始编译，</p>

<pre><code>./sbt/sbt assembly
</code></pre>

<h3 id="section-4">3.3 运行一个例子</h3>

<pre><code>./run-example org.apache.spark.examples.SparkPi local
</code></pre>

<p>说明安装成功了。</p>

<h3 id="spark-shell">3.4 试用 spark shell</h3>
<!-- more -->

<p>./spark-shell</p>

<p>会出现<code>scala&gt;</code>提示符号，可见spark脚本自动下载了scala编译器，其实就是一个jar，例如<code>scala-compiler-2.10.3.jar</code>。</p>

<h3 id="scala">3.5 安装scala</h3>
<p>开发Spark的时候，由于Intellij Idea 需要调用外部的sbt和scala，因此机器上还是需要安装scala和sbt。</p>

<p>打开 <code>projects/SparkBuild.scala</code>，搜索<code>scalaVersion</code>，获得spark所使用的scala编译器版本，然后去scala官网<a href="http://www.scala-lang.org/">http://www.scala-lang.org/</a>，下载该版本的scala编译器，并设置<code>SCALA_HOME</code>环境变量，将bin目录加入PATH。例如下载scala-2.10.3.tgz，解压到/opt，设置环境变量如下：</p>

<pre><code>sudo vim /etc/profile
export SCALA_HOME=/opt/scala-2.10.3
export PATH=$PATH:$SCALA_HOME/bin
</code></pre>

<h3 id="sbt">3.6 安装sbt</h3>

<p>打开<code>projects/build.properties</code>，可以看到spark所使用的sbt版本号，去
官网<a href="http://www.scala-sbt.org/">http://www.scala-sbt.org/</a>下载该版本的sbt，双击安装。并设置<code>SBT_HOME</code>环境变量，将bin目录加入PATH。</p>

<h3 id="idea">3.7 下载并安装idea</h3>

<p>Spark核心团队的hashjoin曾在我博客上留言，说他们都使用idea在开发spark，我用过<a href="www.scala-ide.org">Scala IDE</a>和idea，两者各有优劣，总的来说，idea要好用一些，虽然我是老牌eclipse用户，但我还是转向了idea。</p>

<p>去idea官网下载idea的tar.gz包，解压就行。运行idea，安装scala插件。</p>

<h3 id="idea-1">3.8 生成idea项目文件</h3>
<p>在源码根目录，使用如下命令</p>

<pre><code>./sbt/sbt gen-idea
</code></pre>

<p>就生成了idea项目文件。</p>

<h3 id="open-project">3.9 Open Project</h3>
<p>使用 idea，点击<code>File-&gt;Open project</code>，浏览到 <code>incubator-spark</code>文件夹，打开项目，就可以修改Spark代码了。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用docker打造spark集群]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20131027"/>
    <updated>2013-10-27T20:30:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/build-spark-cluster-with-docker</id>
    <content type="html"><![CDATA[<p><strong>前提条件：</strong>安装好了docker，见我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20131025">Docker安装</a></p>

<p>有两种方式，</p>

<ul>
  <li><a href="https://github.com/apache/incubator-spark">Spark官方repo</a>里，docker文件夹下的脚本。官方的这个脚本封装很薄，尽可能把必要的信息展示出来。</li>
  <li><a href="https://github.com/amplab/docker-scripts">AMPLab开源的这个独立小项目</a>，来打造一个spark集群。这个脚本封装很深，自带了一个DNS服务器，还有hadoop，非常自动化，缺点是很多信息看不到了。</li>
</ul>

<h1 id="section">1. 第1种方式</h1>

<h2 id="git-clone-">git clone 源码</h2>
<p>首先要把官方repo的代码下载下来</p>

<pre><code>git clone git@github.com:apache/incubator-spark.git
</code></pre>

<h2 id="apt">（可选）修改apt源</h2>
<p>在国内，将apt源修改国内源，例如163的源，速度会快很多。将<code>base/Dockerfile</code>里的</p>

<pre><code>RUN echo "deb http://archive.ubuntu.com/ubuntu precise main universe" &gt; /etc/apt/sources.list
</code></pre>

<p>替换为</p>

<pre><code>RUN echo "deb http://mirrors.163.com/ubuntu/ precise main restricted universe multiverse" &gt; /etc/apt/sources.list
RUN echo "deb http://mirrors.163.com/ubuntu/ precise-security main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb http://mirrors.163.com/ubuntu/ precise-updates main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb http://mirrors.163.com/ubuntu/ precise-proposed main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb http://mirrors.163.com/ubuntu/ precise-backports main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise-security main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise-updates main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise-proposed main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise-backports main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
</code></pre>

<h2 id="build">build镜像</h2>
<p>将<code>build</code>和<code>spark-test/build</code>里的<code>docker</code>命令前，添加<code>sudo</code>，然后执行<code>docker</code>下的<code>build</code></p>

<pre><code>cd docker
./build
</code></pre>

<h2 id="master">启动master</h2>

<pre><code>sudo docker run -v $SPARK_HOME:/opt/spark spark-test-master
</code></pre>

<h2 id="worker">启动worker</h2>
<p>新开一个终端窗口（强烈推荐tmux），启动一个worker</p>

<pre><code>sudo docker run -v $SPARK_HOME:/opt/spark spark-test-worker &lt;master_ip&gt;
</code></pre>

<p>可以在master终端窗口看到worker注册上来了。</p>

<p>可以再开多个终端窗口，启动多个worker。</p>

<h1 id="section-1">2. 第2种方式</h1>

<h2 id="wget">升级wget</h2>
<p>如果发现wget不识别<code>--no-proxy</code>选项，需要升级wget。</p>

<h2 id="section-2">下载镜像</h2>
<p>为了让脚本第一次执行的时候更快，还是手动下载所有的镜像吧，amplab在index.docker.io上有一个官方账号，把这个账号有关spark的repo都pull下来。</p>

<pre><code>sudo docker pull amplab/apache-hadoop-hdfs-precise
sudo docker pull amplab/dnsmasq-precise
sudo docker pull amplab/spark-worker
sudo docker pull amplab/spark-master
sudo docker pull amplab/spark-shell
</code></pre>

<h2 id="git-clone--1">git clone 脚本</h2>

<pre><code>git@github.com:amplab/docker-scripts.git
</code></pre>

<p>这个脚本可以一键启动集群，爽啊哈哈哈！</p>

<h2 id="spark">一键启动spark集群</h2>

<pre><code>sudo ./deploy/deploy.sh -i amplab/spark:0.8.0 -w 3 
</code></pre>

<h2 id="spark-shell">启动 Spark shell</h2>
<p>启动一个交互式shell吧，IP为上一步输出的Master的IP</p>

<pre><code>sudo docker run -i -t -dns 172.17.0.90 amplab/spark-shell:0.8.0
</code></pre>

<h2 id="section-3">运行一个简单的的例子</h2>

<pre><code>scala&gt; val textFile = sc.textFile("hdfs://master:9000/user/hdfs/test.txt")
scala&gt; textFile.count()
scala&gt; textFile.map({line =&gt; line}).collect()
</code></pre>

<h2 id="section-4">关闭集群</h2>

<pre><code>$ sudo ./deploy/kill_all.sh spark
$ sudo ./deploy/kill_all.sh nameserver
</code></pre>

<h2 id="section-5">更多详情请参考项目主页的文档</h2>

<p><a href="https://github.com/amplab/docker-scripts">https://github.com/amplab/docker-scripts</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安装Spark 0.8 集群(在CentOS上)]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20131017"/>
    <updated>2013-10-17T11:58:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/installing-spark-on-centos-cn</id>
    <content type="html"><![CDATA[<p><strong>环境</strong>:CentOS 6.4, Hadoop 1.1.2, JDK 1.7, Spark 0.8.0, Scala 2.9.3</p>

<p>Spark 0.7.2 的安装请看之前的一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20130617/">安装Spark集群(在CentOS上)</a> 。</p>

<p>Spark的安装很简单，总结起来一句话：下载，解压，然后拷贝到所有机器，完毕，无需任何配置。</p>

<h1 id="jdk-17">1. 安装 JDK 1.7</h1>
<pre><code>yum search openjdk-devel
sudo yum install java-1.7.0-openjdk-devel.x86_64
/usr/sbin/alternatives --config java
/usr/sbin/alternatives --config javac
sudo vim /etc/profile
# add the following lines at the end
export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.19.x86_64
export JRE_HOME=$JAVA_HOME/jre
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
# test
$ java -version
</code></pre>

<p>参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120423/">安装和配置CentOS服务器的详细步骤</a>。</p>

<h1 id="scala-293">2. 安装 Scala 2.9.3</h1>
<p>Spark 0.8.0 依赖 Scala 2.9.3, 我们必须要安装Scala 2.9.3.</p>

<p>下载 <a href="http://www.scala-lang.org/downloads/distrib/files/scala-2.9.3.tgz">scala-2.9.3.tgz</a> 并 保存到home目录.</p>

<pre><code>$ tar -zxf scala-2.9.3.tgz
$ sudo mv scala-2.9.3 /usr/lib
$ sudo vim /etc/profile
# add the following lines at the end
export SCALA_HOME=/usr/lib/scala-2.9.3
export PATH=$PATH:$SCALA_HOME/bin
# save and exit vim
#make the bash profile take effect immediately
source /etc/profile
# test
$ scala -version
</code></pre>

<h1 id="spark">3. 下载预编译好的Spark</h1>
<p>下载预编译好的Spark, <a href="http://spark-project.org/download/spark-0.8.0-incubating-bin-hadoop1.tgz">spark-0.8.0-incubating-bin-hadoop1.tgz</a>. </p>

<p>如果你想从零开始编译，则下载源码包，但是我不建议你这么做，因为有一个Maven仓库，twitter4j.org, 被墙了，导致编译时需要翻墙，非常麻烦。如果你有DIY精神，并能顺利翻墙，则可以试试这种方式。</p>

<h1 id="local">4. Local模式</h1>

<h2 id="section">4.1 解压</h2>

<pre><code>$ tar -zxf spark-0.8.0-incubating-bin-hadoop1.tgz
</code></pre>

<h2 id="sparkhome">4.2 （可选）设置 SPARK_HOME环境变量</h2>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.8.0
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="sparkpi">4.3 现在可以运行SparkPi了</h2>

<pre><code>$ cd $SPARK_HOME
$ ./run-example org.apache.spark.examples.SparkPi local
</code></pre>

<h1 id="cluster">5. Cluster模式</h1>

<!-- more -->

<h2 id="hadoop">5.1 安装Hadoop</h2>
<p>用VMware Workstation 创建三台CentOS 虚拟机，hostname分别设置为 master, slave01, slave02，设置SSH无密码登陆，安装hadoop，然后启动hadoop集群。参考我的这篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20130612">在CentOS上安装Hadoop</a>. </p>

<h2 id="scala">5.2 Scala</h2>
<p>在三台机器上都要安装 Scala 2.9.3 , 按照第2节的步骤。JDK在安装Hadoop时已经安装了。</p>

<h2 id="masterspark">5.3 在master上安装并配置Spark</h2>
<p>解压</p>

<pre><code>$ tar -zxf spark-0.8.0-incubating-bin-hadoop1.tgz.tgz
</code></pre>

<p>在 in <code>conf/spark-env.sh</code> 中设置<code>SCALA_HOME</code></p>

<pre><code>$ cd ~/spark-0.8.0/conf
$ mv spark-env.sh.template spark-env.sh
$ vim spark-env.sh
# add the following line
export SCALA_HOME=/usr/lib/scala-2.9.3
# save and exit
</code></pre>

<p>在<code>conf/slaves</code>, 添加Spark worker的hostname, 一行一个。</p>

<pre><code>$ vim slaves
slave01
slave02
# save and exit
</code></pre>

<p>（可选）设置 SPARK_HOME环境变量，并将SPARK_HOME/bin加入PATH</p>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.8.0
export PATH=$PATH:$SPARK_HOME/bin
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="workerspark">5.4 在所有worker上安装并配置Spark</h2>
<p>既然master上的这个文件件已经配置好了，把它拷贝到所有的worker即可。<strong>注意，三台机器spark所在目录必须一致，因为master会登陆到worker上执行命令，master认为worker的spark路径与自己一样。</strong></p>

<pre><code>$ cd
$ scp -r spark-0.8.0 dev@slave01:~
$ scp -r spark-0.8.0 dev@slave02:~
</code></pre>

<h2 id="spark-">5.5 启动 Spark 集群</h2>
<p>在master上执行</p>

<pre><code>$ cd ~/spark-0.8.0
$ bin/start-all.sh
</code></pre>

<p>检测进程是否启动</p>

<pre><code>$ jps
11055 Jps
2313 SecondaryNameNode
2409 JobTracker
2152 NameNode
4822 Master
</code></pre>

<p>浏览master的web UI(默认<a href="http://localhost:8080">http://localhost:8080</a>). 这是你应该可以看到所有的word节点，以及他们的CPU个数和内存等信息。</p>

<h2 id="spark-1">5.6 运行Spark自带的例子</h2>

<p>运行SparkPi</p>

<pre><code>$ cd ~/spark-0.8.0
$ ./run-example org.apache.spark.examples.SparkPi spark://master:7077
</code></pre>

<p>运行 SparkLR</p>

<pre><code>#Logistic Regression
#./run-example org.apache.spark.examples.SparkLR spark://master:7077
</code></pre>

<p>运行 SparkKMeans</p>

<pre><code>#kmeans
$ ./run-example org.apache.spark.examples.SparkKMeans spark://master:7077 ./kmeans_data.txt 2 1
</code></pre>

<h2 id="hdfswordcount">5.7 从HDFS读取文件并运行WordCount</h2>

<pre><code>$ cd ~/spark-0.8.0
$ hadoop fs -put README.md .
$ MASTER=spark://master:7077 ./spark-shell
scala&gt; val file = sc.textFile("hdfs://master:9000/user/dev/README.md")
scala&gt; val count = file.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey(_+_)
scala&gt; count.collect()
</code></pre>

<h2 id="spark--1">5.8 停止 Spark 集群</h2>

<pre><code>$ cd ~/spark-0.8.0
$ bin/stop-all.sh
</code></pre>

<h1 id="section-1">参考资料</h1>
<ol>
  <li><a href="http://spark-project.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li>
  <li><a href="https://github.com/mesos/spark/wiki/Running-A-Spark-Standalone-Cluster">Running A Spark Standalone Cluster</a></li>
  <li><a href="http://sprism.blogspot.com/2012/11/lightning-fast-wordcount-using-spark.html">Lightning-Fast WordCount using Spark Alongside Hadoop</a></li>
</ol>

<p>以下博客都已经过时了：</p>

<ol>
  <li><a href="http://chapeau.freevariable.com/2013/04/installing-spark-on-fedora-18.html">Installing Spark on Fedora 18</a></li>
  <li><a href="http://rdc.taobao.com/team/jm/archives/1823">Spark随谈（二）—— 安装攻略</a></li>
  <li><a href="http://www.cnblogs.com/jerrylead/archive/2012/08/13/2636115.html">Spark安装与学习</a></li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安装Spark集群(在CentOS上)]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20130617"/>
    <updated>2013-06-17T22:16:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/installing-spark-on-centos-cn</id>
    <content type="html"><![CDATA[<p><strong>环境</strong>:CentOS 6.4, Hadoop 1.1.2, JDK 1.7, Spark 0.7.2, Scala 2.9.3</p>

<p>折腾了几天，终于把Spark 集群安装成功了，其实比hadoop要简单很多，由于网上搜索到的博客大部分都还停留在需要依赖mesos的版本，走了不少弯路。</p>

<h1 id="jdk-17">1. 安装 JDK 1.7</h1>
<pre><code>yum search openjdk-devel
sudo yum install java-1.7.0-openjdk-devel.x86_64
/usr/sbin/alternatives --config java
/usr/sbin/alternatives --config javac
sudo vim /etc/profile
# add the following lines at the end
export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.19.x86_64
export JRE_HOME=$JAVA_HOME/jre
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
# test
$ java -version
</code></pre>

<p>参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120423/">安装和配置CentOS服务器的详细步骤</a>。</p>

<h1 id="scala-293">2. 安装 Scala 2.9.3</h1>
<p>Spark 0.7.2 依赖 Scala 2.9.3, 我们必须要安装Scala 2.9.3.</p>

<p>下载 <a href="http://www.scala-lang.org/downloads/distrib/files/scala-2.9.3.tgz">scala-2.9.3.tgz</a> 并 保存到home目录.</p>

<pre><code>$ tar -zxf scala-2.9.3.tgz
$ sudo mv scala-2.9.3 /usr/lib
$ sudo vim /etc/profile
# add the following lines at the end
export SCALA_HOME=/usr/lib/scala-2.9.3
export PATH=$PATH:$SCALA_HOME/bin
# save and exit vim
#make the bash profile take effect immediately
source /etc/profile
# test
$ scala -version
</code></pre>

<h1 id="spark">3. 下载预编译好的Spark</h1>
<p>下载预编译好的Spark, <a href="http://www.spark-project.org/download-spark-0.7.2-prebuilt-hadoop1">spark-0.7.2-prebuilt-hadoop1.tgz</a>. </p>

<p>如果你想从零开始编译，则下载源码包，但是我不建议你这么做，因为有一个Maven仓库，twitter4j.org, 被墙了，导致编译时需要翻墙，非常麻烦。如果你有DIY精神，并能顺利翻墙，则可以试试这种方式。</p>

<h1 id="section">4. 本地模式</h1>

<h2 id="section-1">4.1 解压</h2>

<pre><code>$ tar -zxf spark-0.7.2-prebuilt-hadoop1.tgz
</code></pre>

<h2 id="sparkexamplesjar-">4.2 设置SPARK_EXAMPLES_JAR 环境变量</h2>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_EXAMPLES_JAR=$HOME/spark-0.7.2/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.2.jar
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<p>这一步其实最关键，很不幸的是，官方文档和网上的博客，都没有提及这一点。我是偶然看到了这两篇帖子，<a href="https://groups.google.com/forum/?fromgroups#!topic/spark-users/nQ6wB2lcFN8">Running SparkPi</a>, <a href="https://groups.google.com/forum/#!msg/spark-users/x5UczgI-Xm8/wzMm3Mb77-oJ">Null pointer exception when running ./run spark.examples.SparkPi local</a>，才补上了这一步，之前死活都无法运行SparkPi。</p>

<h2 id="sparkhomesparkhomebinpath">4.3 （可选）设置 SPARK_HOME环境变量，并将SPARK_HOME/bin加入PATH</h2>
<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.7.2
export PATH=$PATH:$SPARK_HOME/bin
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="sparkpi">4.4 现在可以运行SparkPi了</h2>

<pre><code>$ cd ~/spark-0.7.2
$ ./run spark.examples.SparkPi local 
</code></pre>

<h1 id="section-2">5. 集群模式</h1>

<!-- more -->

<h2 id="hadoop">5.1 安装Hadoop</h2>
<p>用VMware Workstation 创建三台CentOS 虚拟机，hostname分别设置为 master, slave01, slave02，设置SSH无密码登陆，安装hadoop，然后启动hadoop集群。参考我的这篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20130612">在CentOS上安装Hadoop</a>. </p>

<h2 id="scala">5.2 Scala</h2>
<p>在三台机器上都要安装 Scala 2.9.3 , 按照第2节的步骤。JDK在安装Hadoop时已经安装了。</p>

<h2 id="masterspark">5.3 在master上安装并配置Spark</h2>
<p>解压</p>

<pre><code>$ tar -zxf spark-0.7.2-prebuilt-hadoop1.tgz
</code></pre>

<p>设置SPARK_EXAMPLES_JAR 环境变量</p>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_EXAMPLES_JAR=$HOME/spark-0.7.2/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.2.jar
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<p>在 in <code>conf/spark-env.sh</code> 中设置<code>SCALA_HOME</code></p>

<pre><code>$ cd ~/spark-0.7.2/conf
$ mv spark-env.sh.template spark-env.sh
$ vim spark-env.sh
# add the following line
export SCALA_HOME=/usr/lib/scala-2.9.3
# save and exit
</code></pre>

<p>在<code>conf/slaves</code>, 添加Spark worker的hostname, 一行一个。</p>

<pre><code>$ vim slaves
slave01
slave02
# save and exit
</code></pre>

<p>（可选）设置 SPARK_HOME环境变量，并将SPARK_HOME/bin加入PATH</p>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.7.2
export PATH=$PATH:$SPARK_HOME/bin
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="workerspark">5.4 在所有worker上安装并配置Spark</h2>
<p>既然master上的这个文件件已经配置好了，把它拷贝到所有的worker。<strong>注意，三台机器spark所在目录必须一致，因为master会登陆到worker上执行命令，master认为worker的spark路径与自己一样。</strong></p>

<pre><code>$ cd
$ scp -r spark-0.7.2 dev@slave01:~
$ scp -r spark-0.7.2 dev@slave02:~
</code></pre>

<p>按照第5.3节设置<code>SPARK_EXAMPLES_JAR</code>环境变量，配置文件不用配置了，因为是直接从master复制过来的，已经配置好了。</p>

<h2 id="spark-">5.5 启动 Spark 集群</h2>
<p>在master上执行</p>

<pre><code>$ cd ~/spark-0.7.2
$ bin/start-all.sh
</code></pre>

<p>检测进程是否启动</p>

<pre><code>$ jps
11055 Jps
2313 SecondaryNameNode
2409 JobTracker
2152 NameNode
4822 Master
</code></pre>

<p>浏览master的web UI(默认<a href="http://localhost:8080">http://localhost:8080</a>). 这是你应该可以看到所有的word节点，以及他们的CPU个数和内存等信息。
##5.6 运行SparkPi例子</p>

<pre><code>$ cd ~/spark-0.7.2
$ ./run spark.examples.SparkPi spark://master:7077
</code></pre>

<p>（可选）运行自带的例子，SparkLR 和 SparkKMeans.</p>

<pre><code>#Logistic Regression
#./run spark.examples.SparkLR spark://master:7077
#kmeans
$ ./run spark.examples.SparkKMeans spark://master:7077 ./kmeans_data.txt 2 1
</code></pre>

<h2 id="hdfswordcount">5.7 从HDFS读取文件并运行WordCount</h2>

<pre><code>$ cd ~/spark-0.7.2
$ hadoop fs -put README.md .
$ MASTER=spark://master:7077 ./spark-shell
scala&gt; val file = sc.textFile("hdfs://master:9000/user/dev/README.md")
scala&gt; val count = file.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey(_+_)
scala&gt; count.collect()
</code></pre>

<h2 id="spark--1">5.8 停止 Spark 集群</h2>

<pre><code>$ cd ~/spark-0.7.2
$ bin/stop-all.sh
</code></pre>

<h1 id="section-3">参考资料</h1>
<ol>
  <li><a href="http://spark-project.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li>
  <li><a href="https://github.com/mesos/spark/wiki/Running-A-Spark-Standalone-Cluster">Running A Spark Standalone Cluster</a></li>
  <li><a href="http://sprism.blogspot.com/2012/11/lightning-fast-wordcount-using-spark.html">Lightning-Fast WordCount using Spark Alongside Hadoop</a></li>
</ol>

<p>以下博客都已经过时了：</p>

<ol>
  <li><a href="http://chapeau.freevariable.com/2013/04/installing-spark-on-fedora-18.html">Installing Spark on Fedora 18</a></li>
  <li><a href="http://rdc.taobao.com/team/jm/archives/1823">Spark随谈（二）—— 安装攻略</a></li>
  <li><a href="http://www.cnblogs.com/jerrylead/archive/2012/08/13/2636115.html">Spark安装与学习</a></li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing Spark on CentOS]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20130614"/>
    <updated>2013-06-14T19:06:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/installing-spark-on-centos</id>
    <content type="html"><![CDATA[<p><strong>Environment</strong>:CentOS 6.4, Hadoop 1.1.2, JDK 1.7, Spark 0.7.2, Scala 2.9.3</p>

<p>After a few days hacking , I have found that installing a Spark cluster is exteremely easy :)</p>

<h1 id="install-jdk-17">1. Install JDK 1.7</h1>
<pre><code>yum search openjdk-devel
sudo yum install java-1.7.0-openjdk-devel.x86_64
/usr/sbin/alternatives --config java
/usr/sbin/alternatives --config javac
sudo vim /etc/profile
# add the following lines at the end
export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.19.x86_64
export JRE_HOME=$JAVA_HOME/jre
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
# test
$ java -version
</code></pre>

<h1 id="install-scala-293">2. Install Scala 2.9.3</h1>
<p>Spark 0.7.2 depends on Scala 2.9.3, So we must install Scala of version 2.9.3.</p>

<p>Download <a href="http://www.scala-lang.org/downloads/distrib/files/scala-2.9.3.tgz">scala-2.9.3.tgz</a> and save it to home directory.</p>

<pre><code>$ tar -zxf scala-2.9.3.tgz
$ sudo mv scala-2.9.3 /usr/lib
$ sudo vim /etc/profile
# add the following lines at the end
export SCALA_HOME=/usr/lib/scala-2.9.3
export PATH=$PATH:$SCALA_HOME/bin
# save and exit vim
# make the bash profile take effect immediately
source /etc/profile
# test
$ scala -version
</code></pre>

<h1 id="download-prebuilt-packages">3. Download prebuilt packages</h1>
<p>Download prebuilt packages, <a href="http://www.spark-project.org/download-spark-0.7.2-prebuilt-hadoop1">spark-0.7.2-prebuilt-hadoop1.tgz</a>. </p>

<p>If you want to compile it from scratch, download the source package, but I don’t recommend this way, because in Chinese Mainland the GFW has blocked one of maven repositories, twitter4j.org, which makes the compilation an impossible mission unless you can conquer GFW.</p>

<h1 id="local-mode">4. Local Mode</h1>

<h2 id="untar-the-tarball">4.1 Untar the tarball</h2>

<pre><code>$ tar -zxf spark-0.7.2-prebuilt-hadoop1.tgz
</code></pre>

<h2 id="set-the-sparkexamplesjar-environment-variable">4.2 Set the SPARK_EXAMPLES_JAR environment variable</h2>
<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_EXAMPLES_JAR=$HOME/spark-0.7.2/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.2.jar
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<p>This is the most important step that must be done , but unfortunately the official docs and most web blogs haven’t mentioned this. I found this step when I bumped into these posts, <a href="https://groups.google.com/forum/?fromgroups#!topic/spark-users/nQ6wB2lcFN8">Running SparkPi</a>, <a href="https://groups.google.com/forum/#!msg/spark-users/x5UczgI-Xm8/wzMm3Mb77-oJ">Null pointer exception when running ./run spark.examples.SparkPi local</a>.</p>

<h2 id="optionalset-sparkhome-and-add-sparkhomebin-to-path">4.3 (Optional)Set SPARK_HOME and add SPARK_HOME/bin to PATH</h2>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.7.2
export PATH=$PATH:$SPARK_HOME/bin
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="now-you-can-run-sparkpi">4.4 Now you can run SparkPi.</h2>

<pre><code>$ cd ~/spark-0.7.2
$ ./run spark.examples.SparkPi local 
</code></pre>

<h1 id="cluster-mode">5. Cluster Mode</h1>

<!-- more -->

<h2 id="install-hadoop">5.1 Install hadoop</h2>
<p>Use VMware Workstation to create three CentOS virtual machines, which’s hostnames are master, slave01, slave02, setup password-less ssh to the slaves, install hadoop on the three machines and start up the hadoop cluster. For more details please read another blog of mine, <a href="http://www.yanjiuyanjiu.com/blog/20130612">在CentOS上安装Hadoop</a>.</p>

<h2 id="install-jdk-and-scala">5.2 Install JDK and Scala</h2>
<p>Install JDK 1.7 and Scala 2.9.3 on the three machines, according to section 1 and section 2.</p>

<h2 id="install-and-configure-spark-on-master">5.3 Install and configure Spark on master</h2>
<p>Untar</p>

<pre><code>$ tar -zxf spark-0.7.2-prebuilt-hadoop1.tgz
</code></pre>

<p>Set the SPARK_EXAMPLES_JAR environment variable</p>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_EXAMPLES_JAR=$HOME/spark-0.7.2/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.2.jar
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<p>Set <code>SCALA_HOME</code> in <code>conf/spark-env.sh</code></p>

<pre><code>$ cd ~/spark-0.7.2/conf
$ mv spark-env.sh.template spark-env.sh
$ vim spark-env.sh
# add the following line
export SCALA_HOME=/usr/lib/scala-2.9.3
# save and exit
</code></pre>

<p>In<code>conf/slaves</code>, add hostnames of Spark workers, one per line.</p>

<pre><code>$ vim slaves
slave01
slave02
# save and exit
</code></pre>

<p>(Optional)Set SPARK_HOME and add SPARK_HOME/bin to PATH</p>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.7.2
export PATH=$PATH:$SPARK_HOME/bin
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="install-and-configure-spark-on-workers">5.4 Install and configure Spark on workers</h2>
<p>Copy the spark directory to all slaves. <strong>Remark，the spark directories must locat at the the same path on all machines，because the master will login to work to execute spark commands, it assumes that workers have the same path as itself</strong></p>

<pre><code>$ cd
$ scp -r spark-0.7.2 dev@slave01:~
$ scp -r spark-0.7.2 dev@slave02:~
</code></pre>

<p>Set <code>SPARK_EXAMPLES_JAR</code>  on all slaves as section 5.3. There is no need to edit configuration files because they are copied from master, which are already well configured.</p>

<h2 id="start-spark-cluster">5.5 Start Spark cluster</h2>
<p>On master</p>

<pre><code>$ cd ~/spark-0.7.2
$ bin/start-all.sh
</code></pre>

<p>Check whether the processes have been started.</p>

<pre><code>$ jps
11055 Jps
2313 SecondaryNameNode
2409 JobTracker
2152 NameNode
4822 Master
</code></pre>

<p>Look at the master’s web UI (<a href="http://localhost:8080">http://localhost:8080</a> by default). You should see the new node listed there, along with its number of CPUs and memory (minus one gigabyte left for the OS).</p>

<h2 id="run-the-sparkpi-example-in-cluster-mode">5.6 run the SparkPi example in cluster mode</h2>

<pre><code>$ cd ~/spark-0.7.2
$ ./run spark.examples.SparkPi spark://master:7077
</code></pre>

<p>(Optional)Run built-in examples, SparkLR and SparkKMeans.</p>

<pre><code>#Logistic Regression
#./run spark.examples.SparkLR spark://master:7077
#kmeans
$ ./run spark.examples.SparkKMeans spark://master:7077 ./kmeans_data.txt 2 1
</code></pre>

<h2 id="read-files-from-hdfs-and-run-wordcount">5.7 read files from HDFS and run WordCount</h2>

<pre><code>$ cd ~/spark-0.7.2
$ hadoop fs -put README.md .
$ MASTER=spark://master:7077 ./spark-shell
scala&gt; val file = sc.textFile("hdfs://master:9000/user/dev/README.md")
scala&gt; val count = file.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey(_+_)
scala&gt; count.collect()
</code></pre>

<h2 id="stop-spark-cluster">5.8 Stop Spark cluster</h2>

<pre><code>$ cd ~/spark-0.7.2
$ bin/stop-all.sh
</code></pre>

<h1 id="references">References</h1>
<ol>
  <li><a href="http://spark-project.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li>
  <li><a href="https://github.com/mesos/spark/wiki/Running-A-Spark-Standalone-Cluster">Running A Spark Standalone Cluster</a></li>
  <li><a href="http://sprism.blogspot.com/2012/11/lightning-fast-wordcount-using-spark.html">Lightning-Fast WordCount using Spark Alongside Hadoop</a></li>
</ol>

<p>The following posts are outdated.</p>

<ol>
  <li><a href="http://chapeau.freevariable.com/2013/04/installing-spark-on-fedora-18.html">Installing Spark on Fedora 18</a></li>
  <li><a href="http://rdc.taobao.com/team/jm/archives/1823">Spark随谈（二）—— 安装攻略</a></li>
  <li><a href="http://www.cnblogs.com/jerrylead/archive/2012/08/13/2636115.html">Spark安装与学习</a></li>
</ol>
]]></content>
  </entry>
  
</feed>
