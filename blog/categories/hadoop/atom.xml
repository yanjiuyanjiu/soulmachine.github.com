<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | 研究研究]]></title>
  <link href="http://www.yanjiuyanjiu.com/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://www.yanjiuyanjiu.com/"/>
  <updated>2014-03-20T16:36:10+08:00</updated>
  <id>http://www.yanjiuyanjiu.com/</id>
  <author>
    <name><![CDATA[soulmachine]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[CentOS上编译 Hadoop 2.2.0]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140214"/>
    <updated>2014-02-14T11:56:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/compile-hadoop-220-on-centos</id>
    <content type="html"><![CDATA[<p>下载了Hadoop预编译好的二进制包，hadoop-2.2.0.tar.gz，启动起来后，总是出现这种警告：</p>

<pre><code>WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</code></pre>

<p>原因是apache官网提供的二进制包，里面的native库，是32位的，坑跌啊，现在服务器谁还有32位的啊。</p>

<pre><code>$ file $HADOOP_PREFIX/lib/native/libhadoop.so.1.0.0
libhadoop.so.1.0.0: ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, BuildID[sha1]=0x9eb1d49b05f67d38454e42b216e053a27ae8bac9, not stripped
</code></pre>

<p>我们需要下载Hadoop 2.2.0源码，在 64 位Linux下重新编译，然后把32位的native库用64位的native库替换。</p>

<!-- more -->

<h2 id="hadoop-220-">1. 下载Hadoop 2.2.0 源码包，并解压</h2>

<pre><code>$ wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.2.0/hadoop-2.2.0-src.tar.gz
$ tar zxf hadoop-2.2.0-src.tar.gz
</code></pre>

<h2 id="section">2. 安装下面的软件</h2>

<pre><code> $ sudo yum install lzo-devel  zlib-devel  gcc autoconf automake libtool   ncurses-devel openssl-deve
</code></pre>

<h2 id="maven">3. 安装Maven</h2>
<p>不要使用最新的Maven 3.1.1。Hadoop 2.2.0的源码与Maven3.x存在兼容性问题，所以会出现</p>

<pre><code>java.lang.NoClassDefFoundError: org/sonatype/aether/graph/DependencyFilter
</code></pre>

<p>之类的错误。</p>

<p>安装 Maven 3.0.5</p>

<pre><code>$ wget http://mirror.esocc.com/apache/maven/maven-3/3.0.5/binaries/apache-maven-3.0.5-bin.tar.gz
$ sudo tar zxf apache-maven-3.0.5-bin.tar.gz -C /opt
$ sudo vim /etc/profile
export MAVEN_HOME=/opt/apache-maven-3.0.5
export PATH=$PATH:$MAVEN_HOME/bin
</code></pre>

<p>注销并重新登录，让环境变量生效。</p>

<h2 id="ant">4. 安装Ant</h2>

<pre><code>$ wget http://apache.dataguru.cn//ant/binaries/apache-ant-1.9.3-bin.tar.gz
$ sudo tar zxf apache-ant-1.9.3-bin.tar.gz -C /opt
$ sudo vim /etc/profile
export ANT_HOME=/opt/apache-ant-1.9.3
export PATH=$PATH:$ANT_HOME/bin
</code></pre>

<h2 id="findbugs">5. 安装Findbugs</h2>

<pre><code>$ wget http://prdownloads.sourceforge.net/findbugs/findbugs-2.0.3.tar.gz?download
$ sudo tar zxf findbugs-2.0.3.tar.gz -C /opt
$ sudo vim /etc/profile
export FINDBUGS_HOME=/opt/findbugs-2.0.3
export PATH=$PATH:$FINDBUGS_HOME/bin
</code></pre>

<h2 id="protobuf">6. 安装protobuf</h2>
<p>编译Hadoop 2.2.0，需要protobuf的编译器protoc。一定需要protobuf 2.5.0以上，yum里的是2.3，太老了。因此下载源码，编译安装。</p>

<pre><code>$ wget https://protobuf.googlecode.com/files/protobuf-2.5.0.tar.gz
$ tar zxf protobuf-2.5.0.tar.gz
$ cd protobuf-2.5.0
$ ./configure
$ make
$ sudo make install
</code></pre>

<h2 id="hadooppatch">7. 给Hadoop源码打一个patch</h2>
<p>最新的Hadoop 2.2.0 的Source Code 压缩包解压出来的code有个bug 需要patch后才能编译。否则编译hadoop-auth 会提示下面错误：</p>

<pre><code>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:testCompile (default-testCompile) on project hadoop-auth: Compilation failure: Compilation failure:
[ERROR] /home/chuan/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java:[84,13] cannot access org.mortbay.component.AbstractLifeCycle
[ERROR] class file for org.mortbay.component.AbstractLifeCycle not found
</code></pre>

<p>Patch: <a href="https://issues.apache.org/jira/browse/HADOOP-10110">https://issues.apache.org/jira/browse/HADOOP-10110</a></p>

<h2 id="hadoop">8. 编译 Hadoop</h2>

<pre><code>cd hadoop-2.2.0-src
mvn package -DskipTests -Pdist,native -Dtar
</code></pre>

<h2 id="native">9. 替换掉32位的native库</h2>
<p>用 <code>hadoop-2.2.0-src/hadoop-dist/target/hadoop-2.2.0/lib/native</code> 替换掉 <code>hadoop-2.2.0/lib/native</code>。</p>

<pre><code>rm -rf ~/local/opt/hadoop-2.2.0/lib/native
cp ./hadoop-dist/target/hadoop-2.2.0/lib/native ~/local/opt/hadoop-2.2.0/lib/
</code></pre>

<p>然后重启Hadoop集群，会看到控制台下不再有警告信息了。</p>

<h2 id="ubuntu">10 解决Ubuntu下启动失败的问题</h2>
<p>在Ubuntu上，那就不是一点WARN了，而是启动不起来，会出错，原因在于，在<code>./sbin/start-dfs.sh</code>第55行，</p>

<pre><code>NAMENODES=$($HADOOP_PREFIX/bin/hdfs getconf -namenodes)
</code></pre>

<p>在shell里单独运行这样命令，</p>

<pre><code>./bin/hdfs getconf -namenodes

OpenJDK 64-Bit Server VM warning: You have loaded library /home/soulmachine/local/opt/hadoop-2.2.0/lib/native/libhadoop.so which might have disabled stack guard. The VM will try to fix the stack guard now.
It's highly recommended that you fix the library with 'execstack -c &lt;libfile&gt;', or link it with '-z noexecstack'.
14/02/14 13:14:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
localhost
</code></pre>

<p>最后一行的localhost，才是有效的namenode，但是由于前面有一大堆warning，脚本把这一大堆字符串，按空格隔开，每个单词都看作是namenode，接下来就错的稀里哗啦。</p>

<p>根本原因，还是因为32位native库。</p>

<p>把自带的32位native目录删除，用编译好的64位native目录拷贝过去，再运行</p>

<pre><code>./bin/hdfs getconf -namenodes
localhost
</code></pre>

<p>这下就对了！</p>

<h2 id="section-1">参考资料</h2>

<ol>
  <li><a href="http://blog.csdn.net/lalaguozhe/article/details/10580727">YARN加载本地库抛出Unable to load native-hadoop library解决办法</a></li>
  <li><a href="http://blog.csdn.net/zwj0403/article/details/16855555">CentOS编译Hadoop 2.2.0 Pass 总结</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在CentOS上安装HBase 0.96]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140208"/>
    <updated>2014-02-08T16:42:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/install-hbase-on-centos</id>
    <content type="html"><![CDATA[<p>环境：CentOS 6.5, jdk 1.7, HBase 0.96.1.1</p>

<h2 id="ssh">（可选）创建新用户，并配置好SSH无密码登录</h2>
<p>一般我倾向于把需要启动daemon进程，对外提供服务的程序，即服务器类的程序，安装在单独的用户下面。这样可以做到隔离，运维方面，安全性也提高了。</p>

<p>创建一个新的group,</p>

<pre><code>$ sudo groupadd hbase
</code></pre>

<p>创建一个新的用户，并加入group,</p>

<pre><code>$ sudo useradd -g hbase hbase
</code></pre>

<p>给新用户设置密码，</p>

<pre><code>$ sudo passwd hbase
</code></pre>

<p>在每台机器上创建hbase新用户，并配置好SSH无密码，参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120102/">SSH无密码登录的配置</a></p>

<h2 id="standalone-mode">1. 单机模式(Standalone mode)</h2>

<h3 id="section">1.1 下载，解压</h3>

<pre><code>$ wget wget http://mirror.esocc.com/apache/hbase/hbase-0.96.1.1/hbase-0.96.1.1-hadoop2-bin.tar.gz
$ tar zxf hbase-0.96.1.1-hadoop2-bin.tar.gz -C ~/local/opt
</code></pre>

<h3 id="hbase-envsh">1.2 hbase-env.sh</h3>
<p>在这个文件中要指明JDK 安装在了哪里</p>

<pre><code>$ echo $JAVA_HOME
/usr/lib/jvm/java
$ vim conf/hbase-env.sh
</code></pre>

<p>取消<code>JAVA_HOME</code>那一行的注释，设置正确的JDK位置</p>

<pre><code>export JAVA_HOME=/usr/lib/jvm/java
</code></pre>

<h3 id="confhbase-sitexml">1.3 修改 conf/hbase-site.xml</h3>
<p>内容如下</p>

<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;/home/hbase/local/var/hbase&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
    &lt;value&gt;/home/hbase/local/var/zookeeper&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p><code>hbase.rootdir</code>目录是用来存放HBase的相关信息的，默认值是<code>/tmp/hbase-${user.name}/hbase</code>； <code>hbase.zookeeper.property.dataDir</code>目录是用来存放zookeeper（HBase内置了zookeeper）的相关信息的，默认值是<code>/tmp/hbase-${user.name}/zookeeper</code>。</p>

<h3 id="section-1">1.4 启动</h3>

<pre><code>$ ./bin/start-hbase.sh
starting Master, logging to logs/hbase-user-master-example.org.out
</code></pre>

<!-- more -->

<h3 id="hbase-shell">1.5 试用一下HBase shell</h3>
<p>$ ./bin/hbase shell
    2014-02-09 23:56:28,637 INFO  [main] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
    HBase Shell; enter ‘help<return>' for list of supported commands.
    Type "exit<return>" to leave the HBase Shell
    Version 0.96.1.1-hadoop2, rUnknown, Tue Dec 17 12:22:12 PST 2013</return></return></p>

<pre><code>hbase(main):001:0&gt;
</code></pre>

<p>创建一张名字为<code>test</code>的表，只有一个列，名为<code>cf</code>。为了验证创建是否成功，用<code>list</code>命令查看所有的table，并用<code>put</code>命令插入一些值。</p>

<pre><code>hbase(main):003:0&gt; create 'test', 'cf'
0 row(s) in 1.2200 seconds
hbase(main):003:0&gt; list 'test'
..
1 row(s) in 0.0550 seconds
hbase(main):004:0&gt; put 'test', 'row1', 'cf:a', 'value1'
0 row(s) in 0.0560 seconds
hbase(main):005:0&gt; put 'test', 'row2', 'cf:b', 'value2'
0 row(s) in 0.0370 seconds
hbase(main):006:0&gt; put 'test', 'row3', 'cf:c', 'value3'
0 row(s) in 0.0450 seconds
</code></pre>

<p>用<code>scan</code>命令扫描table，验证一下刚才的插入是否成功。</p>

<pre><code>hbase(main):007:0&gt; scan 'test'
ROW        COLUMN+CELL
row1       column=cf:a, timestamp=1288380727188, value=value1
row2       column=cf:b, timestamp=1288380738440, value=value2
row3       column=cf:c, timestamp=1288380747365, value=value3
3 row(s) in 0.0590 seconds
</code></pre>

<p>现在，disable并drop掉你的表，这会把上面的所有操作清零。</p>

<pre><code>hbase(main):012:0&gt; disable 'test'
0 row(s) in 1.0930 seconds
hbase(main):013:0&gt; drop 'test'
0 row(s) in 0.0770 seconds 
</code></pre>

<p>退出shell，</p>

<pre><code>hbase(main):014:0&gt; exit
</code></pre>

<h3 id="section-2">1.6 停止</h3>

<pre><code>$ ./bin/stop-hbase.sh
stopping hbase...............
</code></pre>

<h2 id="pseudo-distributed-mode">2 伪分布式模式(Pseudo-distributed mode)</h2>

<h3 id="section-3">前提</h3>

<p>HBase集群需要一个正在运行的zookeeper集群，要么用自带的，要么用外部的。</p>

<p>用自带的很方便，不需要任何其他操作。</p>

<p>如果用外部的，要先安装并启动一个ZK集群，参考我的这篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20140207">在CentOS上安装ZooKeeper集群</a>。并在 conf/hbase-env.sh 里，设置<code>HBASE_MANAGES_ZK=false</code>。这个值默认为true，HBase自带了一个zk，启动HBase的时候也会先启动zk，如果把这个值设置为false，那么HBase就不会自己管理zk集群了。</p>

<p>一般用外部的zk。因为一般情况下，公司会在集群上安装好zookeeper集群，然后多个项目共用一个zk集群，有利于提高资源利用率。</p>

<p>HBase还需要一个正在运行的HDFS集群，如何搭建请参考我的这篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20140205">在CentOS上安装Hadoop 2.x 集群</a>。</p>

<h3 id="sshlocalhost">2.1 设置SSH无密码登录localhost</h3>
<p>先检查一下是能够无密码登录本机，</p>

<pre><code>ssh localhost
</code></pre>

<p>如果提示输入密码，说明不能，按如下步骤设置。</p>

<pre><code>$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa 
$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys
</code></pre>

<h3 id="section-4">2.2 下载，解压</h3>

<pre><code>$ wget wget http://mirror.esocc.com/apache/hbase/hbase-0.96.1.1/hbase-0.96.1.1-hadoop2-bin.tar.gz
$ tar zxf hbase-0.96.1.1-hadoop2-bin.tar.gz -C ~/local/opt
</code></pre>

<h3 id="hbase-envsh-1">2.3 hbase-env.sh</h3>
<p>在这个文件中要指明JDK 安装在了哪里</p>

<pre><code>$ echo $JAVA_HOME
/usr/lib/jvm/java
$ vim conf/hbase-env.sh
</code></pre>

<p>取消<code>JAVA_HOME</code>那一行的注释，设置正确的JDK位置</p>

<pre><code>export JAVA_HOME=/usr/lib/jvm/java
</code></pre>

<h3 id="confhbase-sitexml-1">2.4 conf/hbase-site.xml</h3>
<p>内容如下</p>

<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;/home/hadoop/local/var/hadoop/hbase&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;
    &lt;value&gt;2181&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;zk01, zk02, zk03&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
    &lt;value&gt;/home/zookeeper/local/var/zookeeper&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p><code>hbase.rootdir</code>是HBase存放数据的目录，这个值应该从Hadoop集群的core-site.xml里的<code>fs.defaultFS</code>或<code>fs.default.name</code>拷贝过来。</p>

<p>接下来关于ZooKeeper的三项配置都是从ZooKeeper集群的zoo.cfg里拷贝过来的。</p>

<h3 id="section-5">2.5 启动</h3>

<pre><code>$ ./bin/start-hbase.sh
starting Master, logging to logs/hbase-user-master-example.org.out
</code></pre>

<p>查看一下进程，</p>

<pre><code>$ jps
26142 HMaster
26255 HRegionServer
26360 Jps
</code></pre>

<p>启动了一个HMaster和一个HRegionServer。</p>

<h3 id="hbase-shell-1">2.6 试用一下HBase shell</h3>
<p>见第1.5节。</p>

<h3 id="section-6">2.7 停止</h3>

<pre><code>$ ./bin/stop-hbase.sh
stopping hbase...............
</code></pre>

<h2 id="fully-distributed-mode">3 完全分布式模式(Fully-distributed mode)</h2>

<h3 id="section-7">3.1 准备3台机器</h3>
<p>跟这篇文章<a href="http://www.yanjiuyanjiu.com/blog/20140205/">在CentOS上安装Hadoop 2.x 集群</a>的第2.1节很类似。</p>

<p>设3台机器的hostname分别是master, slave01, slave02, master作为HMaster，而slave01,slave02作为HRegionServer。</p>

<h3 id="master-master">3.2 配置 master 无密码登陆到所有机器（包括master自己登陆自己）</h3>
<p>参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120102/">SSH无密码登录的配置</a></p>

<h3 id="hbase">3.3 把HBase压缩包上传到所有机器，并解压</h3>
<p>将 hbase-0.96.1.1-hadoop2-bin.tar.gz 上传到所有机器，然后解压。<strong>注意，所有机器的hbase路径必须一致，因为master会登陆到slave上执行命令，master认为slave的hbase路径与自己一样。</strong></p>

<p>下面开始配置，配置好了后，把<code>conf/</code>目录scp到所有其他机器。</p>

<h3 id="section-8">3.4 修改配置文件</h3>
<p>在第2节的基础上，增加下列修改。</p>

<h4 id="confregionservers">3.4.1 conf/regionservers</h4>
<p>在这个文件里面添加slave，一行一个。</p>

<pre><code>slave01
slave01
</code></pre>

<h4 id="confslaves">3.4.2 将conf/目录拷贝到所有slaves</h4>

<pre><code>$ scp -r conf/ hbase@slave01:$HBASE_HOME/
$ scp -r conf/ hbase@slave02:$HBASE_HOME/
</code></pre>

<h3 id="hbase-1">3.5 启动HBase集群</h3>

<h4 id="section-9">3.5.1 启动</h4>
<p>在master上执行：</p>

<pre><code>$ ./bin/start-hbase.sh
</code></pre>

<h4 id="section-10">3.5.2 检查是否启动成功</h4>
<p>用<code>jps</code>查看java进程。</p>

<p>在master上，应该有一个HMaster进程，在每台slave上，应该有一个HRegionServer进程。</p>

<h4 id="web-ui">3.5.3 Web UI</h4>

<ul>
  <li>HMaster: <a href="http://master:60010">http://master:60010</a></li>
  <li>HRegionServer: <a href="http://slave:60030">http://slave:60030</a></li>
</ul>

<h2 id="section-11">4 客户端</h2>
<p>想要在另一台机器，或者另一个用户下访问HBase，怎么办？把hbase的安装目录整个拷贝过来即可，不用任何配置（跟Hadoop想比简单多了）。</p>

<p>运行<code>./bin/hbase shell</code>，就可以使用HBase集群了。</p>

<h2 id="section-12">参考资料</h2>

<ol>
  <li><a href="http://hbase.apache.org/book/quickstart.html">1.2. Quick Start</a></li>
  <li><a href="http://hbase.apache.org/book/standalone_dist.html">2.2 HBase run modes: Standalone and Distributed</a></li>
  <li><a href="http://hbase.apache.org/book/example_config.html">2.4. Example Configurations</a></li>
  <li><a href="http://hbase.apache.org/book/zookeeper.html">Chapter 17. ZooKeeper</a></li>
  <li><a href="http://blog.csdn.net/iam333/article/details/16358087">CentOS分布式环境安装HBase-0.96.0</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在CentOS上安装ZooKeeper集群]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140207"/>
    <updated>2014-02-07T23:40:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/install-zookeeper-on-centos</id>
    <content type="html"><![CDATA[<p>环境：CentOS 6.5, jdk 1.7, ZooKeeper 3.4.5</p>

<p>本文主要参考官网的<a href="http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html">Getting Started</a></p>

<h2 id="section">（可选）创建新用户</h2>
<p>一般我倾向于把需要启动daemon进程，对外提供服务的程序，即服务器类的程序，安装在单独的用户下面。这样可以做到隔离，运维方面，安全性也提高了。</p>

<p>创建一个新的group,</p>

<pre><code>$ sudo groupadd zookeeper
</code></pre>

<p>创建一个新的用户，并加入group,</p>

<pre><code>$ sudo useradd -g zookeeper zookeeper
</code></pre>

<p>给新用户设置密码，</p>

<pre><code>$ sudo passwd zookeeper
</code></pre>

<h2 id="standalone-mode">1. 单机模式(Standalone mode)</h2>
<p>单机模式在开发和调试阶段很有用。</p>

<h3 id="section-1">1.1 下载，解压</h3>

<pre><code>$ wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.5/zookeeper-3.4.5.tar.gz
$ tar zxf zookeeper-3.4.5.tar.gz -C ~/local/opt
</code></pre>

<h3 id="section-2">1.2 启动</h3>
<p>默认就是单机模式，</p>

<pre><code>$ mv conf/zoo_sample.cfg conf/zoo.cfg
$ ./bin/zdServer.sh start
</code></pre>

<h3 id="java-zookeeper">1.3 使用java 客户端连接ZooKeeper</h3>

<pre><code>$ ./bin/zkCli.sh -server 127.0.0.1:2181
</code></pre>

<p>然后就可以使用各种命令了，跟文件操作命令很类似，输入<code>help</code>可以看到所有命令。</p>

<h4 id="section-3">1.4 关闭</h4>

<pre><code>$ ./bin/zdServer.sh stop
</code></pre>

<h2 id="replicated-mode">2. 分布式模式(Replicated mode)</h2>
<p>在生产环境中，要配置成分布式模式，才能发挥威力。</p>

<!-- more -->

<p>ZooKeeper集群一般被称为ZooKeeper ensemble，或者  quorum.</p>

<h3 id="section-4">2.1 准备3台机器</h3>
<p>假设有三台机器，hostname和ip对应关系是：</p>

<pre><code>192.168.1.131 zk01
192.168.1.132 zk02
192.168.1.133 zk03
</code></pre>

<p>ZooKeeper不存在明显的master/slave关系，各个节点都是服务器，leader挂了，会立马从follower中选举一个出来作为leader.</p>

<p>由于没有主从关系，也不用配置SSH无密码登录了，各个zk服务器是自己启动的，互相之间通过TCP端口来交换数据。</p>

<h3 id="confzoocfg">2.2 修改配置文件conf/zoo.cfg</h3>

<pre><code>tickTime=2000
initLimit=10
syncLimit=5
dataDir=/home/zookeeper/local/var/zookeeper
clientPort=2181
server.1=zk01:2888:3888
server.2=zk02:2888:3888
server.3=zk03:2888:3888
</code></pre>

<p>我一般把服务器程序，即需要启动daemon进程的程序，放在单独的用户里安装；且用户的数据，放在<code>local/var</code>下面，所以我的dataDir是<code>/home/zookeeper/local/var/zookeeper</code>。</p>

<h3 id="myid">2.3 myid文件</h3>
<p>要在每台机器的dataDir下，新建一个myid文件，里面存放一个数字，用来标识当前主机。</p>

<pre><code>zookeeper@zk01:$ echo "1" &gt;&gt; ~/local/var/zookeeper/myid
zookeeper@zk02:$ echo "2" &gt;&gt; ~/local/var/zookeeper/myid
zookeeper@zk03:$ echo "3" &gt;&gt; ~/local/var/zookeeper/myid
</code></pre>

<h3 id="section-5">2.4 启动每台机器</h3>

<pre><code>zookeeper@zk01:$ ~/local/opt/zookeeper-3.4.5/bin/zkServer.sh start
zookeeper@zk02:$ ~/local/opt/zookeeper-3.4.5/bin/zkServer.sh start
zookeeper@zk03:$ ~/local/opt/zookeeper-3.4.5/bin/zkServer.sh start
</code></pre>

<p>因为3个节点的启动是有顺序的，所以在陆续启动三个节点的时候，前面先启动的节点连接未启动的节点的时候会报出一些错误。可以忽略。</p>

<h3 id="section-6">2.5 查看状态</h3>

<pre><code>$ ~/local/opt/zookeeper-3.4.5/bin/zkServer.sh status
</code></pre>

<h2 id="javazookeeper">3 使用java客户端连接ZooKeeper集群</h2>

<p>找一台机器，解压zookeeper压缩包，不用配置，就可以使用java客户端连接ZooKeeper集群中的任意一台服务器了。</p>

<pre><code>$ ./bin/zkCli.sh -server zk01:2181
$ ./bin/zkCli.sh -server zk01:2181
$ ./bin/zkCli.sh -server zk01:2181
</code></pre>

<p>连接上以后，就可以执行各种命令，使用<code>help</code>查看帮助。</p>

<h2 id="section-7">参考资料</h2>

<ol>
  <li><a href="http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html">Getting Started</a></li>
  <li><a href="http://blog.csdn.net/jmy99527/article/details/17582349">Zookeeper 3.4.5 集群安装笔记</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop多用户的配置(Hadoop 2.x)]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140206"/>
    <updated>2014-02-06T10:05:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/hadoop-multiple-users</id>
    <content type="html"><![CDATA[<p>假设我们以名为hadoop的用户，建好了集群，见<a href="http://www.yanjiuyanjiu.com/blog/20140205/">在CentOS上安装Hadoop 2.x 集群</a>。通常，我们会把这个集群共享给多个用户，而不是让大家都登录为hadoop，这样做有几个好处：</p>

<ul>
  <li>一个用户不能修改另一个用户的的文件</li>
  <li>在hadoop web管理页面，可以很方便的看到不同的用户的job</li>
</ul>

<p>现在集群中有一台机器，上面有一个用户名为 hbase 的用户，他想要使用hadoop集群，怎么配置呢？</p>

<h2 id="hadoop">1. 安装hadoop客户端</h2>

<h3 id="section">1.1 下载，解压</h3>
<p>下载跟hadoop集群一样的hadoop软件包，并解压，</p>

<pre><code>$ wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.2.0/hadoop-2.2.0.tar.gz
$ tar -zxf hadoop-2.2.0.tar.gz -C ~/local/opt
$ cd ~/local/opt/hadoop-2.2.0
</code></pre>

<h3 id="hadoop-1">1.2 拷贝Hadoop集群的配置文件</h3>
<p>将Hadoop集群的配置文件全部拷贝到客户端，相当与把集群的信息告诉客户端。</p>

<pre><code>$ scp -r hadoop@localhost:~/local/opt/hadoop-2.2.0/etc/hadoop ./etc/
</code></pre>

<p>修改conf/mapred-site.xml中的<code>mapreduce.cluster.local.dir</code>，改为本机上的某个目录，确保这个目录存在且有写权限。因为这个目录是本地目录，每台机器都可以不同。例如我的是：</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;mapreduce.cluster.local.dir&lt;/name&gt;
  &lt;value&gt;/home/soulmachine/local/var/hadoop/mapred/local&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>确保这个目录存在，</p>

<pre><code>$ mkdir -p ~/local/var/hadoop/mapred/local
</code></pre>

<!-- more -->

<p>还有另一种方法，由于<code>mapreduce.cluster.local.dir</code>默认值是<code>${hadoop.tmp.dir}/mapred/local</code>，也可以通过修改<code>hadoop.tmp.dir</code>达到目的，在<code>core-site.xml</code>中，确保<code>${hadoop.tmp.dir}/mapred/local</code>存在且有写权限。</p>

<h2 id="master">2. 在master上配置权限</h2>
<p>以下操作均在hadoop集群的 namenode 这台机器上进行，且登录为hadoop，因为hadoop这个用户是整个hadoop集群权限最高的用户（但对于Linux系统本身，这个用户其实没有sudo权限）。</p>

<p>Hadoop关于用户权限方面，有很多高级的配置，这里我们简单的利用HDFS本身的文件权限检查机制，来配置多用户。</p>

<p>HDFS本身没有提供用户名、用户组的创建，在客户端调用hadoop 的文件操作命令时，hadoop 识别出执行命令所在进程的linux系统的用户名和用户组，然后使用这个用户名和组来检查文件权限。 用户名=linux命令中的<code>whoami</code>，而组名等于<code>groups</code>。 </p>

<p>启动hadoop hdfs系统的用户即为超级用户（在这里就是名为hadoop的这个用户），可以进行任意的操作。</p>

<p>在客户端机器上，用gropus命令看一下hbase所在的组，</p>

<pre><code>$ groups
hbase
</code></pre>

<p>说明hbase这个用户所在的组为hbase。</p>

<h3 id="home">2.1 为客户端用户创建home文件夹</h3>

<pre><code>$ hdfs dfs -mkdir /user/hbase
$ hdfs dfs -chown hbase /user/hbase
$ hdfs dfs -chgrp hbase /user/hbase
</code></pre>

<h3 id="hdfstmp">2.2 设置HDFS上的/tmp目录的权限</h3>
<p>客户端提交job的时候，需要往/tmp里写入文件，因此最好把/tmp设置为所有用户都有可读、可写和可执行的权限。 </p>

<pre><code>$ hdfs dfs -chmod -R 777 /tmp
</code></pre>

<h3 id="mapreducejobtrackerstagingrootdir">2.3 设置mapreduce.jobtracker.staging.root.dir</h3>
<p>客户端向集群提交任务时，需要把该job需要的文件打包，拷贝到HDFS上。在拷贝之前，得先确定这些资源文件存放在HDFS的什么地方。JobTracker设置有一个工作目录(Staging area, 也称数据中转站)，用来存储与每个job相关的数据。这个目录的前缀由<code>mapreduce.jobtracker.staging.root.dir</code> 参数来指定，默认是<code>${hadoop.tmp.dir}/mapred/staging</code>，每个client user可以提交多个job，在这个目录后就得附加user name的信息。所以这个工作目录(Staging area)默认是<code>${hadoop.tmp.dir}/mapred/staging/denny/.staging/</code>。</p>

<p>一般把前缀设置为<code>/user</code>，这是官方推荐的，见 <a href="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default.xml</a> 里的<code>mapreduce.jobtracker.staging.root.dir</code>处：</p>

<blockquote>
  <p>The root of the staging area for users’ job files In practice, this should be the directory where users’ home directories are located (usually /user)</p>
</blockquote>

<pre><code>#以hadoop用户登录jobtracker机器
$ vim conf/mapred-site.xml
&lt;property&gt;
  &lt;name&gt;mapreduce.jobtracker.staging.root.dir&lt;/name&gt;
  &lt;value&gt;/user&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h3 id="hadoop-2">2.4 重启hadoop集群</h3>
<p>将配置文件scp到所有机器，然后重启集群，</p>

<pre><code>$ ./sbin/stop-yarn.sh
$ ./sbin/start-yarn.sh
$ ./sbin/stop-dfs.sh
$ ./sbin/start-dfs.sh
</code></pre>

<h2 id="section-1">3. 测试一下</h2>
<p>回到客户端机器。</p>

<p>将输入数据拷贝到分布式文件系统中:</p>

<pre><code>$ ./bin/hdfs dfs -put /etc/hadoop input
$ ./bin/hdfs dfs -ls input
</code></pre>

<p>运行 Hadoop 自带的例子:</p>

<pre><code>$ ./bin/$ hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount input output
</code></pre>

<p>查看输出文件:</p>

<pre><code>$ hdfs dfs -lsr output
$ hdfs dfs -cat output/part-r-00000
</code></pre>

<p>如果能看到结果，说明这个例子运行成功。</p>

<h3 id="binpath">4 （可选）将bin目录加入PATH</h3>
<p>这样就不用每次都cd到Hadoop目录，执行命令了。</p>

<p>在 <code>~/.bashrc</code>中添加如下4行：</p>

<pre><code>export HADOOP_PREFIX=$HOME/local/opt/hadoop-2.2.0
export PATH=$PATH:$HADOOP_PREFIX/bin
alias hls="hdfs dfs -ls"
</code></pre>

<p>source使之立刻生效，</p>

<pre><code>$ source ~/.bashrc
</code></pre>

<h2 id="section-2">参考资料</h2>

<ol>
  <li><a href="http://blog.csdn.net/j3smile/article/details/7887826">hadoop远程客户端安装配置、多用户权限配置</a></li>
  <li><a href="http://blog.csdn.net/a999wt/article/details/8718707">hadoop如何创建多用户</a></li>
  <li><a href="http://blog.sina.com.cn/s/blog_605f5b4f0101897z.html">关于多用户时hadoop的权限问题</a></li>
  <li><a href="http://langyu.iteye.com/blog/909170">MapReduce: Job提交过程</a></li>
  <li><a href="http://www.hadoopor.com/archiver/tid-481.html">hadoop中的dfs.name.dir,mapred.local.dir,mapred.system.dir和hadoop.tmp.dir说明</a></li>
  <li><a href="http://fenriswolf.me/2012/08/06/hadoop-%E5%8F%83%E6%95%B8%E8%A8%AD%E5%AE%9A-mapred-site-xml/">Hadoop 參數設定 – mapred-site.xml</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在CentOS上安装Hadoop 2.x 集群]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140205"/>
    <updated>2014-02-05T12:39:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/hadoop-2-installatioin-on-centos</id>
    <content type="html"><![CDATA[<p><strong>环境</strong>：CentOS 6.5, OPenJDK 1.7, Hadoop 2.2.0</p>

<p>本文主要参考官网的文档，<a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/SingleCluster.html">Hadoop 2.2.0 Single Node Setup</a>， <a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/ClusterSetup.html">Hadoop 2.2.0  Cluster Setup</a></p>

<h2 id="section">（可选）创建新用户</h2>
<p>一般我倾向于把需要启动daemon进程，对外提供服务的程序，简单的说，就是服务器类程序，安装在单独的用户下面。这样可以做到隔离，运维方面，安全性也提高了。</p>

<p>创建一个新的group,</p>

<pre><code>$ sudo groupadd hadoop
</code></pre>

<p>创建一个新的用户，并加入group,</p>

<pre><code>$ sudo useradd -g hadoop hadoop
</code></pre>

<p>给新用户设置密码，</p>

<pre><code>$ sudo passwd hadoop
</code></pre>

<h2 id="pseudo-distributed-mode">1 伪分布式模式(Pseudo-Distributed Mode)</h2>
<p>Hadoop能在单台机器上以伪分布式模式运行，即每个Hadoop模块运行在单独的java进程里。</p>

<h3 id="sshlocalhost">1.1 设置SSH无密码登录localhost</h3>
<p>先检查一下是能够无密码登录本机，</p>

<pre><code>ssh localhost
</code></pre>

<p>如果提示输入密码，说明不能，按如下步骤设置。</p>

<pre><code>$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa 
$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys
</code></pre>

<h3 id="section-1">1.2 下载已经编译好的二进制包，解压</h3>
<p>用浏览器下载或wget,</p>

<pre><code>$ wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.2.0/hadoop-2.2.0.tar.gz
$ tar -zxf hadoop-2.2.0.tar.gz -C ~/local/opt
$ cd ~/local/opt/hadoop-2.2.0
</code></pre>

<h3 id="section-2">1.3 设置环境变量</h3>

<pre><code>$ vim ~/.bashrc
export HADOOP_PREFIX=$HOME/local/opt/hadoop-2.2.0
export HADOOP_COMMON_HOME=$HADOOP_PREFIX
export HADOOP_HDFS_HOME=$HADOOP_PREFIX
export HADOOP_MAPRED_HOME=$HADOOP_PREFIX
export HADOOP_YARN_HOME=$HADOOP_PREFIX
export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop
export PATH=$PATH:$HADOOP_PREFIX/bin:$HADOOP_PREFIX/sbin
</code></pre>

<!-- more -->

<h3 id="section-3">1.4 修改配置文件</h3>
<p>配置文件的位置在 <code>$HADOOP_PREIFIX/etc/hadoop</code>下面。</p>

<h3 id="hadoop-envsh">1.4.1 hadoop-env.sh</h3>
<p>在这个文件中要告诉hadoop JDK 安装在了哪里</p>

<pre><code>$ echo $JAVA_HOME
/usr/lib/jvm/java
$ vim conf/hadoop-env.sh
</code></pre>

<p>取消<code>JAVA_HOME</code>那一行的注释，设置正确的JDK位置</p>

<pre><code>export JAVA_HOME=/usr/lib/jvm/java
</code></pre>

<h4 id="hdfs">1.4.2 HDFS的配置</h4>
<p>为了简单，我们仍采用Hadoop 1.x中的HDFS工作模式（不配置HDFS Federation）。</p>

<p>core-site.xml:</p>

<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://localhost&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>hdfs-site.xml:</p>

<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
    &lt;value&gt;file:///home/hadoop/local/var/hadoop/hdfs/datanode&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
    &lt;value&gt;file:///home/hadoop/local/var/hadoop/hdfs/namenode&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;
    &lt;value&gt;file:///home/hadoop/local/var/hadoop/hdfs/namesecondary&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>Hadoop会自动创建目录。</p>

<h4 id="yarn">1.4.3 YARN的配置</h4>
<p>yarn-site.xml，不用修改，保持为空。</p>

<h4 id="mapreduce">1.4.4 MapReduce的配置</h4>
<p>Yarn不仅仅只支持MapReduce这种计算模式，还支持Spar, Tez, MPI等框架，因此，要显式地配置Yarn，使其支持mapreduce。在Hadoop 1.x中，只支持MapReduce，因此需要而且必须配置mapred-site.xml，到了Hadoop 2.x，MapReduce的配置是可选的。</p>

<p>在yarn-site.xml中添加：</p>

<pre><code>&lt;property&gt;
   &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
   &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p><strong>解释</strong>：为了能够运行MapReduce程序，需要让各个NodeManager在启动时加载shuffle server，shuffle server实际上是Jetty/Netty Server，Reduce Task通过该server从各个NodeManager上远程拷贝Map Task产生的中间结果。上面增加的这个配置用于指定shuffle server。</p>

<p>将 mapred-site.xml.template 复制一份，重命名为mapred-site.xml。</p>

<p>mapred-site.xml:</p>

<pre><code>&lt;property&gt;
   &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
   &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p><strong>解释</strong>：用mapreduce.framework.name指定采用的框架名称，默认是将作业提交到MRv1的JobTracker端。</p>

<h3 id="section-4">1.5 测试</h3>

<h4 id="hdfs-1">1.5.1 启动HDFS</h4>

<pre><code>$ hdfs namenode -format
$ start-dfs.sh
</code></pre>

<h4 id="yarn-1">1.5.2 启动Yarn</h4>

<pre><code>$ start-yarn.sh
</code></pre>

<h4 id="mapreduce-1">1.5.3 启动MapReduce</h4>
<p>在Hadoop 2.x中，MapReduce Job不需要额外的daemon进程，在MapReduce Application Master启动的时候会自动启动JobTracker和TaskTracker进程，Job结束的时候会自动被关闭。</p>

<h4 id="distributedshell">1.5.4 运行一个DistributedShell的例子</h4>
<p>运行一个Hadoop自带的例子，名称为<code>DistributedShell</code>，可以同时在多台机器上运行shell命令。</p>

<pre><code>$ hadoop jar $HADOOP_PREFIX/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.2.0.jar org.apache.hadoop.yarn.applications.distributedshell.Client --jar $HADOOP_PREFIX/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.2.0.jar --shell_command date --num_containers 2
</code></pre>

<p>运行完成后，看看倒数第三行，有类似与<code>application_1391783685869_0001</code>的字符串，这是application ID。查看该application的每个container的输出，执行下面的命令，</p>

<pre><code>$ grep "" $HADOOP_PREFIX/logs/userlogs/&lt;APPLICATION ID&gt;/**/stdout
</code></pre>

<h4 id="wordcount">1.5.5 运行wordcount</h4>

<pre><code>$ cd $HADOOP_PREFIX
$ hdfs dfs -put ./etc/hadoop/ input
$ hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount input output
$ hdfs dfs -lsr output
$ hdfs dfs -cat output/part-r-00000
</code></pre>

<p>结束后，关闭 Hadoop:</p>

<pre><code>$ stop-dfs.sh
$ stop-yarn.sh
</code></pre>

<h2 id="fully-distributed-mode">2 分布式模式(Fully-Distributed Mode)</h2>

<h3 id="section-5">2.1 准备3台机器</h3>
<p>如果你已经有了三台机器，这一步可以省略。</p>

<p>如果没有，则可以用VMware Workstation 或 VirtualBox创建3台虚拟机。首先用vmware workstation 新建一台CentOS 6.5，装好操作系统，选择 Basic Server，安装JDK，参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120423/">安装和配置CentOS服务器的详细步骤</a>。安装好后然后用<strong>浅拷贝</strong><code>Create a linked clone</code> 克隆出2台，这样有了3台虚拟机。启动3台机器，假设IP分别为<code>192.168.1.131, 192.168.1.132, 192.168.1.133</code>, 131做为NameNode,JobTracker和SecondaryNameNode，身兼3个角色，这3个角色应该放到3台不同的机器上，这里为了简化，用一台机器来做3个角色；132和133为 slave。假设三台机器上的用户名是<code>hadoop</code>，也可以用其他用户名，但必须三台机器都相同。</p>

<h4 id="section-6">2.1.1 关闭防火墙</h4>
<p>临时关闭防火墙</p>

<pre><code>$ sudo service iptables stop
</code></pre>

<p>下次开机后，防火墙还是会启动。</p>

<p>永久关闭防火墙</p>

<pre><code>$ sudo chkconfig iptables off
</code></pre>

<p>由于这几台虚拟机是开发机，不是生产环境，因此不必考虑安全性，可以永久关闭防火墙，还能给开发阶段带来很多便利。</p>

<h4 id="hostname">2.1.2 修改hostname</h4>
<p>如果集群中的每一台机器事先已经有了hostname，则这一步可以跳过。</p>

<p>这一步看起来貌似不必要，其实是必须的，否则最后运行wordcount等例子时，会出现“Too many fetch-failures”。因为HDFS用hostname而不是IP，来相互之间进行通信（见后面的注意1）。</p>

<p>在CentOS上修改hostname，包含两个步骤(假设将hostname1改为hostname2，参考<a href="http://www.ichiayi.com/wiki/tech/linux_hostname">这里</a>，但不需要第一步)：</p>

<ol>
  <li>将 <code>/etc/sysconfig/network</code> 內的 HOSTNAME 改成 yourhostname</li>
  <li>用<code>hostname</code>命令，临时修改机器名， <code>sudo hostname yourhostname</code></li>
</ol>

<p>用<code>exit</code>命令退出shell，再次登录，命令提示字符串就会变成<code>[username@yourhostname ~]$</code>。</p>

<p>用上述方法，将131改名为master，132改名为slave01，133改名为slave02。</p>

<p><strong>注意</strong>，对于有的Ubuntu/Debia 系统，会把本机的hostname解析成 127.0.1.1，例如：</p>

<pre><code>127.0.0.1       localhost
127.0.1.1       master
</code></pre>

<p>将第二行改为(参考<a href="http://wiki.ubuntu.org.cn/%E5%88%A9%E7%94%A8Cloudera%E5%AE%9E%E7%8E%B0Hadoop">利用Cloudera实现Hadoop</a>)</p>

<pre><code>127.0.0.1       master
</code></pre>

<h4 id="etchosts">2.1.3 修改所有机器的<code>/etc/hosts</code>文件</h4>
<p>在所有机器的<code>/etc/hosts</code>文件中，添加所有hostname对应的IP，一般在一台机器上设置好，然后scp到所有机器。例如</p>

<pre><code>192.168.1.131 master
192.168.1.132 slave01
192.168.1.133 slave02
</code></pre>

<h3 id="master-master">2.2 配置 master 无密码登陆到所有机器（包括master自己登陆自己）</h3>
<p>参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120102/">SSH无密码登录的配置</a></p>

<h3 id="hadoop">2.3 把Hadoop压缩包上传到所有机器，并解压</h3>
<p>将 hadoop-1.2.1-bin.tar.gz 上传到所有机器，然后解压。<strong>注意，所有机器的hadoop路径必须一致，因为master会登陆到slave上执行命令，master认为slave的hadoop路径与自己一样。</strong></p>

<p>下面开始配置，配置好了后，把<code>./etc/hadoop</code>目录scp到所有其他机器。</p>

<h3 id="section-7">2.4 修改配置文件</h3>
<p>在第1节的基础上，增加下列修改。</p>

<h4 id="namenode">2.4.1 指定NameNode</h4>
<p>在core-site.xml中，<code>fs.defaultFS</code>要改为运行NameNode的那台机器的hostname，不再是localhost。</p>

<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://master&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<h4 id="resourcemanager">2.4.2 指定ResourceManager</h4>
<p>在yarn-site.xml中增加，</p>

<pre><code>&lt;property&gt;
   &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
   &lt;value&gt;master&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h4 id="slavenodemanager">2.4.3 添加Slave，即NodeManager</h4>
<p>在 <code>etc/hadoop/slaves</code>中添加，</p>

<pre><code>slave01
slave02
</code></pre>

<h4 id="hadooptmpdir">2.4.4 设置 hadoop.tmp.dir</h4>
<p>在core-site.xml里添加：</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
  &lt;value&gt;/home/hadoop/local/var/hadoop/tmp/hadoop-${user.name}&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h4 id="mapred-sitexml">2.4.4 修改mapred-site.xml</h4>
<p>添加如下内容：</p>

<pre><code>&lt;property&gt;
    &lt;name&gt;mapreduce.jobtracker.staging.root.dir&lt;/name&gt;
    &lt;value&gt;/user&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>这是为以后的多用户支持做准备。</p>

<h4 id="pid">2.4.4 设置pid文件的存放位置</h4>
<p>在hadoop-env.sh中添加</p>

<pre><code>export HADOOP_MAPRED_PID_DIR=/home/hadoop/local/var/hadoop/pids
</code></pre>

<p>在 mapred-env.sh中添加</p>

<pre><code>export HADOOP_PID_DIR=/home/hadoop/local/var/hadoop/pids
</code></pre>

<h4 id="dfsreplicationslave">2.4.5 将dfs.replication设置为slave的个数</h4>
<p>我们这里有2台slave，就设置为2。在hdfs-site.xml里添加：</p>

<pre><code>&lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;2&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h4 id="slaves">2.4.6 将配置文件拷贝到所有slaves</h4>

<pre><code>$ cd $HADOOP_PREFIX/etc/hadoop
$ scp hadoop-env.sh mapred-env.sh core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml slaves hadoop@slave01:$HADOOP_PREFIX/etc/hadoop
$ scp hadoop-env.sh mapred-env.sh core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml slaves hadoop@slave02:$HADOOP_PREFIX/etc/hadoop
</code></pre>

<h3 id="section-8">2.5 设置环境变量</h3>
<p>在所有机器上添加环境变量，与第1.3节相同。</p>

<h3 id="hadoop-1">2.6 启动 hadoop</h3>

<h4 id="hdfs-2">2.6.1 启动HDFS</h4>
<p>在NameNode这个机器（在这里是master）上执行下列命令，</p>

<pre><code>#只需一次，下次启动不再需要格式化，只需 start-dfs.sh
$ hdfs namenode -format
$ start-dfs.sh
</code></pre>

<h4 id="yarn-2">2.6.2 启动Yarn</h4>
<p>在ResourceManager这台机器（在这里仍然是master）上执行，</p>

<pre><code>$ start-yarn.sh
</code></pre>

<h4 id="mapreduce-2">2.6.3 启动MapReduce</h4>
<p>在Hadoop 2.x中，MapReduce Job不需要额外的daemon进程，在Job开始的时候，NodeManager会启动一个MapReduce Application Master（相当与一个精简的JobTracker），Job结束的时候自动被关闭。</p>

<h4 id="section-9">2.6.4 检查是否启动成功</h4>
<p>用<code>jps</code>查看java进程。</p>

<p>在master上，应该有三个进程，NameNode, SecondaryNameNode, ResourceManger；在每台slave上，应该有两个进程，DataNode, NodeManager。</p>

<h4 id="web-ui">2.6.5 Web UI</h4>
<p>可以用浏览器打开NameNode, ResourceManager和各个NodeManager的web界面，</p>

<ul>
  <li>NameNode web UI, <a href="http://master:50070/">http://master:50070/</a></li>
  <li>ResourceManager web UI, <a href="http://master:8088/">http://master:8088/</a></li>
  <li>NodeManager web UI, <a href="http://slave01:8042">http://slave01:8042</a></li>
</ul>

<p>还可以启动JobHistory Server，能够通过Web页面查看集群的历史Job，执行如下命令：</p>

<pre><code>mr-jobhistory-daemon.sh start historyserver
</code></pre>

<p>默认使用19888端口，通过访问<a href="http://master:19888/">http://master:19888/</a>查看历史信息。</p>

<p>终止JobHistory Server，执行如下命令：</p>

<pre><code>mr-jobhistory-daemon.sh stop historyserver
</code></pre>

<h3 id="wordcount-1">2.8 运行wordcount</h3>
<p>将输入数据拷贝到HDFS中:</p>

<pre><code>$ cd $HADOOP_PREFIX
$ hdfs dfs -put ./etc/hadoop input
</code></pre>

<p>这一步会报错，”No such file or directory”, 用<code>hdfs dfs -ls /</code>查看，是空的，难怪了。我们需要手动建立”/user/hadoop”目录，</p>

<pre><code>$ hdfs dfs -mkdir /user/hadoop
</code></pre>

<p>再上传文件，就可以了。</p>

<p>运行WordCount:</p>

<pre><code>$ hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount input output
</code></pre>

<p>查看结果：</p>

<pre><code>$ hdfs dfs -lsr output
$ hdfs dfs -cat output/part-r-00000
</code></pre>

<p>如果能看到结果，说明这个例子运行成功。</p>

<h3 id="hadoop-2">2.9 停止 hadoop集群</h3>
<p>在master上执行：</p>

<pre><code>$ stop-yarn.sh
$ stop-hdfs.sh
</code></pre>

<h2 id="section-10">4. 排除错误</h2>
<p>本文已经尽可能的把步骤详细列出来了，但是我相信大部分人不会一次成功。这时候，查找错误就很重要了。查找错误最重要的手段是查看hadoop的日志，一般在logs目录下。把错误消息复制粘贴到google，搜索一下，慢慢找错误。</p>

<h2 id="section-11">参考资料</h2>

<ol>
  <li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/core-default.xml">core-default</a>, <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default</a>, <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default</a>, <a href="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default</a></li>
  <li><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-install/">Hadoop YARN安装部署初探</a></li>
  <li><a href="http://www.alexjf.net/blog/distributed-systems/hadoop-yarn-installation-definitive-guide">Hadoop YARN Installation: The definitive guide</a></li>
  <li><a href="http://codesfusion.blogspot.com/2013/10/setup-hadoop-2x-220-on-ubuntu.html">Setup newest Hadoop 2.x (2.2.0) on Ubuntu</a></li>
  <li><a href="http://shiyanjun.cn/archives/561.html">Hadoop-2.2.0集群安装配置实践</a></li>
  <li><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-configurations-resourcemanager-nodemanager/">Hadoop YARN配置参数剖析(1)—RM与NM相关参数</a></li>
</ol>

<h2 id="section-12">相关文章</h2>

<ol>
  <li><a href="http://www.yanjiuyanjiu.com/blog/20140202">在CentOS上安装Hadoop集群</a></li>
  <li><a href="http://www.yanjiuyanjiu.com/blog/20120103/">在Ubuntu上安装Hadoop</a></li>
</ol>

]]></content>
  </entry>
  
</feed>
