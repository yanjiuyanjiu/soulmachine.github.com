<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: machine-learning | 研究研究]]></title>
  <link href="http://www.yanjiuyanjiu.com/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://www.yanjiuyanjiu.com/"/>
  <updated>2013-11-08T20:48:21+08:00</updated>
  <id>http://www.yanjiuyanjiu.com/</id>
  <author>
    <name><![CDATA[soulmachine]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[机器学习的一些通俗易懂的tutorial]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20130327"/>
    <updated>2013-03-27T21:50:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/some-classical-machine-learning-tutorials</id>
    <content type="html"><![CDATA[<p>以下记录了我的学习历程，按我的阅读顺序排序。</p>

<h2 id="section">距离和相似度度量</h2>
<p><a href="http://webdataanalysis.net/reference-and-source/distance-and-similarity/">距离和相似度度量 » webdataanalysis.net</a></p>

<p><a href="http://www.zhihu.com/question/19640394">欧氏距离和余弦相似度的区别是什么？ – 知乎</a></p>

<h2 id="knnk-nearest-neighbor">KNN(K Nearest Neighbor)</h2>
<p><a href="http://coolshell.cn/articles/8052.html">K Nearest Neighbor 算法 _ 酷壳 – CoolShell</a></p>

<p><a href="http://en.wikipedia.org/wiki/KNN">K-nearest neighbors algorithm – Wikipedia</a></p>

<h2 id="k-means">K-Means</h2>
<p><a href="http://coolshell.cn/articles/7779.html">K-Means 算法 _ 酷壳 – CoolShell</a></p>

<p><a href="http://en.wikipedia.org/wiki/K-means">k-means clustering – Wikipedia</a></p>

<p><a href="http://kylen314.blog.com/2012/09/10/k-means/">K-Means++ _ 愈宅屋</a></p>

<p><a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/20/k-means.html">算法杂货铺——k均值聚类(K-means) – T2噬菌体 – 博客园</a></p>

<p><a href="http://blog.pluskid.org/?p=17">漫谈 Clustering (1)_ k-means « Free Mind</a></p>

<p><a href="http://www.codeproject.com/Articles/439890/Text-Documents-Clustering-using-K-Means-Algorithm">Text Documents Clustering using K-Means Algorithm – CodeProject</a></p>

<!-- more -->

<h2 id="pcaprincipal-components-analysis">PCA(Principal Components Analysis)</h2>
<p><a href="http://www.ce.yildiz.edu.tr/personal/songul/file/1097/principal_components.pdf">2002. Lindsay I Smith. A tutorial on Principal Components Analysis</a></p>

<h2 id="em-expectation-maximization">期望最大化(EM, Expectation Maximization)</h2>
<p><a href="http://www.seanborman.com/publications/EM_algorithm.pdf">2009. Sean Borman. The Expectation Maximization Algorithm A short tutorial</a></p>

<p>李航.《统计学习方法》，P155 第9章 EM算法及其推广. 2012.</p>

<h2 id="svm-support-vector-machines">支持向量机(SVM, Support Vector Machines)</h2>
<p><a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf">Andrew Ng. CS229 Lecture notes Support Vector Machines</a></p>

<h2 id="crf-conditional-random-field">条件随机场(CRF, Conditional Random Field)</h2>
<p><a href="http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/">Introduction to Conditional Random Fields</a></p>

<p>李航.《统计学习方法》，P192 第11章 条件随机场. 2012.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数值计算库与科学计算库]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20130226"/>
    <updated>2013-02-26T23:15:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/numerical-or-scientific-computation-library</id>
    <content type="html"><![CDATA[<h2 id="blas-">BLAS 接口</h2>
<p><a href="http://www.netlib.org/blas/">BLAS</a>, <a href="http://www.netlib.org/lapack/">LAPACK</a>, <a href="http://math-atlas.sourceforge.net/">ATLAS</a> 这些数值计算库的名字很类似，他们之间有什么关系呢？BLAS是一组线性代数运算接口，目前是事实上的标准，很多数值计算/科学计算都实现了这套接口。</p>

<p>BLAS定义了那些函数呢？可以查看<a href="http://www.netlib.org/blas/">官方文档</a>。</p>

<p>LAPACK是BLAS的第一个实现，是最老牌的数值计算库，用FORTRAN 77语言写的。LAPACK实现了BLAS接口，并扩充了一些功能。很多数值计算库/科学计算库底层调用了LAPACK。</p>

<p>很多硬件厂商都实现BLAS接口，例如<a href="http://software.intel.com/en-us/intel-mkl">Intel MKL</a>(Math Kernel Library), <a href="http://developer.amd.com/tools/cpu-development/amd-core-math-library-acml/">AMCL</a>(AMD Math Core Library)等。很多开源库也支持，例如ATLAS。</p>

<p>还有非常多的库实现了BLAS接口，见<a href="http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">Wikipedia BLAS</a> 的Implementations小节。</p>

<p>下面介绍一些各种语言常用的数值计算/科学计算库。</p>

<!-- more -->

<h2 id="cc">C/C++</h2>
<p>首先是Intel 的MKL 和 AMD 的AMCL，性能一流，不过是商业软件，价格昂贵。</p>

<p><a href="http://www.gnu.org/software/gsl/">GSL - GNU Scientific Library</a>，GNU实现的库，质量很高，不过是用纯C写的，用起来比较繁琐。</p>

<p><a href="http://arma.sourceforge.net/">Armadillo</a>，最新版 2013-02-20 Version 3.6.3</p>

<p><a href="http://itpp.sourceforge.net/">IT++</a>，最后版本是4.2,2010-09-21。</p>

<h2 id="java">Java</h2>
<p>这个页面<a href="http://math.nist.gov/javanumerics/">JavaNumerics page</a>专门收集了关于Java数值计算的库。</p>

<p><a href="https://code.google.com/p/java-matrix-benchmark/">java-matrix-benchmark</a>这个开源项目，比较了各类Java线性代数库的性能。</p>

<p>Java的数值计算库主要分为两类：Pure Java和Natie Wrapper。Pure Java是指用纯Java编写的，Native Wrapper是指该库底层调用了C++或Fortan编写的第三方库，上面封装了一层，提供了更有好的接口。</p>

<p>Pure Java的有：<a href="http://dsd.lbl.gov/~hoschek/colt/">Colt</a>, <a href="http://commons.apache.org/proper/commons-math/">Commons Math</a>, <a href="https://code.google.com/p/efficient-java-matrix-library/">EJML</a>, <a href="http://math.nist.gov/javanumerics/jama/">JAMA</a>, <a href="http://trove.starlight-systems.com/">Trove</a></p>

<p>Native Wrapper有：<a href="http://jblas.org">jblas</a>，<a href="https://github.com/fommil/matrix-toolkits-java">Matrix Toolkit Java</a></p>

<p>下面介绍一些影响力较大的java数值计算/科学计算库。</p>

<p><a href="http://commons.apache.org/proper/commons-math/">Commons Math</a>, 最新版本是3.1.1,2013年1月9号发布。这个库提供一些基本的数学运算，没有high-level的东西，例如矩阵，向量等，用起来会比较繁琐。</p>

<p><a href="http://math.nist.gov/javanumerics/jama/">JAMA</a>, 最新版是Version 1.0.3 (November 9, 2012)。</p>

<p><a href="http://acs.lbl.gov/software/colt/">Colt</a>，已经不更新了，最后版本是1.2.0，2004年9月发布的。</p>

<p>Apache Mahout使用了Colt作为high performance collections，见官方<a href="https://cwiki.apache.org/MAHOUT/mahout-collections.html">这个页面</a>，说“The implementation of Mahout Collections is derived from Cern Colt”，以及quora 这个帖子<a href="http://www.quora.com/Distributed-Algorithms/What-are-the-best-resources-for-distributed-numerical-analysis-matrix-algorithms">What are the best resources for distributed numerical analysis/matrix algorithms</a>。</p>

<h2 id="python">Python</h2>
<p>目前最有影响力的莫过于<a href="http://www.numpy.org/">NumPy</a>和<a href="http://www.scipy.org/">SciPy</a>。Amazon.com上可以搜到专门讲它们的书。</p>

<p>SciPy依赖NumPy，主要是在数值计算方面调用了NumPy。</p>

<h2 id="ruby">Ruby</h2>
<p><a href="http://sciruby.com/">SciRuby</a>, 是SciPy和NumPy的克隆，目前还在开发中。</p>

<h2 id="r">R</h2>
<p>R刚开始时是统计学家开发的语言，专门用于数理统计，现在功能不断增强，内置了很多数值计算和科学计算的功能。R在数据分析领域比较火。</p>

<h2 id="scala">Scala</h2>
<p>目前用google搜索 “scala numerical computing”，能找得到的就是<a href="http://code.google.com/p/scalalab/">ScalaLab</a>了。</p>

<h2 id="matlab">Matlab</h2>
<p>最后，别忘了Matlab是支持多语言调用的。</p>

<p>可以用Matlab生成DLL，给C/C++语言调用。其实，凡是能调用DLL的语言，都可以使用这个DLL，例如Python, Ruby等。</p>

<p>可以用<a href="http://www.mathworks.cn/products/javabuilder/">Matlab JavaBuilder</a>将m文件转换为jar文件，然后在java代码中就可以调用了。</p>

<h2 id="section">如何选择</h2>
<p>本文的重点在于选择一个高性能，同时又比较易用的库，即被让我们调用，用来写程序的库，不是一个集成环境或REPL环境。因此R和Matlab不在讨论范围内。R和Matlab用来做原型或前期Data Exploration比较适合。</p>

<p>选择一个工具（语言，框架，库等），要看其是否成熟。我个人的一些判断指标，主要有</p>

<ol>
  <li>有没有大厂商的支持（作为vendor之类的）；</li>
  <li>amazon.com上能否搜到书。</li>
</ol>

<p>从厂商的支持来看，几个主要的大厂商如 Intel，AMD和Apple都开发了自己的数学库。Python则有很成熟的NumPy，在Amazon上能搜到书，例如“SciPy and NumPy”， “NumPy Cookbook”。 因此，目前来看，C++和Python是比较成熟的方案。</p>

<h2 id="section-1">参考资料</h2>
<p><a href="http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">Wikipedia BLAS</a><br />
<a href="http://en.wikipedia.org/wiki/LAPACK">Wikipedia LAPACK</a><br />
<a href="http://blog.henix.info/blog/blas-lapack-do-matrix-operation.html">用 BLAS/LAPACK 编写矩阵运算程序</a><br />
<a href="https://wikis.utexas.edu/display/~cdupree/BLAS,+LAPACK,+ATLAS">BLAS, LAPACK, ATLAS</a><br />
<a href="http://hi.baidu.com/luckykele2012/item/6a3b25423018c40d6dc2f090">BLAS 和 LAPACK ，以及其他常用数值计算库</a><br />
<a href="http://fdatamining.blogspot.com/2011/10/any-numerical-computing-environment-on.html">Any numerical computing environment on Java platform</a><br />
<a href="http://www.myoutsourcedbrain.com/2009/04/c-libraries-for-numerical-processing.html">C++ Libraries for Scientific Computing</a><br />
<a href="http://stackoverflow.com/questions/3121139/scientific-library-options-for-c-or-c">Scientific Library Options for C or C++</a><br />
<a href="http://programmers.stackexchange.com/questions/138643/why-is-python-used-for-high-performance-scientific-computing-but-ruby-isnt">Why is Python used for high-performance/scientific computing (but Ruby isn’t)?</a>  </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KNN与K-Means的区别]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20130225"/>
    <updated>2013-02-25T23:41:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/differences-between-knn-and-kmeans</id>
    <content type="html"><![CDATA[<h2 id="knnk-nearest-neighbor">KNN(K-Nearest Neighbor)介绍</h2>
<p>Wikipedia上的<a href="http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">KNN词条</a>中有一个比较经典的图如下：</p>

<p><img src="http://yanjiuyanjiu-wordpress.stor.sinaapp.com/uploads/2013/02/022513_0955_KNNKMeans1.png"></p>

<p>KNN的算法过程是是这样的：</p>

<p>从上图中我们可以看到，图中的数据集是良好的数据，即都打好了label，一类是蓝色的正方形，一类是红色的三角形，那个绿色的圆形是我们待分类的数据。</p>

<p>如果K=3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形。</p>

<p>如果K=5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形。（参考 <a href="http://coolshell.cn/articles/8052.html">酷壳的 K Nearest Neighbor 算法</a>）</p>

<p>我们可以看到，KNN本质是基于一种数据统计的方法！其实很多机器学习算法也是基于数据统计的。</p>

<!-- more -->

<p>KNN是一种memory-based learning，也叫instance-based learning，属于lazy learning。即它没有明显的前期训练过程，而是程序开始运行时，把数据集加载到内存后，不需要进行训练，就可以开始分类了。</p>

<p>具体是每次来一个未知的样本点，就在附近找K个最近的点进行投票。</p>

<p>再举一个例子，Locally weighted regression (LWR)也是一种 memory-based 方法，如下图所示的数据集。</p>

<p><img src="http://yanjiuyanjiu-wordpress.stor.sinaapp.com/uploads/2013/02/022513_0955_KNNKMeans2.gif"></p>

<p>用任何一条直线来模拟这个数据集都是不行的，因为这个数据集看起来不像是一条直线。但是每个局部范围内的数据点，可以认为在一条直线上。每次来了一个位置样本x，我们在X轴上以该数据样本为中心，左右各找几个点，把这几个样本点进行线性回归，算出一条局部的直线，然后把位置样本x代入这条直线，就算出了对应的y，完成了一次线性回归。</p>

<p>也就是每次来一个数据点，都要训练一条局部直线，也即训练一次，就用一次。</p>

<p>LWR和KNN是不是很像？都是为位置数据量身定制，在局部进行训练。</p>

<h2 id="k-means">K-Means介绍</h2>
<p><img src="http://yanjiuyanjiu-wordpress.stor.sinaapp.com/uploads/2013/02/022513_0955_KNNKMeans3.jpg"></p>

<p>如图所示，数据样本用圆点表示，每个簇的中心点用叉叉表示。(a)刚开始时是原始数据，杂乱无章，没有label，看起来都一样，都是绿色的。(b)假设数据集可以分为两类，令K=2，随机在坐标上选两个点，作为两个类的中心点。(c-f)演示了聚类的两种迭代。先划分，把每个数据样本划分到最近的中心点那一簇；划分完后，更新每个簇的中心，即把该簇的所有数据点的坐标加起来去平均值。这样不断进行”划分—更新—划分—更新”，直到每个簇的中心不在移动为止。(图文来自Andrew ng的机器学习公开课)。</p>

<p>推荐关于K-Means的两篇博文，<a href="http://coolshell.cn/articles/7779.html">K-Means 算法 _ 酷壳</a>，<a href="http://blog.pluskid.org/?p=17">漫谈 Clustering (1)_ k-means pluskid</a>。</p>

<h2 id="knnk-means">KNN和K-Means的区别</h2>
<table style="border-collapse: collapse;" border="0">
<colgroup>
<col style="width: 277px;" />
<col style="width: 277px;" /></colgroup>
<tbody valign="top">
<tr>
<td style="padding-left: 7px; padding-right: 7px; border: solid 0.5pt;">
<p style="text-align: center;"><span style="font-size: 10pt;"><strong>KNN</strong></span></p>
</td>
<td style="padding-left: 7px; padding-right: 7px; border-top: solid 0.5pt; border-left: none; border-bottom: solid 0.5pt; border-right: solid 0.5pt;">
<p style="text-align: center;"><span style="font-size: 10pt;"><strong>K-Means</strong></span></p>
</td>
</tr>
<tr style="height: 85px;">
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: solid 0.5pt; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"><span style="font-size: 10pt;">1.KNN是分类算法<br />
</span><p></p>
<p><span style="font-size: 10pt;">2.监督学习<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">3.喂给它的数据集是带label的数据，已经是完全正确的数据</span></p>
</td>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: none; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"><span style="font-size: 10pt;">1.K-Means是聚类算法<br />
</span><p></p>
<p><span style="font-size: 10pt;">2.非监督学习<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">3.喂给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序</span></p>
</td>
</tr>
<tr>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: solid 0.5pt; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"><span style="font-size: 10pt;">没有明显的前期训练过程，属于memory-based learning</span></td>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: none; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"><span style="font-size: 10pt;">有明显的前期训练过程</span></td>
</tr>
<tr>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: solid 0.5pt; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"><span style="font-size: 10pt;">K的含义：来了一个样本x，要给它分类，即求出它的y，就从数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c</span></td>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: none; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"><span style="font-size: 10pt;">K的含义：K是人工固定好的数字，假设数据集合可以分为K个簇，由于是依靠人工定好，需要一点先验知识</span></td>
</tr>
<tr>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: solid 0.5pt; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"></td>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: none; border-bottom: solid 0.5pt; border-right: solid 0.5pt;"></td>
</tr>
<tr>
<td style="padding-left: 7px; padding-right: 7px; border-top: none; border-left: solid 0.5pt; border-bottom: solid 0.5pt; border-right: solid 0.5pt;" colspan="2"><span style="font-size: 10pt;">相似点：都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法，一般用KD树来实现NN。</span></td>
</tr>
</tbody>
</table>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[基于朴素贝叶斯的文本分类算法]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20100528"/>
    <updated>2010-05-28T17:15:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/text-classification-algorithm-based-on-naive-bayes</id>
    <content type="html"><![CDATA[<p>作者: 灵魂机器<br />
新浪博客：<a href="www.weibo.com/soulmachine">www.weibo.com/soulmachine</a><br />
作者博客：<a href="www.yanjiuyanjiu.com">www.yanjiuyanjiu.com</a></p>

<p><strong>摘要</strong>：常用的文本分类方法有支持向量机、K-近邻算法和朴素贝叶斯。其中朴素贝叶斯具有容易实现，运行速度快的特点，被广泛使用。本文详细介绍了朴素贝叶斯的基本原理，讨论了两种常见模型：多项式模型（MM）和伯努利模型（BM），实现了可运行的代码，并进行了一些数据测试。</p>

<p><strong>关键字</strong>：朴素贝叶斯；文本分类</p>

<p><strong>Text Classification Algorithm Based on Naive Bayes</strong><br />
<strong>Author</strong>: soulmachine<br />
<strong>Email</strong>：soulmachine@gmail.com<br />
<strong>Blog</strong>：<a href="www.yanjiuyanjiu.com">www.yanjiuyanjiu.com</a></p>

<p><strong>Abstract</strong>:Usually there are three methods for text classification: SVM、KNN and Naïve Bayes. Naïve Bayes is easy to implement and fast, so it is widely used. This article introduced the theory of Naïve Bayes and discussed two popular models: multinomial model(MM) and Bernoulli model(BM) in details, implemented runnable code and performed some data tests.</p>

<p><strong>Keywords</strong>: naïve bayes; text classification</p>

<h2 id="section">1 贝叶斯原理</h2>

<h3 id="section-1">1.1 贝叶斯公式</h3>

<p>设A、B是两个事件，且P(A)&gt;0，称 <script type="math/tex">P(Y \vert X)=\dfrac {P(XY)}{P(X)}</script> 为事件A发生的条件下事件B发生的<strong>条件概率</strong>。</p>

<p><strong>乘法公式</strong> <script type="math/tex">P(XYZ)=P(Z \vert XY)P(Y \vert X)P(X)</script><br />
<strong>全概率公式</strong>  <script type="math/tex">P(X)=P(X \vert Y_1)+ P(X \vert Y_2)+…+ P(X \vert Y_n)</script><br />
<strong>贝叶斯公式</strong>  <script type="math/tex">P(Y_i \vert X)=\dfrac{P(XY_i)}{P(X)}=\dfrac{P(X \vert Y_i)P(Y_i)}{P(X)}=\dfrac{P(X \vert Y_i)P(Y_i)}{\sum\limits _{j=1} ^{n} P(X \vert Y_j)}</script>  </p>

<p>在此处，贝叶斯公式，我们要用到的是 <script type="math/tex">P(Y_i \vert X)=\dfrac{P(X \vert Y_i)P(Y_i)}{P(X)}</script></p>

<p>以上公式，请读者参考<a href="http://book.douban.com/subject/1231189/">《概率论与数理统计（第五版）》</a>的1.4节“条件概率”（这里将原书中的A换成了X，B换成了Y），获得更深的理解。</p>

<!-- more -->

<h3 id="section-2">1.2 贝叶斯定理在分类中的应用</h3>
<p>在分类（classification）问题中，常常需要把一个事物分到某个类别。一个事物具有很多属性，把它的众多属性看做一个向量，即<script type="math/tex">x=(x_1,x_2,x_3,…,x_n)</script>，用x这个向量来代表这个事物。类别也是有很多种，用集合<script type="math/tex">Y={y_1,y_2,…y_m}</script>表示。如果x属于<script type="math/tex">y_1</script>类别，就可以给x打上<script type="math/tex">y_1</script>标签，意思是说x属于<script type="math/tex">y_1</script>类别。这就是所谓的<strong>分类(Classification)</strong>。</p>

<p>x的集合记为X，称为属性集。一般X和Y的关系是不确定的，你只能在某种程度上说x有多大可能性属于类<script type="math/tex">y_1</script>，比如说x有80%的可能性属于类<script type="math/tex">y_1</script>，这时可以把X和Y看做是随机变量，<script type="math/tex">P(Y \vert X)</script>称为Y的<strong>后验概率</strong>（posterior probability），与之相对的，P(Y)称为Y的<strong>先验概率</strong>（prior probability）<sup id="fnref:2"><a href="#fn:2" rel="footnote">1</a></sup>。</p>

<p>在训练阶段，我们要根据从训练数据中收集的信息，<strong>对X和Y的每一种组合学习后验概率<script type="math/tex">P(Y \vert X)</script>。</strong>分类时，来了一个实例x，在刚才训练得到的一堆后验概率中找出所有的<script type="math/tex">P(Y \vert x)</script>， 其中最大的那个y，即为x所属分类。根据贝叶斯公式，后验概率为<script type="math/tex">P(Y \vert X)=\dfrac{P(X \vert Y)P(Y)}{P(X)}</script></p>

<p>在比较不同Y值的后验概率时，分母P(X)总是常数，<strong>因此可以忽略</strong>。先验概率P(Y)可以通过计算训练集中属于每一个类的训练样本所占的比例容易地估计。</p>

<p>我们来举个简单的例子，让读者对上述思路有个形象的认识<sup id="fnref:3"><a href="#fn:3" rel="footnote">2</a></sup>。<br />
考虑一个医疗诊断问题，有两种可能的假设：（1）病人有癌症。（2）病人无癌症。样本数据来自某化验测试，它也有两种可能的结果：阳性和阴性。假设我们已经有先验知识：在所有人口中只有0.008的人患病。此外，化验测试对有病的患者有98%的可能返回阳性结果，对无病患者有97%的可能返回阴性结果。</p>

<p>上面的数据可以用以下概率式子表示：<br />
P(cancer)=0.008,P(无cancer)=0.992<br />
P(阳性|cancer)=0.98,P(阴性|cancer)=0.02<br />
P(阳性|无cancer)=0.03，P(阴性|无cancer)=0.97<br />
假设现在有一个新病人，化验测试返回阳性，是否将病人断定为有癌症呢？</p>

<p>在这里，Y={cancer，无cancer}，共两个类别，这个新病人是一个样本，他有一个属性阳性，可以令x=(阳性)。我们可以来计算各个类别的后验概率：<br />
P(cancer | 阳性) = P(阳性 | cancer)p(cancer)=0.98* 0.008 = 0.0078<br />
P(无cancer | 阳性) =P(阳性 | 无cancer)* p(无cancer)=0.03* 0.992 = 0.0298 <br />
因此，应该判断为无癌症。</p>

<table>
  <tbody>
    <tr>
      <td>在这个例子中，类条件概率，P(cancer</td>
      <td>阳性)和P(无cancer</td>
      <td>阳性)直接告诉了我们。</td>
    </tr>
  </tbody>
</table>

<p>一般地，对<strong>类条件概率<script type="math/tex">P(X \vert Y)</script></strong>的估计，有朴素贝叶斯分类器和贝叶斯信念网络两种方法，这里介绍朴素贝叶斯分类器。</p>

<h3 id="section-3">1.3 朴素贝叶斯分类器</h3>
<p><strong>1、条件独立性</strong><br />
给定类标号y，朴素贝叶斯分类器在估计类条件概率时假设属性之间条件独立。条件独立假设可以形式化的表达如下：<br />
$$
\prod\limits_{i=1}^{n} P(x_i  \vert Y=y)
$$
其中每个训练样本可用一个属性向量<script type="math/tex">X=(x_1,x_2,x_3,…,x_n)</script>表示，各个属性之间条件独立。</p>

<p>比如，对于一篇文章，</p>

<blockquote>
  <p>Good good study,Day day up.</p>
</blockquote>

<p>可以用一个文本特征向量来表示，<code>x=(Good, good, study, Day, day , up)</code>。一般各个词语之间肯定不是相互独立的，有一定的上下文联系。但在朴素贝叶斯文本分类时，我们假设个单词之间没有联系，可以用一个文本特征向量来表示这篇文章，这就是“朴素”的来历。</p>

<p><strong>2、朴素贝叶斯如何工作</strong><br />
<strong>有了条件独立假设，就不必计算X和Y的每一种组合的类条件概率</strong>，只需对给定的Y，计算每个<script type="math/tex">x_i</script>的条件概率。后一种方法更实用，因为它不需要很大的训练集就能获得较好的概率估计。</p>

<p><strong>3、估计分类属性的条件概率</strong><br />
<script type="math/tex">P(x_i \vert Y=y)</script>怎么计算呢？它一般根据类别y下包含属性<script type="math/tex">x_i</script>的实例的比例来估计。以文本分类为例，xi表示一个单词，<script type="math/tex">P(x_i \vert Y=y)=</script>包含该类别下包含单词的xi的文章总数/ 该类别下的文章总数。</p>

<p><strong>4、贝叶斯分类器举例</strong>
假设给定了如下训练样本数据，我们学习的目标是根据给定的天气状况判断你对PlayTennis这个请求的回答是Yes还是No。</p>

<table border="1" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td valign="top" width="95">Day</td>
<td valign="top" width="95">Outlook</td>
<td valign="top" width="95">Temperature</td>
<td valign="top" width="95">Humidity</td>
<td valign="top" width="95">Wind</td>
<td valign="top" width="95">PlayTennis</td>
</tr>
<tr>
<td valign="top" width="95">D1</td>
<td valign="top" width="95">Sunny</td>
<td valign="top" width="95">Hot</td>
<td valign="top" width="95">High</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">No</td>
</tr>
<tr>
<td valign="top" width="95">D2</td>
<td valign="top" width="95">Sunny</td>
<td valign="top" width="95">Hot</td>
<td valign="top" width="95">High</td>
<td valign="top" width="95">Strong</td>
<td valign="top" width="95">No</td>
</tr>
<tr>
<td valign="top" width="95">D3</td>
<td valign="top" width="95">Overcast</td>
<td valign="top" width="95">Hot</td>
<td valign="top" width="95">High</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D4</td>
<td valign="top" width="95">Rain</td>
<td valign="top" width="95">Mild</td>
<td valign="top" width="95">High</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D5</td>
<td valign="top" width="95">Rain</td>
<td valign="top" width="95">Cool</td>
<td valign="top" width="95">Normal</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D6</td>
<td valign="top" width="95">Rain</td>
<td valign="top" width="95">Cool</td>
<td valign="top" width="95">Normal</td>
<td valign="top" width="95">Strong</td>
<td valign="top" width="95">No</td>
</tr>
<tr>
<td valign="top" width="95">D7</td>
<td valign="top" width="95">Overcast</td>
<td valign="top" width="95">Cool</td>
<td valign="top" width="95">Normal</td>
<td valign="top" width="95">Strong</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D8</td>
<td valign="top" width="95">Sunny</td>
<td valign="top" width="95">Mild</td>
<td valign="top" width="95">High</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">No</td>
</tr>
<tr>
<td valign="top" width="95">D9</td>
<td valign="top" width="95">Sunny</td>
<td valign="top" width="95">Cool</td>
<td valign="top" width="95">Normal</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D10</td>
<td valign="top" width="95">Rain</td>
<td valign="top" width="95">Mild</td>
<td valign="top" width="95">Normal</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D11</td>
<td valign="top" width="95">Sunny</td>
<td valign="top" width="95">Mild</td>
<td valign="top" width="95">Normal</td>
<td valign="top" width="95">Strong</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D12</td>
<td valign="top" width="95">Overcast</td>
<td valign="top" width="95">Mild</td>
<td valign="top" width="95">High</td>
<td valign="top" width="95">Strong</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D13</td>
<td valign="top" width="95">Overcast</td>
<td valign="top" width="95">Hot</td>
<td valign="top" width="95">Normal</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D14</td>
<td valign="top" width="95">Rain</td>
<td valign="top" width="95">Mild</td>
<td valign="top" width="95">High</td>
<td valign="top" width="95">Strong</td>
<td valign="top" width="95">No</td>
</tr>
</tbody>
</table>
<p>可以看到这里样本数据集提供了14个训练样本，我们将使用此表的数据，并结合朴素贝叶斯分类器来分类下面的新实例：<br />
x = (Outlook = Sunny,Temprature = Cool,Humidity = High,Wind = Strong)</p>

<p>在这个例子中，属性向量X=(Outlook, Temperature, Humidity, Wind)，类集合Y={Yes, No}。我们需要利用训练数据计算后验概率<script type="math/tex">P(Yes \vert x)</script>和<script type="math/tex">P(No \vert x)</script>，如果<script type="math/tex">P(Yes \vert x)>P(No \vert x)</script>，那么新实例分类为Yes，否则为No。</p>

<p>为了计算后验概率，我们需要计算先验概率P(Yes)和P(No)和类条件概率<script type="math/tex">P(x_i \vert Y)</script>。</p>

<p>因为有9个样本属于Yes，5个样本属于No，所以<script type="math/tex">P(Yes)=\dfrac{9}{14}</script>, <script type="math/tex">P(No)=\dfrac{5}{14}</script>。类条件概率计算如下：<br />
<script type="math/tex">P(Outlook = Sunny \vert Yes)=\dfrac{2}{9}　　　P(Outlook = Sunny \vert No)=\dfrac{3}{5}</script><br />
<script type="math/tex">P(Temprature = Cool  \vert Yes) =\dfrac{3}{9}　　　P(Temprature = Cool  \vert No) =\dfrac{1}{5}</script><br />
<script type="math/tex">P(Humidity = High  \vert Yes) =\dfrac{3}{9}　　　P(Humidity = High  \vert No) =\dfrac{4}{5}</script>
<script type="math/tex">P(Wind = Strong  \vert Yes) =\dfrac{3}{9}　　　P(Wind = Strong  \vert No) =\dfrac{3}{5}</script>    </p>

<p>后验概率计算如下：
$$
\begin{aligned}
P(Yes  \vert  x) &amp; = P(Outlook = Sunny \vert Yes) \times P(Temprature = Cool  \vert Yes) \newline
&amp; \times P(Humidity = High  \vert Yes) \times P(Wind = Strong  \vert Yes) \times P(Yes) \newline
&amp; =\dfrac{2}{9} \times \dfrac{3}{9} \times \dfrac{3}{9} \times \dfrac{3}{9} \times \dfrac{3}{9} \times \dfrac{9}{14}=\dfrac{2}{243}=\dfrac{9}{1701} \approx 0.00529
\end{aligned}
$$
$$
\begin{aligned}
P(No  \vert  x)&amp;= P(Outlook = Sunny \vert No) \times P(Temprature = Cool  \vert No) \newline
&amp; \times P(Humidity = High  \vert No) \times P(Wind = Strong  \vert No) \times P(No) \newline
&amp; =\dfrac{3}{5}\times \dfrac{1}{5} \times \dfrac{4}{5} \times \dfrac{3}{5} \times  \dfrac{5}{14}=\dfrac{18}{875} \approx 0.02057
\end{aligned}
$$
通过计算得出<script type="math/tex">P(No  \vert  x)> P(Yes  \vert  x)</script>，所以该样本分类为No[^3]。</p>

<p><strong>5、条件概率的m估计</strong><br />
假设有来了一个新样本 <script type="math/tex">x_1= (Outlook = Cloudy,Temprature = Cool,Humidity = High,Wind = Strong)</script>，要求对其分类。我们来开始计算，<br />
<script type="math/tex">P(Outlook = Cloudy \vert Yes)=\dfrac{0}{9}=0  P(Outlook = Cloudy  \vert No)=\dfrac{0}{5}=0</script><br />
计算到这里，大家就会意识到，这里出现了一个新的属性值，在训练样本中所没有的。如果有一个属性的类条件概率为0，则整个类的后验概率就等于0，我们可以直接得到后验概率<script type="math/tex">P(Yes  \vert  x_1)= P(No  \vert  x_1)=0</script>，这时二者相等，无法分类。</p>

<p>当训练样本不能覆盖那么多的属性值时，都会出现上述的窘境。简单的使用样本比例来估计类条件概率的方法太脆弱了，尤其是当训练样本少而属性数目又很大时。</p>

<p>解决方法是使用m估计(m-estimate)方法来估计条件概率：
$$
P(x_i \vert y_i)=\dfrac{n_c+mp}{n+m}
$$
n是类<script type="math/tex">y_j</script>中的样本总数，<script type="math/tex">n_c</script>是类<script type="math/tex">y_j</script>中取值<script type="math/tex">x_i</script>的样本数，m是称为等价样本大小的参数，而p是用户指定的参数。如果没有训练集（即n=0），则<script type="math/tex">P(x_i \vert y_i)=p</script>, 因此p可以看作是在类<script type="math/tex">y_j</script>的样本中观察属性值<script type="math/tex">x_i</script>的先验概率。等价样本大小决定先验概率和观测概率<script type="math/tex">\dfrac{n_c}{n}</script>之间的平衡[^2]。</p>

<h2 id="section-4">2 朴素贝叶斯文本分类算法</h2>
<p>现在开始进入本文的主旨部分：如何将贝叶斯分类器应用到文本分类上来。</p>

<h3 id="section-5">2.1文本分类问题</h3>
<p>在文本分类中，假设我们有一个文档d∈X，X是文档向量空间(document space)，和一个固定的类集合C={c1,c2,…,cj}，类别又称为标签。显然，文档向量空间是一个高维度空间。我们把一堆打了标签的文档集合&lt;d,c&gt;作为训练样本，&lt;d,c&gt;∈X×C。例如：<br />
&lt;d,c&gt;={Beijing joins the World Trade Organization, China}<br />
对于这个只有一句话的文档，我们把它归类到 China，即打上china标签。</p>

<p>我们期望用某种训练算法，训练出一个函数γ，能够将文档映射到某一个类别：
γ:X→C</p>

<p>这种类型的学习方法叫做有监督学习，因为事先有一个监督者（我们事先给出了一堆打好标签的文档）像个老师一样监督着整个学习过程。</p>

<p>朴素贝叶斯分类器是一种有监督学习，常见有两种模型，多项式模型(multinomial model)和伯努利模型(Bernoulli model)<sup id="fnref:4"><a href="#fn:4" rel="footnote">3</a></sup>。</p>

<h3 id="section-6">2.2 多项式模型</h3>

<h4 id="section-7">2.2.1 基本原理</h4>
<p>在多项式模型中， 设某文档<script type="math/tex">d=(t_1,t_2,…,t_k)</script>，tk是该文档中出现过的单词，允许重复，则<br />
先验概率<script type="math/tex">P(c)=</script> 类c下单词总数/整个训练样本的单词总数<br />
类条件概率<script type="math/tex">P(t_k \vert c)=</script>(类c下单词tk在各个文档中出现过的次数之和+1)/(类c下单词总数+|V|)</p>

<p>V是训练样本的单词表（即抽取单词，单词出现多次，只算一个），<code>|V|</code>则表示训练样本包含多少种单词。在这里，<code>m=|V|, p=1/|V|</code>。</p>

<p><script type="math/tex">P(t_k \vert c)=</script>可以看作是单词tk在证明d属于类c上提供了多大的证据，而P(c)则可以认为是类别c在整体上占多大比例(有多大可能性)。</p>

<h4 id="section-8">2.2.2 伪代码<sup id="fnref:1"><a href="#fn:1" rel="footnote">4</a></sup></h4>
<p>``` java
//C，类别集合，D，用于训练的文本文件集合
TrainMultiNomialNB(C,D) {
    // 单词出现多次，只算一个
    V←ExtractVocabulary(D)
    // 单词可重复计算
    N←CountTokens(D)
    for each c∈C
        // 计算类别c下的单词总数
        // N和Nc的计算方法和Introduction to Information Retrieval上的不同，个人认为
        //该书是错误的，先验概率和类条件概率的计算方法应当保持一致
        Nc←CountTokensInClass(D,c)
        prior[c]←Nc/N
        // 将类别c下的文档连接成一个大字符串
        textc←ConcatenateTextOfAllDocsInClass(D,c)
        for each t∈V
            // 计算类c下单词t的出现次数
            Tct←CountTokensOfTerm(textc,t)
        for each t∈V
            //计算P(t|c)
            condprob[t][c]← 
    return V,prior,condprob
}</p>

<p>ApplyMultiNomialNB(C,V,prior,condprob,d) {
    // 将文档d中的单词抽取出来，允许重复，如果单词是全新的，在全局单词表V中都
    // 没出现过，则忽略掉
    W←ExtractTokensFromDoc(V,d)
    for each c∈C
        score[c]←prior[c]
        for each t∈W
            if t∈Vd
                score[c] *= condprob[t][c]
    return max(score[c])
}
```</p>

<h4 id="section-9">2.2.3 举例</h4>
<p>给定一组分类好了的文本训练数据，如下：  </p>

<table border="1" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td valign="top" width="64">docId</td>
<td valign="top" width="236">doc</td>
<td valign="top" width="126">类别In c=China?</td>
</tr>
<tr>
<td valign="top" width="64">1</td>
<td valign="top" width="236">Chinese Beijing Chinese</td>
<td valign="top" width="126">yes</td>
</tr>
<tr>
<td valign="top" width="64">2</td>
<td valign="top" width="236">Chinese Chinese Shanghai</td>
<td valign="top" width="126">yes</td>
</tr>
<tr>
<td valign="top" width="64">3</td>
<td valign="top" width="236">Chinese Macao</td>
<td valign="top" width="126">yes</td>
</tr>
<tr>
<td valign="top" width="64">4</td>
<td valign="top" width="236">Tokyo Japan Chinese</td>
<td valign="top" width="126">no</td>
</tr>
</tbody>
</table>

<p>给定一个新样本
&gt; Chinese Chinese Chinese Tokyo Japan</p>

<p>对其进行分类。该文本用属性向量表示为<code>d=(Chinese, Chinese, Chinese, Tokyo, Japan)</code>，类别集合为<code>Y={yes, no}</code>。</p>

<p>类yes下总共有8个单词，类no下总共有3个单词，训练样本单词总数为11，因此<script type="math/tex">P(yes)=\dfrac{8}{11}, P(no)=\dfrac{3}{11}</script>。类条件概率计算如下：<br />
<script type="math/tex">P(Chinese  \vert  yes)=\dfrac{5+1}{8+6}=\dfrac{6}{14}=\dfrac{3}{7}</script><br />
<script type="math/tex">P(Japan  \vert  yes)=P(Tokyo  \vert  yes)= \dfrac{0+1}{8+6}=\dfrac{1}{14}</script><br />
<script type="math/tex">P(Chinese \vert no)=\dfrac{1+1}{3+6}=\dfrac{2}{9}</script><br />
<script type="math/tex">P(Japan \vert no)=P(Tokyo \vert  no) =\dfrac{1+1}{3+6}=\dfrac{2}{9}</script><br />
分母中的8，是指yes类别下textc的长度，也即训练样本的单词总数，6是指训练样本有Chinese,Beijing,Shanghai, Macao, Tokyo, Japan 共6个单词，3是指no类下共有3个单词。</p>

<p>有了以上类条件概率，开始计算后验概率，<br />
<script type="math/tex">P(yes  \vert  d)=\left(\dfrac{3}{7}\right)^3 \times \dfrac{1}{14} \times \dfrac{1}{14} \times \dfrac{8}{11}=\dfrac{108}{184877} \approx 0.00058417</script><br />
<script type="math/tex">P(no  \vert  d)= \left(\dfrac{2}{9}\right)^3 \times \dfrac{2}{9} \times \dfrac{2}{9} \times \dfrac{3}{11}=\dfrac{32}{216513} \approx 0.00014780</script><br />
因此，这个文档属于类别china。</p>

<h3 id="section-10">2.3 伯努利模型</h3>

<h4 id="section-11">2.3.1 基本原理</h4>
<p><script type="math/tex">P(c)=</script> 类c下文件总数/整个训练样本的文件总数<br />
<script type="math/tex">P(t_k \vert c)=</script>(类c下包含单词tk的文件数+1)/(类c下单词总数+2)<br />
在这里，<script type="math/tex">m=2, p=\dfrac{1}{2}</script>。</p>

<p>后验概率的计算，也有点变化，见下面的伪代码。</p>

<h4 id="section-12">2.3.2 伪代码</h4>
<p>``` java
//C，类别集合，D，用于训练的文本文件集合
TrainBernoulliNB(C, D) {
    // 单词出现多次，只算一个
V←ExtractVocabulary(D)
    // 计算文件总数
    N←CountDocs(D)
    for each c∈C
        // 计算类别c下的文件总数
        Nc←CountDocsInClass(D,c)
        prior[c]←Nc/N
        for each t∈V
            // 计算类c下包含单词t的文件数
            Nct←CountDocsInClassContainingTerm(D,c,t)
            //计算P(t|c)
            condprob[t][c]←(Nct+1)/(Nct+2)
    return V,prior,condprob
}</p>

<p>ApplyBernoulliNB(C,V,prior,condprob,d) {
    // 将文档d中单词表抽取出来，如果单词是全新的，在全局单词表V中都没出现过，
    // 则舍弃
    Vd←ExtractTermsFromDoc(V,d)
    for each c∈C
        score[c]←prior[c]
        for each t∈V
            if t∈Vd
                score[c] *= condprob[t][c]
            else
                score[c] *= (1-condprob[t][c])
    return max(score[c])
}
```</p>

<h4 id="section-13">2.3.3 举例</h4>
<p>还是使用前面例子中的数据，不过模型换成了使用伯努利模型。</p>

<p>类yes下总共有3个文件，类no下有1个文件，训练样本文件总数为11，因此<script type="math/tex">P(yes)=\dfrac{3}{4}, P(Chinese  \vert  yes)=\dfrac{3+1}{3+2}=\dfrac{4}{5}</script><br />
<script type="math/tex">P(Japan  \vert  yes)=P(Tokyo  \vert  yes)=\dfrac{0+1}{3+2}=\dfrac{1}{5}</script><br />
<script type="math/tex">P(Beijing  \vert  yes)= P(Macao \vert yes)= P(Shanghai  \vert yes)=\dfrac{1+1}{3+2}=\dfrac{2}{5}</script><br />
<script type="math/tex">P(Chinese \vert no)=\dfrac{1+1}{1+2}=\dfrac{2}{3}</script><br />
<script type="math/tex">P(Japan \vert no)=P(Tokyo \vert  no) =\dfrac{1+1}{1+2}=\dfrac{2}{3}</script><br />
<script type="math/tex">P(Beijing \vert  no)= P(Macao \vert  no)= P(Shanghai  \vert  no)=\dfrac{0+1}{1+2}=\dfrac{1}{3}</script>  </p>

<p>有了以上类条件概率，开始计算后验概率，<br />
$$
\begin{aligned}
P(yes  \vert  d)&amp;=P(yes) \times P(Chinese \vert yes) \times P(Japan \vert yes) \times P(Tokyo \vert yes) \newline
&amp;\times (1-P(Beijing \vert yes)) \times (1-P(Shanghai \vert yes))\newline
&amp;\times (1-P(Macao \vert yes)) \newline
&amp;=\dfrac{3}{4} \times \dfrac{4}{5} \times \dfrac{1}{5} \times \dfrac{1}{5} \times (1-\dfrac{2}{5} \times (1-\dfrac{2}{5}) \times (1-\dfrac{2}{5})=\dfrac{81}{15625} \approx 0.005
\end{aligned}
$$
<script type="math/tex">P(no   \vert   d)= \dfrac{1}{4} \times \dfrac{2}{3} \times \dfrac{2}{5} \times \dfrac{2}{5} \times (1-\dfrac{1}{3}) \times (1-\dfrac{1}{3}) \times (1-\dfrac{1}{3})=\dfrac{16}{729} \approx 0.022</script><br />
因此，这个文档不属于类别china。</p>

<h3 id="section-14">2.4 两个模型的区别</h3>
<p>二者的计算粒度不一样，多项式模型以单词为粒度，伯努利模型以文件为粒度，因此二者的先验概率和类条件概率的计算方法都不同。</p>

<p>计算后验概率时，对于一个文档d，多项式模型中，只有在d中出现过的单词，才会参与后验概率计算，伯努利模型中，没有在d中出现，但是在全局单词表中出现的单词，也会参与计算，不过是作为“反方”参与的。</p>

<h2 id="section-15">3 代码详解</h2>
<p>本文附带了一个eclipse工程，有完整的源代码，以及一个微型文本训练库。</p>

<p>ChineseSpliter用于中文分词，StopWordsHandler用于判断一个单词是否是停止词，ClassifyResult用于保存结果，IntermediateData用于预处理文本语料库，TrainnedModel用于保存训练后得到的数据，NaiveBayesClassifier是基础类，包含了贝叶斯分类器的主要代码，MultiNomialNB是多项式模型，类似的，BernoulliNB是伯努利模型，二者都继承自NaiveBayesClassifier，都只重写了父类的计算先验概率，类条件概率和后验概率这3个函数。</p>

<h3 id="section-16">3.1 中文分词</h3>
<p>中文分词不是本文的重点，这里我们直接使用第三方工具，本源码使用的是<a href="http://www.jesoft.cn/">极易中文分词组件</a>，你还可以使用<a href="http://chtsai.org/">MMSEG</a>，中科院的<a href="http://ictclas.org/">ICTCLAS</a>等等。</p>

<p>``` java
/**
     * 对给定的文本进行中文分词.
     * 
     * @param text
     *            给定的文本
     * @param splitToken
     *            用于分割的标记,如”|”
     * @return 分词完毕的文本
     */
    public String split(final String text, final String splitToken) {
        String result = null;</p>

<pre><code>    try {
        result = analyzer.segment(text, splitToken);
    } catch (IOException e) {
        e.printStackTrace();
    }
    return result; } ```
</code></pre>

<h3 id="section-17">3.2 停止词处理</h3>
<p>停止词(Stop Word)是指那些无意义的字或词，如“的”、“在”等。去掉文档中的停止词也是必须的一项工作,这里简单的定义了一些常见的停止词，并根据这些常用停止词在分词时进行判断。</p>

<p>``` java
/** 常用停用词. */
    private static String[] stopWordsList = {
            // 来自 c:\Windows\System32\NOISE.CHS
            “的”, “一”, “不”, “在”, “人”, “有”, “是”, “为”, “以”, “于”, “上”, “他”, “而”,
            “后”, “之”, “来”, “及”, “了”, “因”, “下”, “可”, “到”, “由”, “这”, “与”, “也”,
            “此”, “但”, “并”, “个”, “其”, “已”, “无”, “小”, “我”, “们”, “起”, “最”, “再”,
            “今”, “去”, “好”, “只”, “又”, “或”, “很”, “亦”, “某”, “把”, “那”, “你”, “乃”,
            “它”,
            // 来自网络
            “要”, “将”, “应”, “位”, “新”, “两”, “中”, “更”, “我们”, “自己”, “没有”, ““”, “””,
            “，”, “（”, “）”, “” };</p>

<pre><code>/**
 * 判断一个词是否是停止词.
 * 
 * @param word
 *            要判断的词
 * @return 是停止词，返回true，否则返回false
 */
public static boolean isStopWord(final String word) {
    for (int i = 0; i &lt; stopWordsList.length; ++i) {
        if (word.equalsIgnoreCase(stopWordsList[i])) {
            return true;
        }
    }
    return false;
} ```
</code></pre>

<h3 id="section-18">3.3 预处理数据</h3>
<p>我们这里使用<a href="http://www.sogou.com/labs/dl/c.html">搜狗的文本分类语料库</a>作为训练样本，把SogouC.reduced.20061102.tar.gz解压到D盘，目录结构为</p>

<p><code>bash
D:\Reduced
         |-- C000008
         |-- C000010
         |-- C000013
         |-- C000014
         |-- C000016
         |-- C000020
         |-- C000022
         |-- C000023
         |-- C000024
</code>
IntermediateData.java主要用于处理文本数据，将所需要的信息计算好，存放到数据库文件中。</p>

<p>中间数据文件主要保存了如下信息，</p>

<p>``` java
/** 单词X在类别C下出现的总数. */
	public HashMap[] filesOfXC;
	/** 给定分类下的文件数目. */
    public int[] filesOfC;
    /** 根目录下的文件总数. */
    public int files;</p>

<pre><code>/** 单词X在类别C下出现的总数 */
public HashMap[] tokensOfXC;
/** 类别C下所有单词的总数. */
public int[] tokensOfC;
/** 整个语料库中单词的总数. */
public int tokens;
/** 整个训练语料所出现的单词. */
public HashSet&lt;String&gt; vocabulary; ``` 我们使用命令
</code></pre>

<p><code>bash
IntermediateData d:\Reduced\ gbk d:\reduced.db
</code>
将文本训练库的信息计算好，保存到中间文件中。以后的阶段，我们都不再需要文本语料库了，只需要reduced.db。</p>

<h3 id="section-19">3.3 训练</h3>
<p>基本的框架代码都在NaiveBayesClassifier中，MultiNomialNB和BernoulliNB都只是重新实现(override)了这三个函数。</p>

<p>``` java
/** 计算先验概率P(c). */
    protected void calculatePc() {
    }</p>

<pre><code>/** 计算类条件概率P(x|c). */
protected void calculatePxc() {
}


/**
 * 计算文本属性向量X在类Cj下的后验概率P(Cj|X).
 * 
 * @param x
 *            文本属性向量
 * @param cj
 *            给定的类别
 * @return 后验概率
 */
protected double calcProd(final String[] x, final int cj) {
    return 0;
} ```
</code></pre>

<p>训练函数如下：</p>

<p>```
public final void train(String intermediateData, String modelFile) {
    	// 加载中间数据文件
    	loadData(intermediateData);</p>

<pre><code>	model = new TrainnedModel(db.classifications.length);
	
	model.classifications = db.classifications;
	model.vocabulary = db.vocabulary;
	// 开始训练
	calculatePc();
	calculatePxc();
	db = null;
	
	try {
		// 用序列化，将训练得到的结果存放到模型文件中
        ObjectOutputStream out = new ObjectOutputStream(
                new FileOutputStream(modelFile));
        out.writeObject(model);
        out.close();
    } catch (IOException e) {
        e.printStackTrace();
    } } ``` 我们使用命令：
</code></pre>

<p><code>bash
MultiNomialNB –t d:\reduced.db d:\reduced.mdl
</code>
开始训练，得到的模型文件保存在reduced.mdl中。</p>

<h3 id="section-20">3.4 分类</h3>
<p>有了模型文件，就可以用它来进行分类了。</p>

<p>可以使用命令</p>

<p><code>bash
MultiNomialNB d:\reduced.mdl d:\temp.txt gbk
</code>
对文本文件temp.txt进行分类。</p>

<p>还可以将当初训练出这个模型文件的文本库，进行分类，看看正确率有多少，即“吃自己的狗食”，命令行如下</p>

<p><code>bash
MultiNomialNB -r d:\reduced\ gbk d:\reduced.mdl
</code></p>

<p>分类函数如下：</p>

<p>``` java
/**
 * 对给定的文本进行分类.
 * 
 * @param text
 *            给定的文本
 * @return 分类结果
 */
public final String classify(final String text) {
    String[] terms = null;
    // 中文分词处理(分词后结果可能还包含有停用词）
terms = textSpliter.split(text, “ “).split(“ “);
    // 去掉停用词，以免影响分类
    terms = ChineseSpliter.dropStopWords(terms); </p>

<p>double probility = 0.0;
    // 分类结果
    List<classifyresult> crs = new ArrayList<classifyresult>(); 
    for (int i = 0; i &lt; model.classifications.length; i++) {
        // 计算给定的文本属性向量terms在给定的分类Ci中的分类条件概率
        probility = calcProd(terms, i);
        // 保存分类结果
        ClassifyResult cr = new ClassifyResult();
         cr.classification = model.classifications[i]; // 分类
        cr.probility = probility; // 关键字在分类的条件概率
        System.out.println("In process....");
        System.out.println(model.classifications[i] + "：" + probility);
        crs.add(cr);
    }</classifyresult></classifyresult></p>

<pre><code>// 找出最大的元素
ClassifyResult maxElem = (ClassifyResult) java.util.Collections.max(
        crs, new Comparator() {
            public int compare(final Object o1, final Object o2) {
                final ClassifyResult m1 = (ClassifyResult) o1;
                final ClassifyResult m2 = (ClassifyResult) o2;
                final double ret = m1.probility - m2.probility;
                if (ret &lt; 0) {
                    return -1;
                } else {
                    return 1;
                }
            }
        });

return maxElem.classification; } ``` 测试正确率的函数getCorrectRate()，核心代码就是对每个文本文件调用classify()，将得到的类别和原始的类别比较，经过统计后就可以得到百分比。
</code></pre>

<p>更多细节请读者阅读<a href="http://yanjiuyanjiu-wordpress.stor.sinaapp.com/uploads/2010/05/NaiveBayesClassifier.zip">源代码</a>。</p>

<h2 id="section-21">参考文献</h2>

<div class="footnotes">
  <ol>
    <li id="fn:2">
      <p>Pang-Ning Tan, Michael Steinbach, Vipin Kumar, 《<a href="http://book.douban.com/subject/1786120/">数据挖掘导论</a>》，北京：人民邮电出版社，2007，第140~145页。<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>石志伟, 吴功宜, “<a href="http://d.wanfangdata.com.cn/Conference_5615512.aspx">基于朴素贝叶斯分类器的文本分类算法</a>”, 第一届全国信息检索与内容安全学术会议，2004<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>洞庭散人，“<a href="http://www.cnblogs.com/phinecos/archive/2008/10/21/1315948.html">基于朴素贝叶斯分类器的文本分类算法（上）</a>”，“<a href="http://www.cnblogs.com/phinecos/archive/2008/10/21/1316044.html">基于朴素贝叶斯分类器的文本分类算法（下）</a>”，2008<a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze, <a href="http://nlp.stanford.edu/IR-book/">Introduction to Information Retrieval</a>, Cambridge University Press, 2008, chapter 13, Text classification and Naive Bayes.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
</feed>
