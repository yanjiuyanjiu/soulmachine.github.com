
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>基于朴素贝叶斯的文本分类算法 - 研究研究</title>
  <meta name="author" content="soulmachine">

  
  <meta name="description" content="一个关于机器学习的技术博客">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://www.yanjiuyanjiu.com/blog/20100528">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/bootstrap/bootstrap.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/bootstrap/responsive.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/syntax/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">
  <style type="text/css">
    body {
      padding-bottom: 40px;
    }
    h1 {
      margin-bottom: 15px;
    }
    img {
      max-width: 100%;
    }
    .sharing, .meta, .pager {
      margin: 20px 0px 20px 0px;
    }
    .page-footer p {
      text-align: center;
    }
  </style>
  <script src="/javascripts/libs/jquery.js"></script>
  <script src="/javascripts/libs/modernizr-2.0.js"></script>
  <script src="/javascripts/libs/bootstrap.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="研究研究" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!-- <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css"> -->
<!-- <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css"> -->

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-7583537-4']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <nav role="navigation"><div class="navbar navbar-inverse">
  <div class="navbar-inner">
    <div class="container">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>

      <a class="brand" href="/">研究研究</a>

      <div class="nav-collapse">
        <ul class="nav">
  <li><a href="/">Home</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about/">About</a></li>
  <li><a href="/notes/">读书笔记</a></li>
</ul>


        <ul class="nav pull-right" data-subscription="rss">
          <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
          
        </ul>

        
          <form class="pull-right navbar-search" action="http://google.com/search" method="get">
            <fieldset role="search">
              <input type="hidden" name="q" value="site:www.yanjiuyanjiu.com" />
              <input class="search-query" type="text" name="q" results="0" placeholder="Search"/>
            </fieldset>
          </form>
        
      </div>
    </div>
  </div>
</div>
</nav>
  <div class="container">
    <div class="row-fluid">
      
<article class="hentry span9" role="article">

  
  <header class="page-header">
    
      <h1 class="entry-title">基于朴素贝叶斯的文本分类算法</h1>
    
    
      <p class="meta">
        








  


<time datetime="2010-05-28T17:15:00+08:00" pubdate data-updated="true">May 28<span>th</span>, 2010</time>
        
		
         | <a href="#duoshuo_thread">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>作者: 灵魂机器<br />
新浪博客：<a href="www.weibo.com/soulmachine">www.weibo.com/soulmachine</a><br />
作者博客：<a href="www.yanjiuyanjiu.com">www.yanjiuyanjiu.com</a></p>

<p><strong>摘要</strong>：常用的文本分类方法有支持向量机、K-近邻算法和朴素贝叶斯。其中朴素贝叶斯具有容易实现，运行速度快的特点，被广泛使用。本文详细介绍了朴素贝叶斯的基本原理，讨论了两种常见模型：多项式模型（MM）和伯努利模型（BM），实现了可运行的代码，并进行了一些数据测试。</p>

<p><strong>关键字</strong>：朴素贝叶斯；文本分类</p>

<p><strong>Text Classification Algorithm Based on Naive Bayes</strong><br />
<strong>Author</strong>: soulmachine<br />
<strong>Email</strong>：soulmachine@gmail.com<br />
<strong>Blog</strong>：<a href="www.yanjiuyanjiu.com">www.yanjiuyanjiu.com</a></p>

<p><strong>Abstract</strong>:Usually there are three methods for text classification: SVM、KNN and Naïve Bayes. Naïve Bayes is easy to implement and fast, so it is widely used. This article introduced the theory of Naïve Bayes and discussed two popular models: multinomial model(MM) and Bernoulli model(BM) in details, implemented runnable code and performed some data tests.</p>

<p><strong>Keywords</strong>: naïve bayes; text classification</p>

<h2 id="section">1 贝叶斯原理</h2>

<h3 id="section-1">1.1 贝叶斯公式</h3>

<p>设A、B是两个事件，且P(A)&gt;0，称 <script type="math/tex">P(Y \vert X)=\dfrac {P(XY)}{P(X)}</script> 为事件A发生的条件下事件B发生的<strong>条件概率</strong>。</p>

<p><strong>乘法公式</strong> <script type="math/tex">P(XYZ)=P(Z \vert XY)P(Y \vert X)P(X)</script><br />
<strong>全概率公式</strong>  <script type="math/tex">P(X)=P(X \vert Y_1)+ P(X \vert Y_2)+…+ P(X \vert Y_n)</script><br />
<strong>贝叶斯公式</strong>  <script type="math/tex">P(Y_i \vert X)=\dfrac{P(XY_i)}{P(X)}=\dfrac{P(X \vert Y_i)P(Y_i)}{P(X)}=\dfrac{P(X \vert Y_i)P(Y_i)}{\sum\limits _{j=1} ^{n} P(X \vert Y_j)}</script>  </p>

<p>在此处，贝叶斯公式，我们要用到的是 <script type="math/tex">P(Y_i \vert X)=\dfrac{P(X \vert Y_i)P(Y_i)}{P(X)}</script></p>

<p>以上公式，请读者参考<a href="http://book.douban.com/subject/1231189/">《概率论与数理统计（第五版）》</a>的1.4节“条件概率”（这里将原书中的A换成了X，B换成了Y），获得更深的理解。</p>

<!-- more -->

<h3 id="section-2">1.2 贝叶斯定理在分类中的应用</h3>
<p>在分类（classification）问题中，常常需要把一个事物分到某个类别。一个事物具有很多属性，把它的众多属性看做一个向量，即<script type="math/tex">x=(x_1,x_2,x_3,…,x_n)</script>，用x这个向量来代表这个事物。类别也是有很多种，用集合<script type="math/tex">Y={y_1,y_2,…y_m}</script>表示。如果x属于<script type="math/tex">y_1</script>类别，就可以给x打上<script type="math/tex">y_1</script>标签，意思是说x属于<script type="math/tex">y_1</script>类别。这就是所谓的<strong>分类(Classification)</strong>。</p>

<p>x的集合记为X，称为属性集。一般X和Y的关系是不确定的，你只能在某种程度上说x有多大可能性属于类<script type="math/tex">y_1</script>，比如说x有80%的可能性属于类<script type="math/tex">y_1</script>，这时可以把X和Y看做是随机变量，<script type="math/tex">P(Y \vert X)</script>称为Y的<strong>后验概率</strong>（posterior probability），与之相对的，P(Y)称为Y的<strong>先验概率</strong>（prior probability）<sup id="fnref:2"><a href="#fn:2" rel="footnote">1</a></sup>。</p>

<p>在训练阶段，我们要根据从训练数据中收集的信息，<strong>对X和Y的每一种组合学习后验概率<script type="math/tex">P(Y \vert X)</script>。</strong>分类时，来了一个实例x，在刚才训练得到的一堆后验概率中找出所有的<script type="math/tex">P(Y \vert x)</script>， 其中最大的那个y，即为x所属分类。根据贝叶斯公式，后验概率为<script type="math/tex">P(Y \vert X)=\dfrac{P(X \vert Y)P(Y)}{P(X)}</script></p>

<p>在比较不同Y值的后验概率时，分母P(X)总是常数，<strong>因此可以忽略</strong>。先验概率P(Y)可以通过计算训练集中属于每一个类的训练样本所占的比例容易地估计。</p>

<p>我们来举个简单的例子，让读者对上述思路有个形象的认识<sup id="fnref:3"><a href="#fn:3" rel="footnote">2</a></sup>。<br />
考虑一个医疗诊断问题，有两种可能的假设：（1）病人有癌症。（2）病人无癌症。样本数据来自某化验测试，它也有两种可能的结果：阳性和阴性。假设我们已经有先验知识：在所有人口中只有0.008的人患病。此外，化验测试对有病的患者有98%的可能返回阳性结果，对无病患者有97%的可能返回阴性结果。</p>

<p>上面的数据可以用以下概率式子表示：<br />
P(cancer)=0.008,P(无cancer)=0.992<br />
P(阳性|cancer)=0.98,P(阴性|cancer)=0.02<br />
P(阳性|无cancer)=0.03，P(阴性|无cancer)=0.97<br />
假设现在有一个新病人，化验测试返回阳性，是否将病人断定为有癌症呢？</p>

<p>在这里，Y={cancer，无cancer}，共两个类别，这个新病人是一个样本，他有一个属性阳性，可以令x=(阳性)。我们可以来计算各个类别的后验概率：<br />
P(cancer | 阳性) = P(阳性 | cancer)p(cancer)=0.98* 0.008 = 0.0078<br />
P(无cancer | 阳性) =P(阳性 | 无cancer)* p(无cancer)=0.03* 0.992 = 0.0298 <br />
因此，应该判断为无癌症。</p>

<table>
  <tbody>
    <tr>
      <td>在这个例子中，类条件概率，P(cancer</td>
      <td>阳性)和P(无cancer</td>
      <td>阳性)直接告诉了我们。</td>
    </tr>
  </tbody>
</table>

<p>一般地，对<strong>类条件概率<script type="math/tex">P(X \vert Y)</script></strong>的估计，有朴素贝叶斯分类器和贝叶斯信念网络两种方法，这里介绍朴素贝叶斯分类器。</p>

<h3 id="section-3">1.3 朴素贝叶斯分类器</h3>
<p><strong>1、条件独立性</strong><br />
给定类标号y，朴素贝叶斯分类器在估计类条件概率时假设属性之间条件独立。条件独立假设可以形式化的表达如下：<br />
$$
\prod\limits_{i=1}^{n} P(x_i  \vert Y=y)
$$
其中每个训练样本可用一个属性向量<script type="math/tex">X=(x_1,x_2,x_3,…,x_n)</script>表示，各个属性之间条件独立。</p>

<p>比如，对于一篇文章，</p>

<blockquote>
  <p>Good good study,Day day up.</p>
</blockquote>

<p>可以用一个文本特征向量来表示，<code>x=(Good, good, study, Day, day , up)</code>。一般各个词语之间肯定不是相互独立的，有一定的上下文联系。但在朴素贝叶斯文本分类时，我们假设个单词之间没有联系，可以用一个文本特征向量来表示这篇文章，这就是“朴素”的来历。</p>

<p><strong>2、朴素贝叶斯如何工作</strong><br />
<strong>有了条件独立假设，就不必计算X和Y的每一种组合的类条件概率</strong>，只需对给定的Y，计算每个<script type="math/tex">x_i</script>的条件概率。后一种方法更实用，因为它不需要很大的训练集就能获得较好的概率估计。</p>

<p><strong>3、估计分类属性的条件概率</strong><br />
<script type="math/tex">P(x_i \vert Y=y)</script>怎么计算呢？它一般根据类别y下包含属性<script type="math/tex">x_i</script>的实例的比例来估计。以文本分类为例，xi表示一个单词，<script type="math/tex">P(x_i \vert Y=y)=</script>包含该类别下包含单词的xi的文章总数/ 该类别下的文章总数。</p>

<p><strong>4、贝叶斯分类器举例</strong>
假设给定了如下训练样本数据，我们学习的目标是根据给定的天气状况判断你对PlayTennis这个请求的回答是Yes还是No。</p>

<table border="1" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td valign="top" width="95">Day</td>
<td valign="top" width="95">Outlook</td>
<td valign="top" width="95">Temperature</td>
<td valign="top" width="95">Humidity</td>
<td valign="top" width="95">Wind</td>
<td valign="top" width="95">PlayTennis</td>
</tr>
<tr>
<td valign="top" width="95">D1</td>
<td valign="top" width="95">Sunny</td>
<td valign="top" width="95">Hot</td>
<td valign="top" width="95">High</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">No</td>
</tr>
<tr>
<td valign="top" width="95">D2</td>
<td valign="top" width="95">Sunny</td>
<td valign="top" width="95">Hot</td>
<td valign="top" width="95">High</td>
<td valign="top" width="95">Strong</td>
<td valign="top" width="95">No</td>
</tr>
<tr>
<td valign="top" width="95">D3</td>
<td valign="top" width="95">Overcast</td>
<td valign="top" width="95">Hot</td>
<td valign="top" width="95">High</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D4</td>
<td valign="top" width="95">Rain</td>
<td valign="top" width="95">Mild</td>
<td valign="top" width="95">High</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D5</td>
<td valign="top" width="95">Rain</td>
<td valign="top" width="95">Cool</td>
<td valign="top" width="95">Normal</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D6</td>
<td valign="top" width="95">Rain</td>
<td valign="top" width="95">Cool</td>
<td valign="top" width="95">Normal</td>
<td valign="top" width="95">Strong</td>
<td valign="top" width="95">No</td>
</tr>
<tr>
<td valign="top" width="95">D7</td>
<td valign="top" width="95">Overcast</td>
<td valign="top" width="95">Cool</td>
<td valign="top" width="95">Normal</td>
<td valign="top" width="95">Strong</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D8</td>
<td valign="top" width="95">Sunny</td>
<td valign="top" width="95">Mild</td>
<td valign="top" width="95">High</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">No</td>
</tr>
<tr>
<td valign="top" width="95">D9</td>
<td valign="top" width="95">Sunny</td>
<td valign="top" width="95">Cool</td>
<td valign="top" width="95">Normal</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D10</td>
<td valign="top" width="95">Rain</td>
<td valign="top" width="95">Mild</td>
<td valign="top" width="95">Normal</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D11</td>
<td valign="top" width="95">Sunny</td>
<td valign="top" width="95">Mild</td>
<td valign="top" width="95">Normal</td>
<td valign="top" width="95">Strong</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D12</td>
<td valign="top" width="95">Overcast</td>
<td valign="top" width="95">Mild</td>
<td valign="top" width="95">High</td>
<td valign="top" width="95">Strong</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D13</td>
<td valign="top" width="95">Overcast</td>
<td valign="top" width="95">Hot</td>
<td valign="top" width="95">Normal</td>
<td valign="top" width="95">Weak</td>
<td valign="top" width="95">Yes</td>
</tr>
<tr>
<td valign="top" width="95">D14</td>
<td valign="top" width="95">Rain</td>
<td valign="top" width="95">Mild</td>
<td valign="top" width="95">High</td>
<td valign="top" width="95">Strong</td>
<td valign="top" width="95">No</td>
</tr>
</tbody>
</table>
<p>可以看到这里样本数据集提供了14个训练样本，我们将使用此表的数据，并结合朴素贝叶斯分类器来分类下面的新实例：<br />
x = (Outlook = Sunny,Temprature = Cool,Humidity = High,Wind = Strong)</p>

<p>在这个例子中，属性向量X=(Outlook, Temperature, Humidity, Wind)，类集合Y={Yes, No}。我们需要利用训练数据计算后验概率<script type="math/tex">P(Yes \vert x)</script>和<script type="math/tex">P(No \vert x)</script>，如果<script type="math/tex">P(Yes \vert x)>P(No \vert x)</script>，那么新实例分类为Yes，否则为No。</p>

<p>为了计算后验概率，我们需要计算先验概率P(Yes)和P(No)和类条件概率<script type="math/tex">P(x_i \vert Y)</script>。</p>

<p>因为有9个样本属于Yes，5个样本属于No，所以<script type="math/tex">P(Yes)=\dfrac{9}{14}</script>, <script type="math/tex">P(No)=\dfrac{5}{14}</script>。类条件概率计算如下：<br />
<script type="math/tex">P(Outlook = Sunny \vert Yes)=\dfrac{2}{9}　　　P(Outlook = Sunny \vert No)=\dfrac{3}{5}</script><br />
<script type="math/tex">P(Temprature = Cool  \vert Yes) =\dfrac{3}{9}　　　P(Temprature = Cool  \vert No) =\dfrac{1}{5}</script><br />
<script type="math/tex">P(Humidity = High  \vert Yes) =\dfrac{3}{9}　　　P(Humidity = High  \vert No) =\dfrac{4}{5}</script>
<script type="math/tex">P(Wind = Strong  \vert Yes) =\dfrac{3}{9}　　　P(Wind = Strong  \vert No) =\dfrac{3}{5}</script>    </p>

<p>后验概率计算如下：
$$
\begin{aligned}
P(Yes  \vert  x) &amp; = P(Outlook = Sunny \vert Yes) \times P(Temprature = Cool  \vert Yes) \newline
&amp; \times P(Humidity = High  \vert Yes) \times P(Wind = Strong  \vert Yes) \times P(Yes) \newline
&amp; =\dfrac{2}{9} \times \dfrac{3}{9} \times \dfrac{3}{9} \times \dfrac{3}{9} \times \dfrac{3}{9} \times \dfrac{9}{14}=\dfrac{2}{243}=\dfrac{9}{1701} \approx 0.00529
\end{aligned}
$$
$$
\begin{aligned}
P(No  \vert  x)&amp;= P(Outlook = Sunny \vert No) \times P(Temprature = Cool  \vert No) \newline
&amp; \times P(Humidity = High  \vert No) \times P(Wind = Strong  \vert No) \times P(No) \newline
&amp; =\dfrac{3}{5}\times \dfrac{1}{5} \times \dfrac{4}{5} \times \dfrac{3}{5} \times  \dfrac{5}{14}=\dfrac{18}{875} \approx 0.02057
\end{aligned}
$$
通过计算得出<script type="math/tex">P(No  \vert  x)> P(Yes  \vert  x)</script>，所以该样本分类为No[^3]。</p>

<p><strong>5、条件概率的m估计</strong><br />
假设有来了一个新样本 <script type="math/tex">x_1= (Outlook = Cloudy,Temprature = Cool,Humidity = High,Wind = Strong)</script>，要求对其分类。我们来开始计算，<br />
<script type="math/tex">P(Outlook = Cloudy \vert Yes)=\dfrac{0}{9}=0  P(Outlook = Cloudy  \vert No)=\dfrac{0}{5}=0</script><br />
计算到这里，大家就会意识到，这里出现了一个新的属性值，在训练样本中所没有的。如果有一个属性的类条件概率为0，则整个类的后验概率就等于0，我们可以直接得到后验概率<script type="math/tex">P(Yes  \vert  x_1)= P(No  \vert  x_1)=0</script>，这时二者相等，无法分类。</p>

<p>当训练样本不能覆盖那么多的属性值时，都会出现上述的窘境。简单的使用样本比例来估计类条件概率的方法太脆弱了，尤其是当训练样本少而属性数目又很大时。</p>

<p>解决方法是使用m估计(m-estimate)方法来估计条件概率：
$$
P(x_i \vert y_i)=\dfrac{n_c+mp}{n+m}
$$
n是类<script type="math/tex">y_j</script>中的样本总数，<script type="math/tex">n_c</script>是类<script type="math/tex">y_j</script>中取值<script type="math/tex">x_i</script>的样本数，m是称为等价样本大小的参数，而p是用户指定的参数。如果没有训练集（即n=0），则<script type="math/tex">P(x_i \vert y_i)=p</script>, 因此p可以看作是在类<script type="math/tex">y_j</script>的样本中观察属性值<script type="math/tex">x_i</script>的先验概率。等价样本大小决定先验概率和观测概率<script type="math/tex">\dfrac{n_c}{n}</script>之间的平衡[^2]。</p>

<h2 id="section-4">2 朴素贝叶斯文本分类算法</h2>
<p>现在开始进入本文的主旨部分：如何将贝叶斯分类器应用到文本分类上来。</p>

<h3 id="section-5">2.1文本分类问题</h3>
<p>在文本分类中，假设我们有一个文档d∈X，X是文档向量空间(document space)，和一个固定的类集合C={c1,c2,…,cj}，类别又称为标签。显然，文档向量空间是一个高维度空间。我们把一堆打了标签的文档集合&lt;d,c&gt;作为训练样本，&lt;d,c&gt;∈X×C。例如：<br />
&lt;d,c&gt;={Beijing joins the World Trade Organization, China}<br />
对于这个只有一句话的文档，我们把它归类到 China，即打上china标签。</p>

<p>我们期望用某种训练算法，训练出一个函数γ，能够将文档映射到某一个类别：
γ:X→C</p>

<p>这种类型的学习方法叫做有监督学习，因为事先有一个监督者（我们事先给出了一堆打好标签的文档）像个老师一样监督着整个学习过程。</p>

<p>朴素贝叶斯分类器是一种有监督学习，常见有两种模型，多项式模型(multinomial model)和伯努利模型(Bernoulli model)<sup id="fnref:4"><a href="#fn:4" rel="footnote">3</a></sup>。</p>

<h3 id="section-6">2.2 多项式模型</h3>

<h4 id="section-7">2.2.1 基本原理</h4>
<p>在多项式模型中， 设某文档<script type="math/tex">d=(t_1,t_2,…,t_k)</script>，tk是该文档中出现过的单词，允许重复，则<br />
先验概率<script type="math/tex">P(c)=</script> 类c下单词总数/整个训练样本的单词总数<br />
类条件概率<script type="math/tex">P(t_k \vert c)=</script>(类c下单词tk在各个文档中出现过的次数之和+1)/(类c下单词总数+|V|)</p>

<p>V是训练样本的单词表（即抽取单词，单词出现多次，只算一个），<code>|V|</code>则表示训练样本包含多少种单词。在这里，<code>m=|V|, p=1/|V|</code>。</p>

<p><script type="math/tex">P(t_k \vert c)=</script>可以看作是单词tk在证明d属于类c上提供了多大的证据，而P(c)则可以认为是类别c在整体上占多大比例(有多大可能性)。</p>

<h4 id="section-8">2.2.2 伪代码<sup id="fnref:1"><a href="#fn:1" rel="footnote">4</a></sup></h4>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="c1">//C，类别集合，D，用于训练的文本文件集合</span>
</span><span class="line"><span class="n">TrainMultiNomialNB</span><span class="o">(</span><span class="n">C</span><span class="o">,</span><span class="n">D</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">    <span class="c1">// 单词出现多次，只算一个</span>
</span><span class="line">    <span class="n">V</span><span class="err">←</span><span class="n">ExtractVocabulary</span><span class="o">(</span><span class="n">D</span><span class="o">)</span>
</span><span class="line">    <span class="c1">// 单词可重复计算</span>
</span><span class="line">    <span class="n">N</span><span class="err">←</span><span class="n">CountTokens</span><span class="o">(</span><span class="n">D</span><span class="o">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">each</span> <span class="n">c</span><span class="err">∈</span><span class="n">C</span>
</span><span class="line">        <span class="c1">// 计算类别c下的单词总数</span>
</span><span class="line">        <span class="c1">// N和Nc的计算方法和Introduction to Information Retrieval上的不同，个人认为</span>
</span><span class="line">        <span class="c1">//该书是错误的，先验概率和类条件概率的计算方法应当保持一致</span>
</span><span class="line">        <span class="n">Nc</span><span class="err">←</span><span class="n">CountTokensInClass</span><span class="o">(</span><span class="n">D</span><span class="o">,</span><span class="n">c</span><span class="o">)</span>
</span><span class="line">        <span class="n">prior</span><span class="o">[</span><span class="n">c</span><span class="o">]</span><span class="err">←</span><span class="n">Nc</span><span class="o">/</span><span class="n">N</span>
</span><span class="line">        <span class="c1">// 将类别c下的文档连接成一个大字符串</span>
</span><span class="line">        <span class="n">textc</span><span class="err">←</span><span class="n">ConcatenateTextOfAllDocsInClass</span><span class="o">(</span><span class="n">D</span><span class="o">,</span><span class="n">c</span><span class="o">)</span>
</span><span class="line">        <span class="k">for</span> <span class="n">each</span> <span class="n">t</span><span class="err">∈</span><span class="n">V</span>
</span><span class="line">            <span class="c1">// 计算类c下单词t的出现次数</span>
</span><span class="line">            <span class="n">Tct</span><span class="err">←</span><span class="n">CountTokensOfTerm</span><span class="o">(</span><span class="n">textc</span><span class="o">,</span><span class="n">t</span><span class="o">)</span>
</span><span class="line">        <span class="k">for</span> <span class="n">each</span> <span class="n">t</span><span class="err">∈</span><span class="n">V</span>
</span><span class="line">            <span class="c1">//计算P(t|c)</span>
</span><span class="line">            <span class="n">condprob</span><span class="o">[</span><span class="n">t</span><span class="o">][</span><span class="n">c</span><span class="o">]</span><span class="err">←</span>
</span><span class="line">    <span class="k">return</span> <span class="n">V</span><span class="o">,</span><span class="n">prior</span><span class="o">,</span><span class="n">condprob</span>
</span><span class="line"><span class="o">}</span>
</span><span class="line">
</span><span class="line"><span class="n">ApplyMultiNomialNB</span><span class="o">(</span><span class="n">C</span><span class="o">,</span><span class="n">V</span><span class="o">,</span><span class="n">prior</span><span class="o">,</span><span class="n">condprob</span><span class="o">,</span><span class="n">d</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">    <span class="c1">// 将文档d中的单词抽取出来，允许重复，如果单词是全新的，在全局单词表V中都</span>
</span><span class="line">    <span class="c1">// 没出现过，则忽略掉</span>
</span><span class="line">    <span class="n">W</span><span class="err">←</span><span class="n">ExtractTokensFromDoc</span><span class="o">(</span><span class="n">V</span><span class="o">,</span><span class="n">d</span><span class="o">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">each</span> <span class="n">c</span><span class="err">∈</span><span class="n">C</span>
</span><span class="line">        <span class="n">score</span><span class="o">[</span><span class="n">c</span><span class="o">]</span><span class="err">←</span><span class="n">prior</span><span class="o">[</span><span class="n">c</span><span class="o">]</span>
</span><span class="line">        <span class="k">for</span> <span class="n">each</span> <span class="n">t</span><span class="err">∈</span><span class="n">W</span>
</span><span class="line">            <span class="k">if</span> <span class="n">t</span><span class="err">∈</span><span class="n">Vd</span>
</span><span class="line">                <span class="n">score</span><span class="o">[</span><span class="n">c</span><span class="o">]</span> <span class="o">*=</span> <span class="n">condprob</span><span class="o">[</span><span class="n">t</span><span class="o">][</span><span class="n">c</span><span class="o">]</span>
</span><span class="line">    <span class="k">return</span> <span class="nf">max</span><span class="o">(</span><span class="n">score</span><span class="o">[</span><span class="n">c</span><span class="o">])</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h4 id="section-9">2.2.3 举例</h4>
<p>给定一组分类好了的文本训练数据，如下：  </p>

<table border="1" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td valign="top" width="64">docId</td>
<td valign="top" width="236">doc</td>
<td valign="top" width="126">类别In c=China?</td>
</tr>
<tr>
<td valign="top" width="64">1</td>
<td valign="top" width="236">Chinese Beijing Chinese</td>
<td valign="top" width="126">yes</td>
</tr>
<tr>
<td valign="top" width="64">2</td>
<td valign="top" width="236">Chinese Chinese Shanghai</td>
<td valign="top" width="126">yes</td>
</tr>
<tr>
<td valign="top" width="64">3</td>
<td valign="top" width="236">Chinese Macao</td>
<td valign="top" width="126">yes</td>
</tr>
<tr>
<td valign="top" width="64">4</td>
<td valign="top" width="236">Tokyo Japan Chinese</td>
<td valign="top" width="126">no</td>
</tr>
</tbody>
</table>

<p>给定一个新样本
&gt; Chinese Chinese Chinese Tokyo Japan</p>

<p>对其进行分类。该文本用属性向量表示为<code>d=(Chinese, Chinese, Chinese, Tokyo, Japan)</code>，类别集合为<code>Y={yes, no}</code>。</p>

<p>类yes下总共有8个单词，类no下总共有3个单词，训练样本单词总数为11，因此<script type="math/tex">P(yes)=\dfrac{8}{11}, P(no)=\dfrac{3}{11}</script>。类条件概率计算如下：<br />
<script type="math/tex">P(Chinese  \vert  yes)=\dfrac{5+1}{8+6}=\dfrac{6}{14}=\dfrac{3}{7}</script><br />
<script type="math/tex">P(Japan  \vert  yes)=P(Tokyo  \vert  yes)= \dfrac{0+1}{8+6}=\dfrac{1}{14}</script><br />
<script type="math/tex">P(Chinese \vert no)=\dfrac{1+1}{3+6}=\dfrac{2}{9}</script><br />
<script type="math/tex">P(Japan \vert no)=P(Tokyo \vert  no) =\dfrac{1+1}{3+6}=\dfrac{2}{9}</script><br />
分母中的8，是指yes类别下textc的长度，也即训练样本的单词总数，6是指训练样本有Chinese,Beijing,Shanghai, Macao, Tokyo, Japan 共6个单词，3是指no类下共有3个单词。</p>

<p>有了以上类条件概率，开始计算后验概率，<br />
<script type="math/tex">P(yes  \vert  d)=\left(\dfrac{3}{7}\right)^3 \times \dfrac{1}{14} \times \dfrac{1}{14} \times \dfrac{8}{11}=\dfrac{108}{184877} \approx 0.00058417</script><br />
<script type="math/tex">P(no  \vert  d)= \left(\dfrac{2}{9}\right)^3 \times \dfrac{2}{9} \times \dfrac{2}{9} \times \dfrac{3}{11}=\dfrac{32}{216513} \approx 0.00014780</script><br />
因此，这个文档属于类别china。</p>

<h3 id="section-10">2.3 伯努利模型</h3>

<h4 id="section-11">2.3.1 基本原理</h4>
<p><script type="math/tex">P(c)=</script> 类c下文件总数/整个训练样本的文件总数<br />
<script type="math/tex">P(t_k \vert c)=</script>(类c下包含单词tk的文件数+1)/(类c下单词总数+2)<br />
在这里，<script type="math/tex">m=2, p=\dfrac{1}{2}</script>。</p>

<p>后验概率的计算，也有点变化，见下面的伪代码。</p>

<h4 id="section-12">2.3.2 伪代码</h4>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="c1">//C，类别集合，D，用于训练的文本文件集合</span>
</span><span class="line"><span class="n">TrainBernoulliNB</span><span class="o">(</span><span class="n">C</span><span class="o">,</span> <span class="n">D</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">    <span class="c1">// 单词出现多次，只算一个</span>
</span><span class="line"><span class="n">V</span><span class="err">←</span><span class="n">ExtractVocabulary</span><span class="o">(</span><span class="n">D</span><span class="o">)</span>
</span><span class="line">    <span class="c1">// 计算文件总数</span>
</span><span class="line">    <span class="n">N</span><span class="err">←</span><span class="n">CountDocs</span><span class="o">(</span><span class="n">D</span><span class="o">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">each</span> <span class="n">c</span><span class="err">∈</span><span class="n">C</span>
</span><span class="line">        <span class="c1">// 计算类别c下的文件总数</span>
</span><span class="line">        <span class="n">Nc</span><span class="err">←</span><span class="n">CountDocsInClass</span><span class="o">(</span><span class="n">D</span><span class="o">,</span><span class="n">c</span><span class="o">)</span>
</span><span class="line">        <span class="n">prior</span><span class="o">[</span><span class="n">c</span><span class="o">]</span><span class="err">←</span><span class="n">Nc</span><span class="o">/</span><span class="n">N</span>
</span><span class="line">        <span class="k">for</span> <span class="n">each</span> <span class="n">t</span><span class="err">∈</span><span class="n">V</span>
</span><span class="line">            <span class="c1">// 计算类c下包含单词t的文件数</span>
</span><span class="line">            <span class="n">Nct</span><span class="err">←</span><span class="n">CountDocsInClassContainingTerm</span><span class="o">(</span><span class="n">D</span><span class="o">,</span><span class="n">c</span><span class="o">,</span><span class="n">t</span><span class="o">)</span>
</span><span class="line">            <span class="c1">//计算P(t|c)</span>
</span><span class="line">            <span class="n">condprob</span><span class="o">[</span><span class="n">t</span><span class="o">][</span><span class="n">c</span><span class="o">]</span><span class="err">←</span><span class="o">(</span><span class="n">Nct</span><span class="o">+</span><span class="mi">1</span><span class="o">)/(</span><span class="n">Nct</span><span class="o">+</span><span class="mi">2</span><span class="o">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">V</span><span class="o">,</span><span class="n">prior</span><span class="o">,</span><span class="n">condprob</span>
</span><span class="line"><span class="o">}</span>
</span><span class="line">
</span><span class="line"><span class="n">ApplyBernoulliNB</span><span class="o">(</span><span class="n">C</span><span class="o">,</span><span class="n">V</span><span class="o">,</span><span class="n">prior</span><span class="o">,</span><span class="n">condprob</span><span class="o">,</span><span class="n">d</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">    <span class="c1">// 将文档d中单词表抽取出来，如果单词是全新的，在全局单词表V中都没出现过，</span>
</span><span class="line">    <span class="c1">// 则舍弃</span>
</span><span class="line">    <span class="n">Vd</span><span class="err">←</span><span class="n">ExtractTermsFromDoc</span><span class="o">(</span><span class="n">V</span><span class="o">,</span><span class="n">d</span><span class="o">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">each</span> <span class="n">c</span><span class="err">∈</span><span class="n">C</span>
</span><span class="line">        <span class="n">score</span><span class="o">[</span><span class="n">c</span><span class="o">]</span><span class="err">←</span><span class="n">prior</span><span class="o">[</span><span class="n">c</span><span class="o">]</span>
</span><span class="line">        <span class="k">for</span> <span class="n">each</span> <span class="n">t</span><span class="err">∈</span><span class="n">V</span>
</span><span class="line">            <span class="k">if</span> <span class="n">t</span><span class="err">∈</span><span class="n">Vd</span>
</span><span class="line">                <span class="n">score</span><span class="o">[</span><span class="n">c</span><span class="o">]</span> <span class="o">*=</span> <span class="n">condprob</span><span class="o">[</span><span class="n">t</span><span class="o">][</span><span class="n">c</span><span class="o">]</span>
</span><span class="line">            <span class="k">else</span>
</span><span class="line">                <span class="n">score</span><span class="o">[</span><span class="n">c</span><span class="o">]</span> <span class="o">*=</span> <span class="o">(</span><span class="mi">1</span><span class="o">-</span><span class="n">condprob</span><span class="o">[</span><span class="n">t</span><span class="o">][</span><span class="n">c</span><span class="o">])</span>
</span><span class="line">    <span class="k">return</span> <span class="nf">max</span><span class="o">(</span><span class="n">score</span><span class="o">[</span><span class="n">c</span><span class="o">])</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h4 id="section-13">2.3.3 举例</h4>
<p>还是使用前面例子中的数据，不过模型换成了使用伯努利模型。</p>

<p>类yes下总共有3个文件，类no下有1个文件，训练样本文件总数为11，因此<script type="math/tex">P(yes)=\dfrac{3}{4}, P(Chinese  \vert  yes)=\dfrac{3+1}{3+2}=\dfrac{4}{5}</script><br />
<script type="math/tex">P(Japan  \vert  yes)=P(Tokyo  \vert  yes)=\dfrac{0+1}{3+2}=\dfrac{1}{5}</script><br />
<script type="math/tex">P(Beijing  \vert  yes)= P(Macao \vert yes)= P(Shanghai  \vert yes)=\dfrac{1+1}{3+2}=\dfrac{2}{5}</script><br />
<script type="math/tex">P(Chinese \vert no)=\dfrac{1+1}{1+2}=\dfrac{2}{3}</script><br />
<script type="math/tex">P(Japan \vert no)=P(Tokyo \vert  no) =\dfrac{1+1}{1+2}=\dfrac{2}{3}</script><br />
<script type="math/tex">P(Beijing \vert  no)= P(Macao \vert  no)= P(Shanghai  \vert  no)=\dfrac{0+1}{1+2}=\dfrac{1}{3}</script>  </p>

<p>有了以上类条件概率，开始计算后验概率，<br />
$$
\begin{aligned}
P(yes  \vert  d)&amp;=P(yes) \times P(Chinese \vert yes) \times P(Japan \vert yes) \times P(Tokyo \vert yes) \newline
&amp;\times (1-P(Beijing \vert yes)) \times (1-P(Shanghai \vert yes))\newline
&amp;\times (1-P(Macao \vert yes)) \newline
&amp;=\dfrac{3}{4} \times \dfrac{4}{5} \times \dfrac{1}{5} \times \dfrac{1}{5} \times (1-\dfrac{2}{5} \times (1-\dfrac{2}{5}) \times (1-\dfrac{2}{5})=\dfrac{81}{15625} \approx 0.005
\end{aligned}
$$
<script type="math/tex">P(no   \vert   d)= \dfrac{1}{4} \times \dfrac{2}{3} \times \dfrac{2}{5} \times \dfrac{2}{5} \times (1-\dfrac{1}{3}) \times (1-\dfrac{1}{3}) \times (1-\dfrac{1}{3})=\dfrac{16}{729} \approx 0.022</script><br />
因此，这个文档不属于类别china。</p>

<h3 id="section-14">2.4 两个模型的区别</h3>
<p>二者的计算粒度不一样，多项式模型以单词为粒度，伯努利模型以文件为粒度，因此二者的先验概率和类条件概率的计算方法都不同。</p>

<p>计算后验概率时，对于一个文档d，多项式模型中，只有在d中出现过的单词，才会参与后验概率计算，伯努利模型中，没有在d中出现，但是在全局单词表中出现的单词，也会参与计算，不过是作为“反方”参与的。</p>

<h2 id="section-15">3 代码详解</h2>
<p>本文附带了一个eclipse工程，有完整的源代码，以及一个微型文本训练库。</p>

<p>ChineseSpliter用于中文分词，StopWordsHandler用于判断一个单词是否是停止词，ClassifyResult用于保存结果，IntermediateData用于预处理文本语料库，TrainnedModel用于保存训练后得到的数据，NaiveBayesClassifier是基础类，包含了贝叶斯分类器的主要代码，MultiNomialNB是多项式模型，类似的，BernoulliNB是伯努利模型，二者都继承自NaiveBayesClassifier，都只重写了父类的计算先验概率，类条件概率和后验概率这3个函数。</p>

<h3 id="section-16">3.1 中文分词</h3>
<p>中文分词不是本文的重点，这里我们直接使用第三方工具，本源码使用的是<a href="http://www.jesoft.cn/">极易中文分词组件</a>，你还可以使用<a href="http://chtsai.org/">MMSEG</a>，中科院的<a href="http://ictclas.org/">ICTCLAS</a>等等。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="cm">/**</span>
</span><span class="line"><span class="cm">     * 对给定的文本进行中文分词.</span>
</span><span class="line"><span class="cm">     * </span>
</span><span class="line"><span class="cm">     * @param text</span>
</span><span class="line"><span class="cm">     *            给定的文本</span>
</span><span class="line"><span class="cm">     * @param splitToken</span>
</span><span class="line"><span class="cm">     *            用于分割的标记,如&quot;|&quot;</span>
</span><span class="line"><span class="cm">     * @return 分词完毕的文本</span>
</span><span class="line"><span class="cm">     */</span>
</span><span class="line">    <span class="kd">public</span> <span class="n">String</span> <span class="nf">split</span><span class="o">(</span><span class="kd">final</span> <span class="n">String</span> <span class="n">text</span><span class="o">,</span> <span class="kd">final</span> <span class="n">String</span> <span class="n">splitToken</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">        <span class="n">String</span> <span class="n">result</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
</span><span class="line">
</span><span class="line">        <span class="k">try</span> <span class="o">{</span>
</span><span class="line">            <span class="n">result</span> <span class="o">=</span> <span class="n">analyzer</span><span class="o">.</span><span class="na">segment</span><span class="o">(</span><span class="n">text</span><span class="o">,</span> <span class="n">splitToken</span><span class="o">);</span>
</span><span class="line">        <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">IOException</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">            <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
</span><span class="line">        <span class="o">}</span>
</span><span class="line">        <span class="k">return</span> <span class="n">result</span><span class="o">;</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-17">3.2 停止词处理</h3>
<p>停止词(Stop Word)是指那些无意义的字或词，如“的”、“在”等。去掉文档中的停止词也是必须的一项工作,这里简单的定义了一些常见的停止词，并根据这些常用停止词在分词时进行判断。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="cm">/** 常用停用词. */</span>
</span><span class="line">    <span class="kd">private</span> <span class="kd">static</span> <span class="n">String</span><span class="o">[]</span> <span class="n">stopWordsList</span> <span class="o">=</span> <span class="o">{</span>
</span><span class="line">            <span class="c1">// 来自 c:\Windows\System32\NOISE.CHS</span>
</span><span class="line">            <span class="s">&quot;的&quot;</span><span class="o">,</span> <span class="s">&quot;一&quot;</span><span class="o">,</span> <span class="s">&quot;不&quot;</span><span class="o">,</span> <span class="s">&quot;在&quot;</span><span class="o">,</span> <span class="s">&quot;人&quot;</span><span class="o">,</span> <span class="s">&quot;有&quot;</span><span class="o">,</span> <span class="s">&quot;是&quot;</span><span class="o">,</span> <span class="s">&quot;为&quot;</span><span class="o">,</span> <span class="s">&quot;以&quot;</span><span class="o">,</span> <span class="s">&quot;于&quot;</span><span class="o">,</span> <span class="s">&quot;上&quot;</span><span class="o">,</span> <span class="s">&quot;他&quot;</span><span class="o">,</span> <span class="s">&quot;而&quot;</span><span class="o">,</span>
</span><span class="line">            <span class="s">&quot;后&quot;</span><span class="o">,</span> <span class="s">&quot;之&quot;</span><span class="o">,</span> <span class="s">&quot;来&quot;</span><span class="o">,</span> <span class="s">&quot;及&quot;</span><span class="o">,</span> <span class="s">&quot;了&quot;</span><span class="o">,</span> <span class="s">&quot;因&quot;</span><span class="o">,</span> <span class="s">&quot;下&quot;</span><span class="o">,</span> <span class="s">&quot;可&quot;</span><span class="o">,</span> <span class="s">&quot;到&quot;</span><span class="o">,</span> <span class="s">&quot;由&quot;</span><span class="o">,</span> <span class="s">&quot;这&quot;</span><span class="o">,</span> <span class="s">&quot;与&quot;</span><span class="o">,</span> <span class="s">&quot;也&quot;</span><span class="o">,</span>
</span><span class="line">            <span class="s">&quot;此&quot;</span><span class="o">,</span> <span class="s">&quot;但&quot;</span><span class="o">,</span> <span class="s">&quot;并&quot;</span><span class="o">,</span> <span class="s">&quot;个&quot;</span><span class="o">,</span> <span class="s">&quot;其&quot;</span><span class="o">,</span> <span class="s">&quot;已&quot;</span><span class="o">,</span> <span class="s">&quot;无&quot;</span><span class="o">,</span> <span class="s">&quot;小&quot;</span><span class="o">,</span> <span class="s">&quot;我&quot;</span><span class="o">,</span> <span class="s">&quot;们&quot;</span><span class="o">,</span> <span class="s">&quot;起&quot;</span><span class="o">,</span> <span class="s">&quot;最&quot;</span><span class="o">,</span> <span class="s">&quot;再&quot;</span><span class="o">,</span>
</span><span class="line">            <span class="s">&quot;今&quot;</span><span class="o">,</span> <span class="s">&quot;去&quot;</span><span class="o">,</span> <span class="s">&quot;好&quot;</span><span class="o">,</span> <span class="s">&quot;只&quot;</span><span class="o">,</span> <span class="s">&quot;又&quot;</span><span class="o">,</span> <span class="s">&quot;或&quot;</span><span class="o">,</span> <span class="s">&quot;很&quot;</span><span class="o">,</span> <span class="s">&quot;亦&quot;</span><span class="o">,</span> <span class="s">&quot;某&quot;</span><span class="o">,</span> <span class="s">&quot;把&quot;</span><span class="o">,</span> <span class="s">&quot;那&quot;</span><span class="o">,</span> <span class="s">&quot;你&quot;</span><span class="o">,</span> <span class="s">&quot;乃&quot;</span><span class="o">,</span>
</span><span class="line">            <span class="s">&quot;它&quot;</span><span class="o">,</span>
</span><span class="line">            <span class="c1">// 来自网络</span>
</span><span class="line">            <span class="s">&quot;要&quot;</span><span class="o">,</span> <span class="s">&quot;将&quot;</span><span class="o">,</span> <span class="s">&quot;应&quot;</span><span class="o">,</span> <span class="s">&quot;位&quot;</span><span class="o">,</span> <span class="s">&quot;新&quot;</span><span class="o">,</span> <span class="s">&quot;两&quot;</span><span class="o">,</span> <span class="s">&quot;中&quot;</span><span class="o">,</span> <span class="s">&quot;更&quot;</span><span class="o">,</span> <span class="s">&quot;我们&quot;</span><span class="o">,</span> <span class="s">&quot;自己&quot;</span><span class="o">,</span> <span class="s">&quot;没有&quot;</span><span class="o">,</span> <span class="s">&quot;“&quot;</span><span class="o">,</span> <span class="s">&quot;”&quot;</span><span class="o">,</span>
</span><span class="line">            <span class="s">&quot;，&quot;</span><span class="o">,</span> <span class="s">&quot;（&quot;</span><span class="o">,</span> <span class="s">&quot;）&quot;</span><span class="o">,</span> <span class="s">&quot;&quot;</span> <span class="o">};</span>
</span><span class="line">
</span><span class="line">    <span class="cm">/**</span>
</span><span class="line"><span class="cm">     * 判断一个词是否是停止词.</span>
</span><span class="line"><span class="cm">     * </span>
</span><span class="line"><span class="cm">     * @param word</span>
</span><span class="line"><span class="cm">     *            要判断的词</span>
</span><span class="line"><span class="cm">     * @return 是停止词，返回true，否则返回false</span>
</span><span class="line"><span class="cm">     */</span>
</span><span class="line">    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">boolean</span> <span class="nf">isStopWord</span><span class="o">(</span><span class="kd">final</span> <span class="n">String</span> <span class="n">word</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">        <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">stopWordsList</span><span class="o">.</span><span class="na">length</span><span class="o">;</span> <span class="o">++</span><span class="n">i</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">            <span class="k">if</span> <span class="o">(</span><span class="n">word</span><span class="o">.</span><span class="na">equalsIgnoreCase</span><span class="o">(</span><span class="n">stopWordsList</span><span class="o">[</span><span class="n">i</span><span class="o">]))</span> <span class="o">{</span>
</span><span class="line">                <span class="k">return</span> <span class="kc">true</span><span class="o">;</span>
</span><span class="line">            <span class="o">}</span>
</span><span class="line">        <span class="o">}</span>
</span><span class="line">        <span class="k">return</span> <span class="kc">false</span><span class="o">;</span>
</span><span class="line">    <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-18">3.3 预处理数据</h3>
<p>我们这里使用<a href="http://www.sogou.com/labs/dl/c.html">搜狗的文本分类语料库</a>作为训练样本，把SogouC.reduced.20061102.tar.gz解压到D盘，目录结构为</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">D:<span class="se">\R</span>educed
</span><span class="line">         |-- C000008
</span><span class="line">         |-- C000010
</span><span class="line">         |-- C000013
</span><span class="line">         |-- C000014
</span><span class="line">         |-- C000016
</span><span class="line">         |-- C000020
</span><span class="line">         |-- C000022
</span><span class="line">         |-- C000023
</span><span class="line">         |-- C000024
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>IntermediateData.java主要用于处理文本数据，将所需要的信息计算好，存放到数据库文件中。</p>

<p>中间数据文件主要保存了如下信息，</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="cm">/** 单词X在类别C下出现的总数. */</span>
</span><span class="line">	<span class="kd">public</span> <span class="n">HashMap</span><span class="o">[]</span> <span class="n">filesOfXC</span><span class="o">;</span>
</span><span class="line">	<span class="cm">/** 给定分类下的文件数目. */</span>
</span><span class="line">    <span class="kd">public</span> <span class="kt">int</span><span class="o">[]</span> <span class="n">filesOfC</span><span class="o">;</span>
</span><span class="line">    <span class="cm">/** 根目录下的文件总数. */</span>
</span><span class="line">    <span class="kd">public</span> <span class="kt">int</span> <span class="n">files</span><span class="o">;</span>
</span><span class="line">
</span><span class="line">	<span class="cm">/** 单词X在类别C下出现的总数 */</span>
</span><span class="line">	<span class="kd">public</span> <span class="n">HashMap</span><span class="o">[]</span> <span class="n">tokensOfXC</span><span class="o">;</span>
</span><span class="line">    <span class="cm">/** 类别C下所有单词的总数. */</span>
</span><span class="line">    <span class="kd">public</span> <span class="kt">int</span><span class="o">[]</span> <span class="n">tokensOfC</span><span class="o">;</span>
</span><span class="line">    <span class="cm">/** 整个语料库中单词的总数. */</span>
</span><span class="line">    <span class="kd">public</span> <span class="kt">int</span> <span class="n">tokens</span><span class="o">;</span>
</span><span class="line">    <span class="cm">/** 整个训练语料所出现的单词. */</span>
</span><span class="line">    <span class="kd">public</span> <span class="n">HashSet</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">vocabulary</span><span class="o">;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>我们使用命令</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">IntermediateData d:<span class="se">\R</span>educed<span class="se">\ </span>gbk d:<span class="se">\r</span>educed.db
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>将文本训练库的信息计算好，保存到中间文件中。以后的阶段，我们都不再需要文本语料库了，只需要reduced.db。</p>

<h3 id="section-19">3.3 训练</h3>
<p>基本的框架代码都在NaiveBayesClassifier中，MultiNomialNB和BernoulliNB都只是重新实现(override)了这三个函数。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="cm">/** 计算先验概率P(c). */</span>
</span><span class="line">    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">calculatePc</span><span class="o">()</span> <span class="o">{</span>
</span><span class="line">    <span class="o">}</span>
</span><span class="line">
</span><span class="line">    <span class="cm">/** 计算类条件概率P(x|c). */</span>
</span><span class="line">    <span class="kd">protected</span> <span class="kt">void</span> <span class="nf">calculatePxc</span><span class="o">()</span> <span class="o">{</span>
</span><span class="line">    <span class="o">}</span>
</span><span class="line">
</span><span class="line">
</span><span class="line">    <span class="cm">/**</span>
</span><span class="line"><span class="cm">     * 计算文本属性向量X在类Cj下的后验概率P(Cj|X).</span>
</span><span class="line"><span class="cm">     * </span>
</span><span class="line"><span class="cm">     * @param x</span>
</span><span class="line"><span class="cm">     *            文本属性向量</span>
</span><span class="line"><span class="cm">     * @param cj</span>
</span><span class="line"><span class="cm">     *            给定的类别</span>
</span><span class="line"><span class="cm">     * @return 后验概率</span>
</span><span class="line"><span class="cm">     */</span>
</span><span class="line">    <span class="kd">protected</span> <span class="kt">double</span> <span class="nf">calcProd</span><span class="o">(</span><span class="kd">final</span> <span class="n">String</span><span class="o">[]</span> <span class="n">x</span><span class="o">,</span> <span class="kd">final</span> <span class="kt">int</span> <span class="n">cj</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">        <span class="k">return</span> <span class="mi">0</span><span class="o">;</span>
</span><span class="line">    <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>训练函数如下：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="kd">public</span> <span class="kd">final</span> <span class="kt">void</span> <span class="nf">train</span><span class="o">(</span><span class="n">String</span> <span class="n">intermediateData</span><span class="o">,</span> <span class="n">String</span> <span class="n">modelFile</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">    	<span class="c1">// 加载中间数据文件</span>
</span><span class="line">    	<span class="n">loadData</span><span class="o">(</span><span class="n">intermediateData</span><span class="o">);</span>
</span><span class="line">    	
</span><span class="line">    	<span class="n">model</span> <span class="o">=</span> <span class="k">new</span> <span class="n">TrainnedModel</span><span class="o">(</span><span class="n">db</span><span class="o">.</span><span class="na">classifications</span><span class="o">.</span><span class="na">length</span><span class="o">);</span>
</span><span class="line">    	
</span><span class="line">    	<span class="n">model</span><span class="o">.</span><span class="na">classifications</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="na">classifications</span><span class="o">;</span>
</span><span class="line">    	<span class="n">model</span><span class="o">.</span><span class="na">vocabulary</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="na">vocabulary</span><span class="o">;</span>
</span><span class="line">    	<span class="c1">// 开始训练</span>
</span><span class="line">    	<span class="n">calculatePc</span><span class="o">();</span>
</span><span class="line">    	<span class="n">calculatePxc</span><span class="o">();</span>
</span><span class="line">    	<span class="n">db</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
</span><span class="line">    	
</span><span class="line">    	<span class="k">try</span> <span class="o">{</span>
</span><span class="line">    		<span class="c1">// 用序列化，将训练得到的结果存放到模型文件中</span>
</span><span class="line">            <span class="n">ObjectOutputStream</span> <span class="n">out</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ObjectOutputStream</span><span class="o">(</span>
</span><span class="line">                    <span class="k">new</span> <span class="nf">FileOutputStream</span><span class="o">(</span><span class="n">modelFile</span><span class="o">));</span>
</span><span class="line">            <span class="n">out</span><span class="o">.</span><span class="na">writeObject</span><span class="o">(</span><span class="n">model</span><span class="o">);</span>
</span><span class="line">            <span class="n">out</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span><span class="line">        <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">IOException</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">            <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
</span><span class="line">        <span class="o">}</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>我们使用命令：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">MultiNomialNB –t d:<span class="se">\r</span>educed.db d:<span class="se">\r</span>educed.mdl
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>开始训练，得到的模型文件保存在reduced.mdl中。</p>

<h3 id="section-20">3.4 分类</h3>
<p>有了模型文件，就可以用它来进行分类了。</p>

<p>可以使用命令</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">MultiNomialNB d:<span class="se">\r</span>educed.mdl d:<span class="se">\t</span>emp.txt gbk
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>对文本文件temp.txt进行分类。</p>

<p>还可以将当初训练出这个模型文件的文本库，进行分类，看看正确率有多少，即“吃自己的狗食”，命令行如下</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">MultiNomialNB -r d:<span class="se">\r</span>educed<span class="se">\ </span>gbk d:<span class="se">\r</span>educed.mdl
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>分类函数如下：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
</pre></td><td class="code"><pre><code class="java"><span class="line"><span class="cm">/**</span>
</span><span class="line"><span class="cm"> * 对给定的文本进行分类.</span>
</span><span class="line"><span class="cm"> * </span>
</span><span class="line"><span class="cm"> * @param text</span>
</span><span class="line"><span class="cm"> *            给定的文本</span>
</span><span class="line"><span class="cm"> * @return 分类结果</span>
</span><span class="line"><span class="cm"> */</span>
</span><span class="line"><span class="kd">public</span> <span class="kd">final</span> <span class="n">String</span> <span class="nf">classify</span><span class="o">(</span><span class="kd">final</span> <span class="n">String</span> <span class="n">text</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">    <span class="n">String</span><span class="o">[]</span> <span class="n">terms</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
</span><span class="line">    <span class="c1">// 中文分词处理(分词后结果可能还包含有停用词）</span>
</span><span class="line"><span class="n">terms</span> <span class="o">=</span> <span class="n">textSpliter</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="n">text</span><span class="o">,</span> <span class="s">&quot; &quot;</span><span class="o">).</span><span class="na">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">);</span>
</span><span class="line">    <span class="c1">// 去掉停用词，以免影响分类</span>
</span><span class="line">    <span class="n">terms</span> <span class="o">=</span> <span class="n">ChineseSpliter</span><span class="o">.</span><span class="na">dropStopWords</span><span class="o">(</span><span class="n">terms</span><span class="o">);</span>
</span><span class="line">
</span><span class="line"><span class="kt">double</span> <span class="n">probility</span> <span class="o">=</span> <span class="mf">0.0</span><span class="o">;</span>
</span><span class="line">    <span class="c1">// 分类结果</span>
</span><span class="line">    <span class="n">List</span><span class="o">&lt;</span><span class="n">ClassifyResult</span><span class="o">&gt;</span> <span class="n">crs</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;</span><span class="n">ClassifyResult</span><span class="o">&gt;();</span>
</span><span class="line">    <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">model</span><span class="o">.</span><span class="na">classifications</span><span class="o">.</span><span class="na">length</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
</span><span class="line">        <span class="c1">// 计算给定的文本属性向量terms在给定的分类Ci中的分类条件概率</span>
</span><span class="line">        <span class="n">probility</span> <span class="o">=</span> <span class="n">calcProd</span><span class="o">(</span><span class="n">terms</span><span class="o">,</span> <span class="n">i</span><span class="o">);</span>
</span><span class="line">        <span class="c1">// 保存分类结果</span>
</span><span class="line">        <span class="n">ClassifyResult</span> <span class="n">cr</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ClassifyResult</span><span class="o">();</span>
</span><span class="line">         <span class="n">cr</span><span class="o">.</span><span class="na">classification</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="na">classifications</span><span class="o">[</span><span class="n">i</span><span class="o">];</span> <span class="c1">// 分类</span>
</span><span class="line">        <span class="n">cr</span><span class="o">.</span><span class="na">probility</span> <span class="o">=</span> <span class="n">probility</span><span class="o">;</span> <span class="c1">// 关键字在分类的条件概率</span>
</span><span class="line">        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">&quot;In process....&quot;</span><span class="o">);</span>
</span><span class="line">        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="n">model</span><span class="o">.</span><span class="na">classifications</span><span class="o">[</span><span class="n">i</span><span class="o">]</span> <span class="o">+</span> <span class="s">&quot;：&quot;</span> <span class="o">+</span> <span class="n">probility</span><span class="o">);</span>
</span><span class="line">        <span class="n">crs</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">cr</span><span class="o">);</span>
</span><span class="line">    <span class="o">}</span>
</span><span class="line">
</span><span class="line">    <span class="c1">// 找出最大的元素</span>
</span><span class="line">    <span class="n">ClassifyResult</span> <span class="n">maxElem</span> <span class="o">=</span> <span class="o">(</span><span class="n">ClassifyResult</span><span class="o">)</span> <span class="n">java</span><span class="o">.</span><span class="na">util</span><span class="o">.</span><span class="na">Collections</span><span class="o">.</span><span class="na">max</span><span class="o">(</span>
</span><span class="line">            <span class="n">crs</span><span class="o">,</span> <span class="k">new</span> <span class="n">Comparator</span><span class="o">()</span> <span class="o">{</span>
</span><span class="line">                <span class="kd">public</span> <span class="kt">int</span> <span class="nf">compare</span><span class="o">(</span><span class="kd">final</span> <span class="n">Object</span> <span class="n">o1</span><span class="o">,</span> <span class="kd">final</span> <span class="n">Object</span> <span class="n">o2</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">                    <span class="kd">final</span> <span class="n">ClassifyResult</span> <span class="n">m1</span> <span class="o">=</span> <span class="o">(</span><span class="n">ClassifyResult</span><span class="o">)</span> <span class="n">o1</span><span class="o">;</span>
</span><span class="line">                    <span class="kd">final</span> <span class="n">ClassifyResult</span> <span class="n">m2</span> <span class="o">=</span> <span class="o">(</span><span class="n">ClassifyResult</span><span class="o">)</span> <span class="n">o2</span><span class="o">;</span>
</span><span class="line">                    <span class="kd">final</span> <span class="kt">double</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">m1</span><span class="o">.</span><span class="na">probility</span> <span class="o">-</span> <span class="n">m2</span><span class="o">.</span><span class="na">probility</span><span class="o">;</span>
</span><span class="line">                    <span class="k">if</span> <span class="o">(</span><span class="n">ret</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
</span><span class="line">                        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="o">;</span>
</span><span class="line">                    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class="line">                        <span class="k">return</span> <span class="mi">1</span><span class="o">;</span>
</span><span class="line">                    <span class="o">}</span>
</span><span class="line">                <span class="o">}</span>
</span><span class="line">            <span class="o">});</span>
</span><span class="line">
</span><span class="line">    <span class="k">return</span> <span class="n">maxElem</span><span class="o">.</span><span class="na">classification</span><span class="o">;</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>测试正确率的函数getCorrectRate()，核心代码就是对每个文本文件调用classify()，将得到的类别和原始的类别比较，经过统计后就可以得到百分比。</p>

<p>更多细节请读者阅读<a href="http://yanjiuyanjiu-wordpress.stor.sinaapp.com/uploads/2010/05/NaiveBayesClassifier.zip">源代码</a>。</p>

<h2 id="section-21">参考文献</h2>

<div class="footnotes">
  <ol>
    <li id="fn:2">
      <p>Pang-Ning Tan, Michael Steinbach, Vipin Kumar, 《<a href="http://book.douban.com/subject/1786120/">数据挖掘导论</a>》，北京：人民邮电出版社，2007，第140~145页。<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>石志伟, 吴功宜, “<a href="http://d.wanfangdata.com.cn/Conference_5615512.aspx">基于朴素贝叶斯分类器的文本分类算法</a>”, 第一届全国信息检索与内容安全学术会议，2004<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>洞庭散人，“<a href="http://www.cnblogs.com/phinecos/archive/2008/10/21/1315948.html">基于朴素贝叶斯分类器的文本分类算法（上）</a>”，“<a href="http://www.cnblogs.com/phinecos/archive/2008/10/21/1316044.html">基于朴素贝叶斯分类器的文本分类算法（下）</a>”，2008<a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze, <a href="http://nlp.stanford.edu/IR-book/">Introduction to Information Retrieval</a>, Cambridge University Press, 2008, chapter 13, Text classification and Naive Bayes.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">soulmachine</span></span>

      








  


<time datetime="2010-05-28T17:15:00+08:00" pubdate data-updated="true">May 28<span>th</span>, 2010</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/machine-learning/'>Machine-Learning</a>
  
</span>


    </p>
    
      <div class="sharing">
  <!-- AddThis Button BEGIN -->
  <div class="addthis_toolbox addthis_default_style addthis_32x32_style">
    <a class="addthis_button_sinaweibo"></a>
    <a class="addthis_button_facebook"></a>
    <a class="addthis_button_twitter"></a>
    <a class="addthis_button_google_plusone_share"></a>
    <a class="addthis_button_delicious"></a>
    <a class="addthis_button_digg"></a>
    <a class="addthis_button_reddit"></a>
    <a class="addthis_button_compact"></a><a class="addthis_counter addthis_bubble_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=undefined"></script>
  <!-- AddThis Button END -->
  
  
  
</div>

    
    
	
    <section>
      <div id="duoshuo_thread" aria-live="polite"><!-- Duoshuo Comment BEGIN -->
<div class="ds-thread" data-thread-key="/blog/20100528" data-title="基于朴素贝叶斯的文本分类算法"></div>
<script type="text/javascript">
var duoshuoQuery = {short_name:"yanjiuyanjiu"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = 'http://static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
</script>
<!-- Duoshuo Comment END --></div>
    </section>
    
    <ul class="pager">
      
      <li class="previous"><a class="basic-alignment left"
        href="/blog/20100401" title="Previous Post:
        Java使用imageio 读写图像">&laquo; Java使用imageio 读写图像</a></li>
      
      <li><a href="/blog/archives">Blog Archives</a></li>
      
      <li class="next"><a class="basic-alignment right" href="/blog/20110417"
        title="Next Post: 推荐给TeX新手的电子书和书籍">推荐给TeX新手的电子书和书籍
        &raquo;</a></li>
      
    </ul>
  </footer>
</article>

<aside class="sidebar-nav span3">
  
    <section>
  <h2>公告</h2>
  <p>独学而无友，则孤陋而寡闻，我每周在清华举办机器学习读书会，欢迎大家前来交流切磋。</p>
  <p>详情请见<a href="http://q.weibo.com/1644133">读书会微博群</a></p>
</section>
<section>
  <h2>分类目录</h2>
    <ul id="category-list"><li><a href='/blog/categories/algorithm/'>Algorithm (1)</a></li><li><a href='/blog/categories/devops/'>DevOps (8)</a></li><li><a href='/blog/categories/docker/'>Docker (3)</a></li><li><a href='/blog/categories/hadoop/'>Hadoop (8)</a></li><li><a href='/blog/categories/language/'>Language (3)</a></li><li><a href='/blog/categories/machine-learning/'>Machine-Learning (5)</a></li><li><a href='/blog/categories/search-engine/'>Search-Engine (5)</a></li><li><a href='/blog/categories/spark/'>Spark (6)</a></li><li><a href='/blog/categories/tools/'>Tools (11)</a></li></ul>
</section>
<section>
  <h2>友情链接</h2>
  <ul>
    <li>
      <a href="http://yewen.us/" title="大学同学，ACM高手，曾在百度，现在人人网">笨狗随手留下</a>
    </li>
	<li>
      <a href="http://blog.liancheng.info/" title="网易，百度，技术高手">连城</a>
    </li>
    <li>
      <a href="http://www.rational.so/" title="系统方向的清华博士">阎栋</a>
    </li>
	<li>
      <a href="http://www.parallellabs.com/" title="冠诚，IBM研究院研究员">并行实验室</a>
    </li>
	<li>
      <a href="http://shenfeng.me/" title="http-kit, clojure">沈峰</a>
    </li>
	<li>
      <a href="http://www.lihaipeng.info/" title="百度商业产品高级研发工程师">李海鹏</a>
    </li>
	<li>
      <a href="http://www.chioka.in/" title="Google工程师">Eric</a>
    </li>
    <li>
      <a href="http://blog.csdn.net/lgnlgn" title="好朋友，曾在赶集网，现在老家的猫扑">梁兄的技术博客</a>
    </li>
    <li>
      <a href="http://www.doesbetter.com/" title="机器学习，北邮">王孝舒的博客</a>
    </li>
    <li>
      <a href="http://www.foreverlee.net/" title="机器学习，中科院计算所">ForeverLee</a>
    </li>
  </ul>
</section>




<section>
  <h2>最新文章</h2>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/20140220">编写Nutch插件</a>
      </li>
    
      <li class="post">
        <a href="/blog/20140214">CentOS上编译 Hadoop 2.2.0</a>
      </li>
    
      <li class="post">
        <a href="/blog/20140208">在CentOS上安装HBase 0.96</a>
      </li>
    
      <li class="post">
        <a href="/blog/20140207">在CentOS上安装ZooKeeper集群</a>
      </li>
    
      <li class="post">
        <a href="/blog/20140206">Hadoop多用户的配置(Hadoop 2.x)</a>
      </li>
    
  </ul>
</section><section>
<h2>最新评论</h2>
<ul class="ds-recent-comments" data-num-items="5" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="0" data-excerpt-length="32"></ul>

</section>

  
</aside>


    </div>
  </div>
  <footer role="contentinfo" class="page-footer"><!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
<p>
  Copyright &copy; 2014 - soulmachine -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  











</body>
</html>
