<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[研究研究]]></title>
  <link href="http://www.yanjiuyanjiu.com/atom.xml" rel="self"/>
  <link href="http://www.yanjiuyanjiu.com/"/>
  <updated>2014-03-19T18:21:38+08:00</updated>
  <id>http://www.yanjiuyanjiu.com/</id>
  <author>
    <name><![CDATA[soulmachine]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[编写Nutch插件]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140220"/>
    <updated>2014-02-20T16:53:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/writing-nutch-plugins</id>
    <content type="html"><![CDATA[<p>软件版本：Nutch 1.7</p>

<p>Nutch Plugin的所有资料，都在官网这里, <a href="http://wiki.apache.org/nutch/PluginCentral">PluginCentral</a></p>

<h2 id="section">前提</h2>
<p><a href="http://www.yanjiuyanjiu.com/blog/20140120">在Eclipse里运行Nutch</a></p>

<h2 id="extension--extension-point">Extension 和 Extension-point的关系</h2>
<p>Extension point类似与Java语言里的接口(interface), extension 则是具体的实现(implementation)。</p>

<p><a href="http://wiki.apache.org/nutch/AboutPlugins">About Plugins</a>里有一句话，<strong>Each extension-point defines an interface that must be implemented by the extension.</strong></p>

<h2 id="nutch">Nutch里的各种概念</h2>
<p>Extension point, extension, plugin, 这些概念是什么意思？见 <a href="http://wiki.apache.org/nutch/WhichTechnicalConceptsAreBehindTheNutchPluginSystem">Technical Concepts Behind the Nutch Plugin System</a></p>

<h2 id="nutch-17--extension-point">Nutch 1.7 有哪些 Extension-point</h2>
<p><img src="http://www.yanjiuyanjiu.com/images/extension-points.png" alt="" /></p>

<p>ExtensionPoint 这个东西，本身也是一个插件，可以看看 <code>src/plugin/nutch-extensionpoints/plugin.xml</code>，里面定义了所有的扩展点，跟上图基本一致。</p>

<p><a href="http://wiki.apache.org/nutch/AboutPlugins">AboutPlugins</a>这里列出来的是 Nuch 1.4的扩展点，有点过时了。</p>

<h2 id="nutch-1">一个Nutch的组成文件</h2>
<p>build.xml, plugins.xml 等等</p>

<h2 id="nutch-">Nutch 插件例子</h2>

<ol>
  <li><a href="http://wiki.apache.org/nutch/WritingPluginExample">WritingPluginExample</a></li>
  <li><a href="http://wiki.apache.org/nutch/WritingPluginExample-1.2">WritingPluginExample-1.2</a>，针对Nutch 1.2的，有点老，但是值得一看</li>
  <li><a href="http://www.ryanpfister.com/2009/04/how-to-sort-by-date-with-nutch/">Writing a plugin to add dates by Ryan Pfister</a></li>
</ol>

<p>看来这3个例子，你应该就知道怎么开发插件了。</p>

<h2 id="nutch--1">Nutch 的缺点</h2>
<p>在抓取的过程中，真正的难度在于, ip limit 和 user limit，可惜 Nutch 对这两个问题都没有解决方案。</p>

<ol>
  <li>Nutch 的 <a href="http://wiki.apache.org/nutch/HttpPostAuthentication">HttpPostAuthentication</a> 现在还没有开发完，导致无法抓取需要登录的网站，例如新浪微波，豆瓣等UGC网站，都是需要登录的。没有这个<code>HttpPostAuthentication</code>，Nutch其实只能抓取不需要登录的网页，适用范围大打折扣，现在是web 2.0时代，真正优质的内容，几乎都是需要登录的。</li>
  <li>多个代理的管理。Nutch 没有提供多个代理的管理功能，只能在<code>nutch-site.xml</code>里配置一个代理。比如我在网上抓取了几百个免费的http代理，怎么让Nutch的各个线程均匀的使用这些代理，平能自动判断代理的速度，优先选择速度高的代理呢？</li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CentOS上编译 Hadoop 2.2.0]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140214"/>
    <updated>2014-02-14T11:56:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/compile-hadoop-220-on-centos</id>
    <content type="html"><![CDATA[<p>下载了Hadoop预编译好的二进制包，hadoop-2.2.0.tar.gz，启动起来后，总是出现这种警告：</p>

<pre><code>WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</code></pre>

<p>原因是apache官网提供的二进制包，里面的native库，是32位的，坑跌啊，现在服务器谁还有32位的啊。</p>

<pre><code>$ file $HADOOP_PREFIX/lib/native/libhadoop.so.1.0.0
libhadoop.so.1.0.0: ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, BuildID[sha1]=0x9eb1d49b05f67d38454e42b216e053a27ae8bac9, not stripped
</code></pre>

<p>我们需要下载Hadoop 2.2.0源码，在 64 位Linux下重新编译，然后把32位的native库用64位的native库替换。</p>

<!-- more -->

<h2 id="hadoop-220-">1. 下载Hadoop 2.2.0 源码包，并解压</h2>

<pre><code>$ wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.2.0/hadoop-2.2.0-src.tar.gz
$ tar zxf hadoop-2.2.0-src.tar.gz
</code></pre>

<h2 id="section">2. 安装下面的软件</h2>

<pre><code> $ sudo yum install lzo-devel  zlib-devel  gcc autoconf automake libtool   ncurses-devel openssl-deve
</code></pre>

<h2 id="maven">3. 安装Maven</h2>
<p>不要使用最新的Maven 3.1.1。Hadoop 2.2.0的源码与Maven3.x存在兼容性问题，所以会出现</p>

<pre><code>java.lang.NoClassDefFoundError: org/sonatype/aether/graph/DependencyFilter
</code></pre>

<p>之类的错误。</p>

<p>安装 Maven 3.0.5</p>

<pre><code>$ wget http://mirror.esocc.com/apache/maven/maven-3/3.0.5/binaries/apache-maven-3.0.5-bin.tar.gz
$ sudo tar zxf apache-maven-3.0.5-bin.tar.gz -C /opt
$ sudo vim /etc/profile
export MAVEN_HOME=/opt/apache-maven-3.0.5
export PATH=$PATH:$MAVEN_HOME/bin
</code></pre>

<p>注销并重新登录，让环境变量生效。</p>

<h2 id="ant">4. 安装Ant</h2>

<pre><code>$ wget http://apache.dataguru.cn//ant/binaries/apache-ant-1.9.3-bin.tar.gz
$ sudo tar zxf apache-ant-1.9.3-bin.tar.gz -C /opt
$ sudo vim /etc/profile
export ANT_HOME=/opt/apache-ant-1.9.3
export PATH=$PATH:$ANT_HOME/bin
</code></pre>

<h2 id="findbugs">5. 安装Findbugs</h2>

<pre><code>$ wget http://prdownloads.sourceforge.net/findbugs/findbugs-2.0.3.tar.gz?download
$ sudo tar zxf findbugs-2.0.3.tar.gz -C /opt
$ sudo vim /etc/profile
export FINDBUGS_HOME=/opt/findbugs-2.0.3
export PATH=$PATH:$FINDBUGS_HOME/bin
</code></pre>

<h2 id="protobuf">6. 安装protobuf</h2>
<p>编译Hadoop 2.2.0，需要protobuf的编译器protoc。一定需要protobuf 2.5.0以上，yum里的是2.3，太老了。因此下载源码，编译安装。</p>

<pre><code>$ wget https://protobuf.googlecode.com/files/protobuf-2.5.0.tar.gz
$ tar zxf protobuf-2.5.0.tar.gz
$ cd protobuf-2.5.0
$ ./configure
$ make
$ sudo make install
</code></pre>

<h2 id="hadooppatch">7. 给Hadoop源码打一个patch</h2>
<p>最新的Hadoop 2.2.0 的Source Code 压缩包解压出来的code有个bug 需要patch后才能编译。否则编译hadoop-auth 会提示下面错误：</p>

<pre><code>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:testCompile (default-testCompile) on project hadoop-auth: Compilation failure: Compilation failure:
[ERROR] /home/chuan/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java:[84,13] cannot access org.mortbay.component.AbstractLifeCycle
[ERROR] class file for org.mortbay.component.AbstractLifeCycle not found
</code></pre>

<p>Patch: <a href="https://issues.apache.org/jira/browse/HADOOP-10110">https://issues.apache.org/jira/browse/HADOOP-10110</a></p>

<h2 id="hadoop">8. 编译 Hadoop</h2>

<pre><code>cd hadoop-2.2.0-src
mvn package -DskipTests -Pdist,native -Dtar
</code></pre>

<h2 id="native">9. 替换掉32位的native库</h2>
<p>用 <code>hadoop-2.2.0-src/hadoop-dist/target/hadoop-2.2.0/lib/native</code> 替换掉 <code>hadoop-2.2.0/lib/native</code>。</p>

<pre><code>rm -rf ~/local/opt/hadoop-2.2.0/lib/native
cp ./hadoop-dist/target/hadoop-2.2.0/lib/native ~/local/opt/hadoop-2.2.0/lib/
</code></pre>

<p>然后重启Hadoop集群，会看到控制台下不再有警告信息了。</p>

<h2 id="ubuntu">10 解决Ubuntu下启动失败的问题</h2>
<p>在Ubuntu上，那就不是一点WARN了，而是启动不起来，会出错，原因在于，在<code>./sbin/start-dfs.sh</code>第55行，</p>

<pre><code>NAMENODES=$($HADOOP_PREFIX/bin/hdfs getconf -namenodes)
</code></pre>

<p>在shell里单独运行这样命令，</p>

<pre><code>./bin/hdfs getconf -namenodes

OpenJDK 64-Bit Server VM warning: You have loaded library /home/soulmachine/local/opt/hadoop-2.2.0/lib/native/libhadoop.so which might have disabled stack guard. The VM will try to fix the stack guard now.
It's highly recommended that you fix the library with 'execstack -c &lt;libfile&gt;', or link it with '-z noexecstack'.
14/02/14 13:14:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
localhost
</code></pre>

<p>最后一行的localhost，才是有效的namenode，但是由于前面有一大堆warning，脚本把这一大堆字符串，按空格隔开，每个单词都看作是namenode，接下来就错的稀里哗啦。</p>

<p>根本原因，还是因为32位native库。</p>

<p>把自带的32位native目录删除，用编译好的64位native目录拷贝过去，再运行</p>

<pre><code>./bin/hdfs getconf -namenodes
localhost
</code></pre>

<p>这下就对了！</p>

<h2 id="section-1">参考资料</h2>

<ol>
  <li><a href="http://blog.csdn.net/lalaguozhe/article/details/10580727">YARN加载本地库抛出Unable to load native-hadoop library解决办法</a></li>
  <li><a href="http://blog.csdn.net/zwj0403/article/details/16855555">CentOS编译Hadoop 2.2.0 Pass 总结</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在CentOS上安装HBase 0.96]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140208"/>
    <updated>2014-02-08T16:42:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/install-hbase-on-centos</id>
    <content type="html"><![CDATA[<p>环境：CentOS 6.5, jdk 1.7, HBase 0.96.1.1</p>

<h2 id="ssh">（可选）创建新用户，并配置好SSH无密码登录</h2>
<p>一般我倾向于把需要启动daemon进程，对外提供服务的程序，即服务器类的程序，安装在单独的用户下面。这样可以做到隔离，运维方面，安全性也提高了。</p>

<p>创建一个新的group,</p>

<pre><code>$ sudo groupadd hbase
</code></pre>

<p>创建一个新的用户，并加入group,</p>

<pre><code>$ sudo useradd -g hbase hbase
</code></pre>

<p>给新用户设置密码，</p>

<pre><code>$ sudo passwd hbase
</code></pre>

<p>在每台机器上创建hbase新用户，并配置好SSH无密码，参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120102/">SSH无密码登录的配置</a></p>

<h2 id="standalone-mode">1. 单机模式(Standalone mode)</h2>

<h3 id="section">1.1 下载，解压</h3>

<pre><code>$ wget wget http://mirror.esocc.com/apache/hbase/hbase-0.96.1.1/hbase-0.96.1.1-hadoop2-bin.tar.gz
$ tar zxf hbase-0.96.1.1-hadoop2-bin.tar.gz -C ~/local/opt
</code></pre>

<h3 id="hbase-envsh">1.2 hbase-env.sh</h3>
<p>在这个文件中要指明JDK 安装在了哪里</p>

<pre><code>$ echo $JAVA_HOME
/usr/lib/jvm/java
$ vim conf/hbase-env.sh
</code></pre>

<p>取消<code>JAVA_HOME</code>那一行的注释，设置正确的JDK位置</p>

<pre><code>export JAVA_HOME=/usr/lib/jvm/java
</code></pre>

<h3 id="confhbase-sitexml">1.3 修改 conf/hbase-site.xml</h3>
<p>内容如下</p>

<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;/home/hbase/local/var/hbase&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
    &lt;value&gt;/home/hbase/local/var/zookeeper&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p><code>hbase.rootdir</code>目录是用来存放HBase的相关信息的，默认值是<code>/tmp/hbase-${user.name}/hbase</code>； <code>hbase.zookeeper.property.dataDir</code>目录是用来存放zookeeper（HBase内置了zookeeper）的相关信息的，默认值是<code>/tmp/hbase-${user.name}/zookeeper</code>。</p>

<h3 id="section-1">1.4 启动</h3>

<pre><code>$ ./bin/start-hbase.sh
starting Master, logging to logs/hbase-user-master-example.org.out
</code></pre>

<!-- more -->

<h3 id="hbase-shell">1.5 试用一下HBase shell</h3>
<p>$ ./bin/hbase shell
    2014-02-09 23:56:28,637 INFO  [main] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
    HBase Shell; enter ‘help<return>&#8217; for list of supported commands.
    Type &#8220;exit<return>&#8221; to leave the HBase Shell
    Version 0.96.1.1-hadoop2, rUnknown, Tue Dec 17 12:22:12 PST 2013</return></return></p>

<pre><code>hbase(main):001:0&gt;
</code></pre>

<p>创建一张名字为<code>test</code>的表，只有一个列，名为<code>cf</code>。为了验证创建是否成功，用<code>list</code>命令查看所有的table，并用<code>put</code>命令插入一些值。</p>

<pre><code>hbase(main):003:0&gt; create 'test', 'cf'
0 row(s) in 1.2200 seconds
hbase(main):003:0&gt; list 'test'
..
1 row(s) in 0.0550 seconds
hbase(main):004:0&gt; put 'test', 'row1', 'cf:a', 'value1'
0 row(s) in 0.0560 seconds
hbase(main):005:0&gt; put 'test', 'row2', 'cf:b', 'value2'
0 row(s) in 0.0370 seconds
hbase(main):006:0&gt; put 'test', 'row3', 'cf:c', 'value3'
0 row(s) in 0.0450 seconds
</code></pre>

<p>用<code>scan</code>命令扫描table，验证一下刚才的插入是否成功。</p>

<pre><code>hbase(main):007:0&gt; scan 'test'
ROW        COLUMN+CELL
row1       column=cf:a, timestamp=1288380727188, value=value1
row2       column=cf:b, timestamp=1288380738440, value=value2
row3       column=cf:c, timestamp=1288380747365, value=value3
3 row(s) in 0.0590 seconds
</code></pre>

<p>现在，disable并drop掉你的表，这会把上面的所有操作清零。</p>

<pre><code>hbase(main):012:0&gt; disable 'test'
0 row(s) in 1.0930 seconds
hbase(main):013:0&gt; drop 'test'
0 row(s) in 0.0770 seconds 
</code></pre>

<p>退出shell，</p>

<pre><code>hbase(main):014:0&gt; exit
</code></pre>

<h3 id="section-2">1.6 停止</h3>

<pre><code>$ ./bin/stop-hbase.sh
stopping hbase...............
</code></pre>

<h2 id="pseudo-distributed-mode">2 伪分布式模式(Pseudo-distributed mode)</h2>

<h3 id="section-3">前提</h3>

<p>HBase集群需要一个正在运行的zookeeper集群，要么用自带的，要么用外部的。</p>

<p>用自带的很方便，不需要任何其他操作。</p>

<p>如果用外部的，要先安装并启动一个ZK集群，参考我的这篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20140207">在CentOS上安装ZooKeeper集群</a>。并在 conf/hbase-env.sh 里，设置<code>HBASE_MANAGES_ZK=false</code>。这个值默认为true，HBase自带了一个zk，启动HBase的时候也会先启动zk，如果把这个值设置为false，那么HBase就不会自己管理zk集群了。</p>

<p>一般用外部的zk。因为一般情况下，公司会在集群上安装好zookeeper集群，然后多个项目共用一个zk集群，有利于提高资源利用率。</p>

<p>HBase还需要一个正在运行的HDFS集群，如何搭建请参考我的这篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20140205">在CentOS上安装Hadoop 2.x 集群</a>。</p>

<h3 id="sshlocalhost">2.1 设置SSH无密码登录localhost</h3>
<p>先检查一下是能够无密码登录本机，</p>

<pre><code>ssh localhost
</code></pre>

<p>如果提示输入密码，说明不能，按如下步骤设置。</p>

<pre><code>$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa 
$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys
</code></pre>

<h3 id="section-4">2.2 下载，解压</h3>

<pre><code>$ wget wget http://mirror.esocc.com/apache/hbase/hbase-0.96.1.1/hbase-0.96.1.1-hadoop2-bin.tar.gz
$ tar zxf hbase-0.96.1.1-hadoop2-bin.tar.gz -C ~/local/opt
</code></pre>

<h3 id="hbase-envsh-1">2.3 hbase-env.sh</h3>
<p>在这个文件中要指明JDK 安装在了哪里</p>

<pre><code>$ echo $JAVA_HOME
/usr/lib/jvm/java
$ vim conf/hbase-env.sh
</code></pre>

<p>取消<code>JAVA_HOME</code>那一行的注释，设置正确的JDK位置</p>

<pre><code>export JAVA_HOME=/usr/lib/jvm/java
</code></pre>

<h3 id="confhbase-sitexml-1">2.4 conf/hbase-site.xml</h3>
<p>内容如下</p>

<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;/home/hadoop/local/var/hadoop/hbase&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;
    &lt;value&gt;2181&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;zk01, zk02, zk03&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
    &lt;value&gt;/home/zookeeper/local/var/zookeeper&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p><code>hbase.rootdir</code>是HBase存放数据的目录，这个值应该从Hadoop集群的core-site.xml里的<code>fs.defaultFS</code>或<code>fs.default.name</code>拷贝过来。</p>

<p>接下来关于ZooKeeper的三项配置都是从ZooKeeper集群的zoo.cfg里拷贝过来的。</p>

<h3 id="section-5">2.5 启动</h3>

<pre><code>$ ./bin/start-hbase.sh
starting Master, logging to logs/hbase-user-master-example.org.out
</code></pre>

<p>查看一下进程，</p>

<pre><code>$ jps
26142 HMaster
26255 HRegionServer
26360 Jps
</code></pre>

<p>启动了一个HMaster和一个HRegionServer。</p>

<h3 id="hbase-shell-1">2.6 试用一下HBase shell</h3>
<p>见第1.5节。</p>

<h3 id="section-6">2.7 停止</h3>

<pre><code>$ ./bin/stop-hbase.sh
stopping hbase...............
</code></pre>

<h2 id="fully-distributed-mode">3 完全分布式模式(Fully-distributed mode)</h2>

<h3 id="section-7">3.1 准备3台机器</h3>
<p>跟这篇文章<a href="http://www.yanjiuyanjiu.com/blog/20140205/">在CentOS上安装Hadoop 2.x 集群</a>的第2.1节很类似。</p>

<p>设3台机器的hostname分别是master, slave01, slave02, master作为HMaster，而slave01,slave02作为HRegionServer。</p>

<h3 id="master-master">3.2 配置 master 无密码登陆到所有机器（包括master自己登陆自己）</h3>
<p>参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120102/">SSH无密码登录的配置</a></p>

<h3 id="hbase">3.3 把HBase压缩包上传到所有机器，并解压</h3>
<p>将 hbase-0.96.1.1-hadoop2-bin.tar.gz 上传到所有机器，然后解压。<strong>注意，所有机器的hbase路径必须一致，因为master会登陆到slave上执行命令，master认为slave的hbase路径与自己一样。</strong></p>

<p>下面开始配置，配置好了后，把<code>conf/</code>目录scp到所有其他机器。</p>

<h3 id="section-8">3.4 修改配置文件</h3>
<p>在第2节的基础上，增加下列修改。</p>

<h4 id="confregionservers">3.4.1 conf/regionservers</h4>
<p>在这个文件里面添加slave，一行一个。</p>

<pre><code>slave01
slave01
</code></pre>

<h4 id="confslaves">3.4.2 将conf/目录拷贝到所有slaves</h4>

<pre><code>$ scp -r conf/ hbase@slave01:$HBASE_HOME/
$ scp -r conf/ hbase@slave02:$HBASE_HOME/
</code></pre>

<h3 id="hbase-1">3.5 启动HBase集群</h3>

<h4 id="section-9">3.5.1 启动</h4>
<p>在master上执行：</p>

<pre><code>$ ./bin/start-hbase.sh
</code></pre>

<h4 id="section-10">3.5.2 检查是否启动成功</h4>
<p>用<code>jps</code>查看java进程。</p>

<p>在master上，应该有一个HMaster进程，在每台slave上，应该有一个HRegionServer进程。</p>

<h4 id="web-ui">3.5.3 Web UI</h4>

<ul>
  <li>HMaster: <a href="http://master:60010">http://master:60010</a></li>
  <li>HRegionServer: <a href="http://slave:60030">http://slave:60030</a></li>
</ul>

<h2 id="section-11">4 客户端</h2>
<p>想要在另一台机器，或者另一个用户下访问HBase，怎么办？把hbase的安装目录整个拷贝过来即可，不用任何配置（跟Hadoop想比简单多了）。</p>

<p>运行<code>./bin/hbase shell</code>，就可以使用HBase集群了。</p>

<h2 id="section-12">参考资料</h2>

<ol>
  <li><a href="http://hbase.apache.org/book/quickstart.html">1.2. Quick Start</a></li>
  <li><a href="http://hbase.apache.org/book/standalone_dist.html">2.2 HBase run modes: Standalone and Distributed</a></li>
  <li><a href="http://hbase.apache.org/book/example_config.html">2.4. Example Configurations</a></li>
  <li><a href="http://hbase.apache.org/book/zookeeper.html">Chapter 17. ZooKeeper</a></li>
  <li><a href="http://blog.csdn.net/iam333/article/details/16358087">CentOS分布式环境安装HBase-0.96.0</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在CentOS上安装ZooKeeper集群]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140207"/>
    <updated>2014-02-07T23:40:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/install-zookeeper-on-centos</id>
    <content type="html"><![CDATA[<p>环境：CentOS 6.5, jdk 1.7, ZooKeeper 3.4.5</p>

<p>本文主要参考官网的<a href="http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html">Getting Started</a></p>

<h2 id="section">（可选）创建新用户</h2>
<p>一般我倾向于把需要启动daemon进程，对外提供服务的程序，即服务器类的程序，安装在单独的用户下面。这样可以做到隔离，运维方面，安全性也提高了。</p>

<p>创建一个新的group,</p>

<pre><code>$ sudo groupadd zookeeper
</code></pre>

<p>创建一个新的用户，并加入group,</p>

<pre><code>$ sudo useradd -g zookeeper zookeeper
</code></pre>

<p>给新用户设置密码，</p>

<pre><code>$ sudo passwd zookeeper
</code></pre>

<h2 id="standalone-mode">1. 单机模式(Standalone mode)</h2>
<p>单机模式在开发和调试阶段很有用。</p>

<h3 id="section-1">1.1 下载，解压</h3>

<pre><code>$ wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.5/zookeeper-3.4.5.tar.gz
$ tar zxf zookeeper-3.4.5.tar.gz -C ~/local/opt
</code></pre>

<h3 id="section-2">1.2 启动</h3>
<p>默认就是单机模式，</p>

<pre><code>$ mv conf/zoo_sample.cfg conf/zoo.cfg
$ ./bin/zdServer.sh start
</code></pre>

<h3 id="java-zookeeper">1.3 使用java 客户端连接ZooKeeper</h3>

<pre><code>$ ./bin/zkCli.sh -server 127.0.0.1:2181
</code></pre>

<p>然后就可以使用各种命令了，跟文件操作命令很类似，输入<code>help</code>可以看到所有命令。</p>

<h4 id="section-3">1.4 关闭</h4>

<pre><code>$ ./bin/zdServer.sh stop
</code></pre>

<h2 id="replicated-mode">2. 分布式模式(Replicated mode)</h2>
<p>在生产环境中，要配置成分布式模式，才能发挥威力。</p>

<!-- more -->

<p>ZooKeeper集群一般被称为ZooKeeper ensemble，或者  quorum.</p>

<h3 id="section-4">2.1 准备3台机器</h3>
<p>假设有三台机器，hostname和ip对应关系是：</p>

<pre><code>192.168.1.131 zk01
192.168.1.132 zk02
192.168.1.133 zk03
</code></pre>

<p>ZooKeeper不存在明显的master/slave关系，各个节点都是服务器，leader挂了，会立马从follower中选举一个出来作为leader.</p>

<p>由于没有主从关系，也不用配置SSH无密码登录了，各个zk服务器是自己启动的，互相之间通过TCP端口来交换数据。</p>

<h3 id="confzoocfg">2.2 修改配置文件conf/zoo.cfg</h3>

<pre><code>tickTime=2000
initLimit=10
syncLimit=5
dataDir=/home/zookeeper/local/var/zookeeper
clientPort=2181
server.1=zk01:2888:3888
server.2=zk02:2888:3888
server.3=zk03:2888:3888
</code></pre>

<p>我一般把服务器程序，即需要启动daemon进程的程序，放在单独的用户里安装；且用户的数据，放在<code>local/var</code>下面，所以我的dataDir是<code>/home/zookeeper/local/var/zookeeper</code>。</p>

<h3 id="myid">2.3 myid文件</h3>
<p>要在每台机器的dataDir下，新建一个myid文件，里面存放一个数字，用来标识当前主机。</p>

<pre><code>zookeeper@zk01:$ echo "1" &gt;&gt; ~/local/var/zookeeper/myid
zookeeper@zk02:$ echo "2" &gt;&gt; ~/local/var/zookeeper/myid
zookeeper@zk03:$ echo "3" &gt;&gt; ~/local/var/zookeeper/myid
</code></pre>

<h3 id="section-5">2.4 启动每台机器</h3>

<pre><code>zookeeper@zk01:$ ~/local/opt/zookeeper-3.4.5/bin/zkServer.sh start
zookeeper@zk02:$ ~/local/opt/zookeeper-3.4.5/bin/zkServer.sh start
zookeeper@zk03:$ ~/local/opt/zookeeper-3.4.5/bin/zkServer.sh start
</code></pre>

<p>因为3个节点的启动是有顺序的，所以在陆续启动三个节点的时候，前面先启动的节点连接未启动的节点的时候会报出一些错误。可以忽略。</p>

<h3 id="section-6">2.5 查看状态</h3>

<pre><code>$ ~/local/opt/zookeeper-3.4.5/bin/zkServer.sh status
</code></pre>

<h2 id="javazookeeper">3 使用java客户端连接ZooKeeper集群</h2>

<p>找一台机器，解压zookeeper压缩包，不用配置，就可以使用java客户端连接ZooKeeper集群中的任意一台服务器了。</p>

<pre><code>$ ./bin/zkCli.sh -server zk01:2181
$ ./bin/zkCli.sh -server zk01:2181
$ ./bin/zkCli.sh -server zk01:2181
</code></pre>

<p>连接上以后，就可以执行各种命令，使用<code>help</code>查看帮助。</p>

<h2 id="section-7">参考资料</h2>

<ol>
  <li><a href="http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html">Getting Started</a></li>
  <li><a href="http://blog.csdn.net/jmy99527/article/details/17582349">Zookeeper 3.4.5 集群安装笔记</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop多用户的配置(Hadoop 2.x)]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140206"/>
    <updated>2014-02-06T10:05:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/hadoop-multiple-users</id>
    <content type="html"><![CDATA[<p>假设我们以名为hadoop的用户，建好了集群，见<a href="http://www.yanjiuyanjiu.com/blog/20140205/">在CentOS上安装Hadoop 2.x 集群</a>。通常，我们会把这个集群共享给多个用户，而不是让大家都登录为hadoop，这样做有几个好处：</p>

<ul>
  <li>一个用户不能修改另一个用户的的文件</li>
  <li>在hadoop web管理页面，可以很方便的看到不同的用户的job</li>
</ul>

<p>现在集群中有一台机器，上面有一个用户名为 hbase 的用户，他想要使用hadoop集群，怎么配置呢？</p>

<h2 id="hadoop">1. 安装hadoop客户端</h2>

<h3 id="section">1.1 下载，解压</h3>
<p>下载跟hadoop集群一样的hadoop软件包，并解压，</p>

<pre><code>$ wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.2.0/hadoop-2.2.0.tar.gz
$ tar -zxf hadoop-2.2.0.tar.gz -C ~/local/opt
$ cd ~/local/opt/hadoop-2.2.0
</code></pre>

<h3 id="hadoop-1">1.2 拷贝Hadoop集群的配置文件</h3>
<p>将Hadoop集群的配置文件全部拷贝到客户端，相当与把集群的信息告诉客户端。</p>

<pre><code>$ scp -r hadoop@localhost:~/local/opt/hadoop-2.2.0/etc/hadoop ./etc/
</code></pre>

<p>修改conf/mapred-site.xml中的<code>mapreduce.cluster.local.dir</code>，改为本机上的某个目录，确保这个目录存在且有写权限。因为这个目录是本地目录，每台机器都可以不同。例如我的是：</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;mapreduce.cluster.local.dir&lt;/name&gt;
  &lt;value&gt;/home/soulmachine/local/var/hadoop/mapred/local&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>确保这个目录存在，</p>

<pre><code>$ mkdir -p ~/local/var/hadoop/mapred/local
</code></pre>

<!-- more -->

<p>还有另一种方法，由于<code>mapreduce.cluster.local.dir</code>默认值是<code>${hadoop.tmp.dir}/mapred/local</code>，也可以通过修改<code>hadoop.tmp.dir</code>达到目的，在<code>core-site.xml</code>中，确保<code>${hadoop.tmp.dir}/mapred/local</code>存在且有写权限。</p>

<h2 id="master">2. 在master上配置权限</h2>
<p>以下操作均在hadoop集群的 namenode 这台机器上进行，且登录为hadoop，因为hadoop这个用户是整个hadoop集群权限最高的用户（但对于Linux系统本身，这个用户其实没有sudo权限）。</p>

<p>Hadoop关于用户权限方面，有很多高级的配置，这里我们简单的利用HDFS本身的文件权限检查机制，来配置多用户。</p>

<p>HDFS本身没有提供用户名、用户组的创建，在客户端调用hadoop 的文件操作命令时，hadoop 识别出执行命令所在进程的linux系统的用户名和用户组，然后使用这个用户名和组来检查文件权限。 用户名=linux命令中的<code>whoami</code>，而组名等于<code>groups</code>。 </p>

<p>启动hadoop hdfs系统的用户即为超级用户（在这里就是名为hadoop的这个用户），可以进行任意的操作。</p>

<p>在客户端机器上，用gropus命令看一下hbase所在的组，</p>

<pre><code>$ groups
hbase
</code></pre>

<p>说明hbase这个用户所在的组为hbase。</p>

<h3 id="home">2.1 为客户端用户创建home文件夹</h3>

<pre><code>$ hdfs dfs -mkdir /user/hbase
$ hdfs dfs -chown hbase /user/hbase
$ hdfs dfs -chgrp hbase /user/hbase
</code></pre>

<h3 id="hdfstmp">2.2 设置HDFS上的/tmp目录的权限</h3>
<p>客户端提交job的时候，需要往/tmp里写入文件，因此最好把/tmp设置为所有用户都有可读、可写和可执行的权限。 </p>

<pre><code>$ hdfs dfs -chmod -R 777 /tmp
</code></pre>

<h3 id="mapreducejobtrackerstagingrootdir">2.3 设置mapreduce.jobtracker.staging.root.dir</h3>
<p>客户端向集群提交任务时，需要把该job需要的文件打包，拷贝到HDFS上。在拷贝之前，得先确定这些资源文件存放在HDFS的什么地方。JobTracker设置有一个工作目录(Staging area, 也称数据中转站)，用来存储与每个job相关的数据。这个目录的前缀由<code>mapreduce.jobtracker.staging.root.dir</code> 参数来指定，默认是<code>${hadoop.tmp.dir}/mapred/staging</code>，每个client user可以提交多个job，在这个目录后就得附加user name的信息。所以这个工作目录(Staging area)默认是<code>${hadoop.tmp.dir}/mapred/staging/denny/.staging/</code>。</p>

<p>一般把前缀设置为<code>/user</code>，这是官方推荐的，见 <a href="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default.xml</a> 里的<code>mapreduce.jobtracker.staging.root.dir</code>处：</p>

<blockquote>
  <p>The root of the staging area for users’ job files In practice, this should be the directory where users’ home directories are located (usually /user)</p>
</blockquote>

<pre><code>#以hadoop用户登录jobtracker机器
$ vim conf/mapred-site.xml
&lt;property&gt;
  &lt;name&gt;mapreduce.jobtracker.staging.root.dir&lt;/name&gt;
  &lt;value&gt;/user&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h3 id="hadoop-2">2.4 重启hadoop集群</h3>
<p>将配置文件scp到所有机器，然后重启集群，</p>

<pre><code>$ ./sbin/stop-yarn.sh
$ ./sbin/start-yarn.sh
$ ./sbin/stop-dfs.sh
$ ./sbin/start-dfs.sh
</code></pre>

<h2 id="section-1">3. 测试一下</h2>
<p>回到客户端机器。</p>

<p>将输入数据拷贝到分布式文件系统中:</p>

<pre><code>$ ./bin/hdfs dfs -put /etc/hadoop input
$ ./bin/hdfs dfs -ls input
</code></pre>

<p>运行 Hadoop 自带的例子:</p>

<pre><code>$ ./bin/$ hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount input output
</code></pre>

<p>查看输出文件:</p>

<pre><code>$ hdfs dfs -lsr output
$ hdfs dfs -cat output/part-r-00000
</code></pre>

<p>如果能看到结果，说明这个例子运行成功。</p>

<h3 id="binpath">4 （可选）将bin目录加入PATH</h3>
<p>这样就不用每次都cd到Hadoop目录，执行命令了。</p>

<p>在 <code>~/.bashrc</code>中添加如下4行：</p>

<pre><code>export HADOOP_PREFIX=$HOME/local/opt/hadoop-2.2.0
export PATH=$PATH:$HADOOP_PREFIX/bin
alias hls="hdfs dfs -ls"
</code></pre>

<p>source使之立刻生效，</p>

<pre><code>$ source ~/.bashrc
</code></pre>

<h2 id="section-2">参考资料</h2>

<ol>
  <li><a href="http://blog.csdn.net/j3smile/article/details/7887826">hadoop远程客户端安装配置、多用户权限配置</a></li>
  <li><a href="http://blog.csdn.net/a999wt/article/details/8718707">hadoop如何创建多用户</a></li>
  <li><a href="http://blog.sina.com.cn/s/blog_605f5b4f0101897z.html">关于多用户时hadoop的权限问题</a></li>
  <li><a href="http://langyu.iteye.com/blog/909170">MapReduce: Job提交过程</a></li>
  <li><a href="http://www.hadoopor.com/archiver/tid-481.html">hadoop中的dfs.name.dir,mapred.local.dir,mapred.system.dir和hadoop.tmp.dir说明</a></li>
  <li><a href="http://fenriswolf.me/2012/08/06/hadoop-%E5%8F%83%E6%95%B8%E8%A8%AD%E5%AE%9A-mapred-site-xml/">Hadoop 參數設定 – mapred-site.xml</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在CentOS上安装Hadoop 2.x 集群]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140205"/>
    <updated>2014-02-05T12:39:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/hadoop-2-installatioin-on-centos</id>
    <content type="html"><![CDATA[<p><strong>环境</strong>：CentOS 6.5, OPenJDK 1.7, Hadoop 2.2.0</p>

<p>本文主要参考官网的文档，<a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/SingleCluster.html">Hadoop 2.2.0 Single Node Setup</a>， <a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/ClusterSetup.html">Hadoop 2.2.0  Cluster Setup</a></p>

<h2 id="section">（可选）创建新用户</h2>
<p>一般我倾向于把需要启动daemon进程，对外提供服务的程序，简单的说，就是服务器类程序，安装在单独的用户下面。这样可以做到隔离，运维方面，安全性也提高了。</p>

<p>创建一个新的group,</p>

<pre><code>$ sudo groupadd hadoop
</code></pre>

<p>创建一个新的用户，并加入group,</p>

<pre><code>$ sudo useradd -g hadoop hadoop
</code></pre>

<p>给新用户设置密码，</p>

<pre><code>$ sudo passwd hadoop
</code></pre>

<h2 id="pseudo-distributed-mode">1 伪分布式模式(Pseudo-Distributed Mode)</h2>
<p>Hadoop能在单台机器上以伪分布式模式运行，即每个Hadoop模块运行在单独的java进程里。</p>

<h3 id="sshlocalhost">1.1 设置SSH无密码登录localhost</h3>
<p>先检查一下是能够无密码登录本机，</p>

<pre><code>ssh localhost
</code></pre>

<p>如果提示输入密码，说明不能，按如下步骤设置。</p>

<pre><code>$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa 
$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys
</code></pre>

<h3 id="section-1">1.2 下载已经编译好的二进制包，解压</h3>
<p>用浏览器下载或wget,</p>

<pre><code>$ wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.2.0/hadoop-2.2.0.tar.gz
$ tar -zxf hadoop-2.2.0.tar.gz -C ~/local/opt
$ cd ~/local/opt/hadoop-2.2.0
</code></pre>

<h3 id="section-2">1.3 设置环境变量</h3>

<pre><code>$ vim ~/.bashrc
export HADOOP_PREFIX=$HOME/local/opt/hadoop-2.2.0
export HADOOP_COMMON_HOME=$HADOOP_PREFIX
export HADOOP_HDFS_HOME=$HADOOP_PREFIX
export HADOOP_MAPRED_HOME=$HADOOP_PREFIX
export HADOOP_YARN_HOME=$HADOOP_PREFIX
export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop
export PATH=$PATH:$HADOOP_PREFIX/bin:$HADOOP_PREFIX/sbin
</code></pre>

<!-- more -->

<h3 id="section-3">1.4 修改配置文件</h3>
<p>配置文件的位置在 <code>$HADOOP_PREIFIX/etc/hadoop</code>下面。</p>

<h3 id="hadoop-envsh">1.4.1 hadoop-env.sh</h3>
<p>在这个文件中要告诉hadoop JDK 安装在了哪里</p>

<pre><code>$ echo $JAVA_HOME
/usr/lib/jvm/java
$ vim conf/hadoop-env.sh
</code></pre>

<p>取消<code>JAVA_HOME</code>那一行的注释，设置正确的JDK位置</p>

<pre><code>export JAVA_HOME=/usr/lib/jvm/java
</code></pre>

<h4 id="hdfs">1.4.2 HDFS的配置</h4>
<p>为了简单，我们仍采用Hadoop 1.x中的HDFS工作模式（不配置HDFS Federation）。</p>

<p>core-site.xml:</p>

<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://localhost&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>hdfs-site.xml:</p>

<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
    &lt;value&gt;file:///home/hadoop/local/var/hadoop/hdfs/datanode&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
    &lt;value&gt;file:///home/hadoop/local/var/hadoop/hdfs/namenode&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;
    &lt;value&gt;file:///home/hadoop/local/var/hadoop/hdfs/namesecondary&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>Hadoop会自动创建目录。</p>

<h4 id="yarn">1.4.3 YARN的配置</h4>
<p>yarn-site.xml，不用修改，保持为空。</p>

<h4 id="mapreduce">1.4.4 MapReduce的配置</h4>
<p>Yarn不仅仅只支持MapReduce这种计算模式，还支持Spar, Tez, MPI等框架，因此，要显式地配置Yarn，使其支持mapreduce。在Hadoop 1.x中，只支持MapReduce，因此需要而且必须配置mapred-site.xml，到了Hadoop 2.x，MapReduce的配置是可选的。</p>

<p>在yarn-site.xml中添加：</p>

<pre><code>&lt;property&gt;
   &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
   &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p><strong>解释</strong>：为了能够运行MapReduce程序，需要让各个NodeManager在启动时加载shuffle server，shuffle server实际上是Jetty/Netty Server，Reduce Task通过该server从各个NodeManager上远程拷贝Map Task产生的中间结果。上面增加的这个配置用于指定shuffle server。</p>

<p>将 mapred-site.xml.template 复制一份，重命名为mapred-site.xml。</p>

<p>mapred-site.xml:</p>

<pre><code>&lt;property&gt;
   &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
   &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p><strong>解释</strong>：用mapreduce.framework.name指定采用的框架名称，默认是将作业提交到MRv1的JobTracker端。</p>

<h3 id="section-4">1.5 测试</h3>

<h4 id="hdfs-1">1.5.1 启动HDFS</h4>

<pre><code>$ hdfs namenode -format
$ start-dfs.sh
</code></pre>

<h4 id="yarn-1">1.5.2 启动Yarn</h4>

<pre><code>$ start-yarn.sh
</code></pre>

<h4 id="mapreduce-1">1.5.3 启动MapReduce</h4>
<p>在Hadoop 2.x中，MapReduce Job不需要额外的daemon进程，在MapReduce Application Master启动的时候会自动启动JobTracker和TaskTracker进程，Job结束的时候会自动被关闭。</p>

<h4 id="distributedshell">1.5.4 运行一个DistributedShell的例子</h4>
<p>运行一个Hadoop自带的例子，名称为<code>DistributedShell</code>，可以同时在多台机器上运行shell命令。</p>

<pre><code>$ hadoop jar $HADOOP_PREFIX/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.2.0.jar org.apache.hadoop.yarn.applications.distributedshell.Client --jar $HADOOP_PREFIX/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.2.0.jar --shell_command date --num_containers 2
</code></pre>

<p>运行完成后，看看倒数第三行，有类似与<code>application_1391783685869_0001</code>的字符串，这是application ID。查看该application的每个container的输出，执行下面的命令，</p>

<pre><code>$ grep "" $HADOOP_PREFIX/logs/userlogs/&lt;APPLICATION ID&gt;/**/stdout
</code></pre>

<h4 id="wordcount">1.5.5 运行wordcount</h4>

<pre><code>$ cd $HADOOP_PREFIX
$ hdfs dfs -put ./etc/hadoop/ input
$ hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount input output
$ hdfs dfs -lsr output
$ hdfs dfs -cat output/part-r-00000
</code></pre>

<p>结束后，关闭 Hadoop:</p>

<pre><code>$ stop-dfs.sh
$ stop-yarn.sh
</code></pre>

<h2 id="fully-distributed-mode">2 分布式模式(Fully-Distributed Mode)</h2>

<h3 id="section-5">2.1 准备3台机器</h3>
<p>如果你已经有了三台机器，这一步可以省略。</p>

<p>如果没有，则可以用VMware Workstation 或 VirtualBox创建3台虚拟机。首先用vmware workstation 新建一台CentOS 6.5，装好操作系统，选择 Basic Server，安装JDK，参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120423/">安装和配置CentOS服务器的详细步骤</a>。安装好后然后用<strong>浅拷贝</strong><code>Create a linked clone</code> 克隆出2台，这样有了3台虚拟机。启动3台机器，假设IP分别为<code>192.168.1.131, 192.168.1.132, 192.168.1.133</code>, 131做为NameNode,JobTracker和SecondaryNameNode，身兼3个角色，这3个角色应该放到3台不同的机器上，这里为了简化，用一台机器来做3个角色；132和133为 slave。假设三台机器上的用户名是<code>hadoop</code>，也可以用其他用户名，但必须三台机器都相同。</p>

<h4 id="section-6">2.1.1 关闭防火墙</h4>
<p>临时关闭防火墙</p>

<pre><code>$ sudo service iptables stop
</code></pre>

<p>下次开机后，防火墙还是会启动。</p>

<p>永久关闭防火墙</p>

<pre><code>$ sudo chkconfig iptables off
</code></pre>

<p>由于这几台虚拟机是开发机，不是生产环境，因此不必考虑安全性，可以永久关闭防火墙，还能给开发阶段带来很多便利。</p>

<h4 id="hostname">2.1.2 修改hostname</h4>
<p>如果集群中的每一台机器事先已经有了hostname，则这一步可以跳过。</p>

<p>这一步看起来貌似不必要，其实是必须的，否则最后运行wordcount等例子时，会出现“Too many fetch-failures”。因为HDFS用hostname而不是IP，来相互之间进行通信（见后面的注意1）。</p>

<p>在CentOS上修改hostname，包含两个步骤(假设将hostname1改为hostname2，参考<a href="http://www.ichiayi.com/wiki/tech/linux_hostname">这里</a>，但不需要第一步)：</p>

<ol>
  <li>将 <code>/etc/sysconfig/network</code> 內的 HOSTNAME 改成 yourhostname</li>
  <li>用<code>hostname</code>命令，临时修改机器名， <code>sudo hostname yourhostname</code></li>
</ol>

<p>用<code>exit</code>命令退出shell，再次登录，命令提示字符串就会变成<code>[username@yourhostname ~]$</code>。</p>

<p>用上述方法，将131改名为master，132改名为slave01，133改名为slave02。</p>

<p><strong>注意</strong>，对于有的Ubuntu/Debia 系统，会把本机的hostname解析成 127.0.1.1，例如：</p>

<pre><code>127.0.0.1       localhost
127.0.1.1       master
</code></pre>

<p>将第二行改为(参考<a href="http://wiki.ubuntu.org.cn/%E5%88%A9%E7%94%A8Cloudera%E5%AE%9E%E7%8E%B0Hadoop">利用Cloudera实现Hadoop</a>)</p>

<pre><code>127.0.0.1       master
</code></pre>

<h4 id="etchosts">2.1.3 修改所有机器的<code>/etc/hosts</code>文件</h4>
<p>在所有机器的<code>/etc/hosts</code>文件中，添加所有hostname对应的IP，一般在一台机器上设置好，然后scp到所有机器。例如</p>

<pre><code>192.168.1.131 master
192.168.1.132 slave01
192.168.1.133 slave02
</code></pre>

<h3 id="master-master">2.2 配置 master 无密码登陆到所有机器（包括master自己登陆自己）</h3>
<p>参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120102/">SSH无密码登录的配置</a></p>

<h3 id="hadoop">2.3 把Hadoop压缩包上传到所有机器，并解压</h3>
<p>将 hadoop-1.2.1-bin.tar.gz 上传到所有机器，然后解压。<strong>注意，所有机器的hadoop路径必须一致，因为master会登陆到slave上执行命令，master认为slave的hadoop路径与自己一样。</strong></p>

<p>下面开始配置，配置好了后，把<code>./etc/hadoop</code>目录scp到所有其他机器。</p>

<h3 id="section-7">2.4 修改配置文件</h3>
<p>在第1节的基础上，增加下列修改。</p>

<h4 id="namenode">2.4.1 指定NameNode</h4>
<p>在core-site.xml中，<code>fs.defaultFS</code>要改为运行NameNode的那台机器的hostname，不再是localhost。</p>

<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://master&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<h4 id="resourcemanager">2.4.2 指定ResourceManager</h4>
<p>在yarn-site.xml中增加，</p>

<pre><code>&lt;property&gt;
   &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
   &lt;value&gt;master&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h4 id="slavenodemanager">2.4.3 添加Slave，即NodeManager</h4>
<p>在 <code>etc/hadoop/slaves</code>中添加，</p>

<pre><code>slave01
slave02
</code></pre>

<h4 id="hadooptmpdir">2.4.4 设置 hadoop.tmp.dir</h4>
<p>在core-site.xml里添加：</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
  &lt;value&gt;/home/hadoop/local/var/hadoop/tmp/hadoop-${user.name}&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h4 id="mapred-sitexml">2.4.4 修改mapred-site.xml</h4>
<p>添加如下内容：</p>

<pre><code>&lt;property&gt;
    &lt;name&gt;mapreduce.jobtracker.staging.root.dir&lt;/name&gt;
    &lt;value&gt;/user&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>这是为以后的多用户支持做准备。</p>

<h4 id="pid">2.4.4 设置pid文件的存放位置</h4>
<p>在hadoop-env.sh中添加</p>

<pre><code>export HADOOP_MAPRED_PID_DIR=/home/hadoop/local/var/hadoop/pids
</code></pre>

<p>在 mapred-env.sh中添加</p>

<pre><code>export HADOOP_PID_DIR=/home/hadoop/local/var/hadoop/pids
</code></pre>

<h4 id="dfsreplicationslave">2.4.5 将dfs.replication设置为slave的个数</h4>
<p>我们这里有2台slave，就设置为2。在hdfs-site.xml里添加：</p>

<pre><code>&lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;2&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h4 id="slaves">2.4.6 将配置文件拷贝到所有slaves</h4>

<pre><code>$ cd $HADOOP_PREFIX/etc/hadoop
$ scp hadoop-env.sh mapred-env.sh core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml slaves hadoop@slave01:$HADOOP_PREFIX/etc/hadoop
$ scp hadoop-env.sh mapred-env.sh core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml slaves hadoop@slave02:$HADOOP_PREFIX/etc/hadoop
</code></pre>

<h3 id="section-8">2.5 设置环境变量</h3>
<p>在所有机器上添加环境变量，与第1.3节相同。</p>

<h3 id="hadoop-1">2.6 启动 hadoop</h3>

<h4 id="hdfs-2">2.6.1 启动HDFS</h4>
<p>在NameNode这个机器（在这里是master）上执行下列命令，</p>

<pre><code>#只需一次，下次启动不再需要格式化，只需 start-dfs.sh
$ hdfs namenode -format
$ start-dfs.sh
</code></pre>

<h4 id="yarn-2">2.6.2 启动Yarn</h4>
<p>在ResourceManager这台机器（在这里仍然是master）上执行，</p>

<pre><code>$ start-yarn.sh
</code></pre>

<h4 id="mapreduce-2">2.6.3 启动MapReduce</h4>
<p>在Hadoop 2.x中，MapReduce Job不需要额外的daemon进程，在Job开始的时候，NodeManager会启动一个MapReduce Application Master（相当与一个精简的JobTracker），Job结束的时候自动被关闭。</p>

<h4 id="section-9">2.6.4 检查是否启动成功</h4>
<p>用<code>jps</code>查看java进程。</p>

<p>在master上，应该有三个进程，NameNode, SecondaryNameNode, ResourceManger；在每台slave上，应该有两个进程，DataNode, NodeManager。</p>

<h4 id="web-ui">2.6.5 Web UI</h4>
<p>可以用浏览器打开NameNode, ResourceManager和各个NodeManager的web界面，</p>

<ul>
  <li>NameNode web UI, <a href="http://master:50070/">http://master:50070/</a></li>
  <li>ResourceManager web UI, <a href="http://master:8088/">http://master:8088/</a></li>
  <li>NodeManager web UI, <a href="http://slave01:8042">http://slave01:8042</a></li>
</ul>

<p>还可以启动JobHistory Server，能够通过Web页面查看集群的历史Job，执行如下命令：</p>

<pre><code>mr-jobhistory-daemon.sh start historyserver
</code></pre>

<p>默认使用19888端口，通过访问<a href="http://master:19888/">http://master:19888/</a>查看历史信息。</p>

<p>终止JobHistory Server，执行如下命令：</p>

<pre><code>mr-jobhistory-daemon.sh stop historyserver
</code></pre>

<h3 id="wordcount-1">2.8 运行wordcount</h3>
<p>将输入数据拷贝到HDFS中:</p>

<pre><code>$ cd $HADOOP_PREFIX
$ hdfs dfs -put ./etc/hadoop input
</code></pre>

<p>这一步会报错，”No such file or directory”, 用<code>hdfs dfs -ls /</code>查看，是空的，难怪了。我们需要手动建立”/user/hadoop”目录，</p>

<pre><code>$ hdfs dfs -mkdir /user/hadoop
</code></pre>

<p>再上传文件，就可以了。</p>

<p>运行WordCount:</p>

<pre><code>$ hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount input output
</code></pre>

<p>查看结果：</p>

<pre><code>$ hdfs dfs -lsr output
$ hdfs dfs -cat output/part-r-00000
</code></pre>

<p>如果能看到结果，说明这个例子运行成功。</p>

<h3 id="hadoop-2">2.9 停止 hadoop集群</h3>
<p>在master上执行：</p>

<pre><code>$ stop-yarn.sh
$ stop-hdfs.sh
</code></pre>

<h2 id="section-10">4. 排除错误</h2>
<p>本文已经尽可能的把步骤详细列出来了，但是我相信大部分人不会一次成功。这时候，查找错误就很重要了。查找错误最重要的手段是查看hadoop的日志，一般在logs目录下。把错误消息复制粘贴到google，搜索一下，慢慢找错误。</p>

<h2 id="section-11">参考资料</h2>

<ol>
  <li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/core-default.xml">core-default</a>, <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default</a>, <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default</a>, <a href="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default</a></li>
  <li><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-install/">Hadoop YARN安装部署初探</a></li>
  <li><a href="http://www.alexjf.net/blog/distributed-systems/hadoop-yarn-installation-definitive-guide">Hadoop YARN Installation: The definitive guide</a></li>
  <li><a href="http://codesfusion.blogspot.com/2013/10/setup-hadoop-2x-220-on-ubuntu.html">Setup newest Hadoop 2.x (2.2.0) on Ubuntu</a></li>
  <li><a href="http://shiyanjun.cn/archives/561.html">Hadoop-2.2.0集群安装配置实践</a></li>
  <li><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-configurations-resourcemanager-nodemanager/">Hadoop YARN配置参数剖析(1)—RM与NM相关参数</a></li>
</ol>

<h2 id="section-12">相关文章</h2>

<ol>
  <li><a href="http://www.yanjiuyanjiu.com/blog/20140202">在CentOS上安装Hadoop集群</a></li>
  <li><a href="http://www.yanjiuyanjiu.com/blog/20120103/">在Ubuntu上安装Hadoop</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[把Nutch爬虫部署到Hadoop集群上]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140204"/>
    <updated>2014-02-04T22:36:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/running-nutch-on-hadoop-cluster</id>
    <content type="html"><![CDATA[<p>软件版本：Nutch 1.7, Hadoop 1.2.1, CentOS 6.5, JDK 1.7</p>

<p>前面的3篇文章中，<a href="http://www.yanjiuyanjiu.com/blog/20140121">Nutch 快速入门(Nutch 1.7)</a>，<a href="http://www.yanjiuyanjiu.com/blog/20140201">Nutch 快速入门(Nutch 2.2.1)</a>，<a href="http://www.yanjiuyanjiu.com/blog/20140120">在Eclipse里运行Nutch</a>，Nutch都是跑在单机上，本文把Nutch部署到Hadoop集群上，在真正的分布式Hadoop集群上跑。</p>

<h2 id="section">前提</h2>

<ul>
  <li>学会了搭建一个分布式Hadoop集群，见<a href="http://www.yanjiuyanjiu.com/blog/20140202">在CentOS上安装Hadoop集群</a></li>
  <li>学会了单机跑Nutch，见<a href="http://www.yanjiuyanjiu.com/blog/20140121">Nutch 快速入门(Nutch 1.7)</a></li>
</ul>

<h2 id="hadoop">1 启动Hadoop集群</h2>
<p>伪分布式或真分布式的Hadoop集群都可以，无所谓。</p>

<p>选择一台配置好了的Hadoop客户端的机器（见<a href="http://www.yanjiuyanjiu.com/blog/20140203">Hadoop多用户的配置</a>），作为客户机，以下操作均在这台客户机上进行。</p>

<h2 id="nutch">2 下载Nutch源码</h2>
<p>有两种方法，</p>

<ol>
  <li>去官网首页下载apache-nutch-1.7-src.tar.gz</li>
  <li>
    <p>用svn checkout</p>

    <pre><code> $ svn co https://svn.apache.org/repos/asf/nutch/tags/release-1.7
</code></pre>
  </li>
</ol>

<h2 id="hadoop6nutchconf">3 把Hadoop的6个配置文件拷贝到Nutch的conf/目录</h2>
<p>将Hadoop的六个配置文件，拷贝到Nutch的conf/目录，相当于把Hadoop集群的配置信息告诉Nutch，</p>

<!-- more -->

<p>在伪分布式模式下，</p>

<pre><code>$ cd ~/local/opt/hadoop-1.2.1/conf
$ cp hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml masters slaves /home/soulmachine/local/src/apache-nutch-1.7/conf
</code></pre>

<p>在分布式模式下，</p>

<pre><code>$ ssh hadoop@localhost
$ cd ~/local/opt/hadoop-1.2.1/conf
$ scp hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml masters slaves soulmachine@localhost:~/local/src/apache-nutch-1.7/conf
$ exit
</code></pre>

<h2 id="nutch-1">4 修改Nutch的配置文件</h2>
<p>修改 conf/nutch-site.xml:</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;http.agent.name&lt;/name&gt;
  &lt;value&gt;My Nutch Spider&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>修改 <code>regex-urlfilter.txt</code>, 见<a href="http://www.yanjiuyanjiu.com/blog/20140121/">Nutch 快速入门(Nutch 1.7)</a> 第4节，</p>

<pre><code>#注释掉这一行
# skip URLs containing certain characters as probable queries, etc.
#-[?*!@=]
# accept anything else
#注释掉这行
#+.
+^http:\/\/movie\.douban\.com\/subject\/[0-9]+\/(\?.+)?$
</code></pre>

<h2 id="nutch-2">5 重新编译Nutch</h2>
<p>每次修改了<code>$NUTCH_HOME/conf</code>下的的文件，都需要重新编译Nutch，重新打包生成一个nutch-x.x.x.job文件，见这里，<a href="http://wiki.apache.org/nutch/NutchHadoopSingleNodeTutorial">Running Nutch in (pseudo) distributed-mode</a>。也可以打开build.xml看看里面的”runtime”这个task干了什么，就明白了。</p>

<pre><code>$ ant runtime
</code></pre>

<p>这会在<code>runtime/deploy</code>下生成一个Job文件，<code>apache-nutch-1.7.job</code>，它本质上是一个zip压缩包，可以打开看一下它里面的内容。可以看到它包含了很多编译好的class文件，以及从conf/目录下的拷贝出来的xml配置文件。</p>

<h2 id="hadoopjob">6 向Hadoop集群提交Job，进行抓取</h2>

<p>首先，要在con/hadoop-env.sh 添加<code>HADOOP_CLASSPATH</code>，让Hadoop知道去哪里找Nutch所依赖的jar包，</p>

<pre><code>export HADOOP_CLASSPATH=/home/soulmachine/local/opt/apache-nutch-1.7/runtime/local/lib/*.jar
</code></pre>

<p>上传种子URL列表，</p>

<pre><code>$ hadoop fs -put ~/urls urls
$ hadoop fs -lsr urls
</code></pre>

<p>提交Job,</p>

<pre><code>$ hadoop jar ./runtime/deploy/apache-nutch-1.7.job org.apache.nutch.crawl.Crawl urls -dir TestCrawl -depth 1 -topN 5
</code></pre>

<p>可以打开web页面监控job的进度，</p>

<ul>
  <li>Jobtracer: <a href="http://master:50030">http://master:50030</a></li>
  <li>Namenode: <a href="http://master:50070">http://master:50070</a></li>
</ul>

<p>把Nutch运行在伪分布式Hadoop集群上，比Standalone模式要好，因为可以通过web页面监控job。</p>

<p>查看结果</p>

<pre><code>$ hadoop fs -ls TestCrawl

Found 3 items
drwxr-xr-x   - soulmachine supergroup          0 2014-02-04 02:17 /user/soulmachine/TestCrawl/crawldb
drwxr-xr-x   - soulmachine supergroup          0 2014-02-04 02:18 /user/soulmachine/TestCrawl/linkdb
drwxr-xr-x   - soulmachine supergroup          0 2014-02-04 02:16 /user/soulmachine/TestCrawl/segments
</code></pre>

<h2 id="section-1">7 注意</h2>
<p>如果出现<code>java.io.IOException: No valid local directories in property: mapred.local.dir</code>的错误，说明你的客户机的mapred-site.xml是从hadoop集群拷贝过来的，没有修改过，<code>mapred.local.dir</code>是一个本地目录，集群上的机器有这个目录，但是你的本机上没有，所以出现了这个错误。解决办法是，在本地新建一个目录，然后把<code>mapred.local.dir</code>设置为这个路径。</p>

<p>如果出现<code>org.apache.hadoop.security.AccessControlException: Permission denied: user=soulmachine, access=WRITE, inode="tmp"</code>的错误，多半是因为你没有给这个用户创建<code>hadoop.tmp.dir</code>文件夹，见<a href="http://www.yanjiuyanjiu.com/blog/20140203">Hadoop多用户的配置</a>第2.2节。</p>

<h2 id="nutch-17-hadoop-2x">8 把Nutch 1.7 爬虫部署到Hadoop 2.x集群上</h2>
<p>事实证明是完全可行的，Hadoop 2.x 向后兼容。</p>

<p>把hadoop 2.x的配置文件，全部拷贝到 nutch 的conf目录下</p>

<pre><code>cp ~/local/opt/hadoop-2.2.0/etc/hadoop* ~/local/src/apache-nutch-1.7/conf
</code></pre>

<p>然后编译，</p>

<pre><code>ant runtime
</code></pre>

<p>把种子列表上传到hdfs,</p>

<pre><code>$ hdfs dfs -put ~/urls urls
$ hdfs dfs -lsr urls
</code></pre>

<p>提交Job,</p>

<pre><code>$ hadoop jar ./runtime/deploy/apache-nutch-1.7.job org.apache.nutch.crawl.Crawl urls -dir TestCrawl -depth 2
</code></pre>

<p>查看结果，</p>

<pre><code>$ cd runtime/deploy
$ ./bin/readdb hdfs://localhost/user/soulmachine/TestCrawl/crawldb/ -stats
14/02/14 16:51:07 INFO crawl.CrawlDbReader: Statistics for CrawlDb: hdfs://localhost/user/soulmachine/TestCrawl/crawldb/
14/02/14 16:51:07 INFO crawl.CrawlDbReader: TOTAL urls:	70
14/02/14 16:51:07 INFO crawl.CrawlDbReader: retry 0:	70
14/02/14 16:51:07 INFO crawl.CrawlDbReader: min score:	0.006
14/02/14 16:51:07 INFO crawl.CrawlDbReader: avg score:	0.03972857
14/02/14 16:51:07 INFO crawl.CrawlDbReader: max score:	1.2
14/02/14 16:51:07 INFO crawl.CrawlDbReader: status 1 (db_unfetched):	59
14/02/14 16:51:07 INFO crawl.CrawlDbReader: status 2 (db_fetched):	11
14/02/14 16:51:07 INFO crawl.CrawlDbReader: CrawlDb statistics: done
</code></pre>

<h2 id="section-2">参考资料</h2>
<ol>
  <li><a href="http://packtlib.packtpub.com/library/web-crawling-and-data-mining-with-apache-nutch/ch03lvl1sec20">Web Crawling and Data Mining with Apache Nutch 的第3.2节</a></li>
  <li>
    <p><a href="http://nutchhadoop.blogspot.com/">Install Nutch 1.7 and Hadoop 1.2.0</a></p>
  </li>
  <li><a href="http://blog.csdn.net/azhao_dn/article/details/6921398">hadoop mapred(hive)执行目录 文件权限问题</a></li>
</ol>

<h2 id="section-3">废弃的资料</h2>
<ol>
  <li>
    <p><a href="http://wiki.apache.org/nutch/NutchHadoopTutorial">Nutch and Hadoop Tutorial</a>，讲的是Nutch 1.3的，太老了，完全不适用Nutch 1.7</p>
  </li>
  <li>
    <p><a href="http://wiki.apache.org/nutch/NutchHadoopSingleNodeTutorial">Running Nutch in (pseudo) distributed-mode</a>，太短了，没什么内容</p>
  </li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop多用户的配置(Hadoop 1.x)]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140203"/>
    <updated>2014-02-03T10:05:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/hadoop-multiple-users</id>
    <content type="html"><![CDATA[<p>假设我们以名为hadoop的用户，建好了集群，见<a href="http://www.yanjiuyanjiu.com/blog/20140202">在CentOS上安装Hadoop集群</a>。通常，我们会把这个集群共享给多个用户，而不是让大家都登录为hadoop，这样做有几个好处：</p>

<ul>
  <li>一个用户不能修改另一个用户的的文件</li>
  <li>在hadoop web管理页面，可以很方便的看到不同的用户的job</li>
</ul>

<p>现在集群中有一台机器，上面有一个用户名为 hbase 的用户，他想要使用hadoop集群，怎么配置呢？</p>

<h2 id="hadoop">1. 安装hadoop客户端</h2>

<h3 id="section">1.1 下载，解压</h3>
<p>下载跟hadoop集群一样的hadoop软件包，并解压，</p>

<pre><code>$ wget http://mirror.olnevhost.net/pub/apache/hadoop/common/stable1/hadoop-1.2.1-bin.tar.gz
$ tar -zxf hadoop-1.2.1-bin.tar.gz -C ~/local/opt
$ cd ~/local/opt/hadoop-1.2.1
</code></pre>

<h3 id="section-1">1.2 配置</h3>
<p>在客户端只需配置集群namenode 和 jobtracker 的相关信息, 把namenode和jobtracker的信息告诉客户端即可。</p>

<p>把hadoop用户下的<code>conf/core-site.xml</code>和<code>conf/mapred-site.xml</code>拷贝到本用户的conf/目录下</p>

<pre><code>$ scp hadoop@localhost:~/local/opt/hadoop-1.2.1/conf/core-site.xml ./conf/
$ scp hadoop@localhost:~/local/opt/hadoop-1.2.1/conf/mapred-site.xml ./conf/
</code></pre>

<p>修改conf/mapred-site.xml中的<code>mapred.local.dir</code>，改为本机上的某个目录，确保这个目录存在且有写入。因为这个目录是本地目录，每台机器都可以不同。例如我的是：</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;mapred.local.dir&lt;/name&gt;
  &lt;value&gt;/home/soulmachine/local/var/hadoop/mapred/local&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>确保这个目录存在，</p>

<pre><code>$ mkdir -p ~/local/var/hadoop/mapred/local
</code></pre>

<!-- more -->

<p>还有另一种方法，由于<code>mapred.local.dir</code>默认值是<code>${hadoop.tmp.dir}/mapred/local</code>，也可以通过修改<code>hadoop.tmp.dir</code>达到目的，在<code>core-site.xml</code>中，确保<code>${hadoop.tmp.dir}/mapred/local</code>存在且有写权限。</p>

<h2 id="master">2. 在master上配置权限</h2>
<p>以下操作均在hadoop集群的 namenode 这台机器上进行，且登录为hadoop，因为hadoop这个用户是整个hadoop集群权限最高的用户（但对于Linux系统本身，这个用户其实没有sudo权限）。</p>

<p>Hadoop关于用户权限方面，有很多高级的配置，这里我们简单的利用HDFS本身的文件权限检查机制，来配置多用户。</p>

<p>HDFS本身没有提供用户名、用户组的创建，在客户端调用hadoop 的文件操作命令时，hadoop 识别出执行命令所在进程的linux系统的用户名和用户组，然后使用这个用户名和组来检查文件权限。 用户名=linux命令中的<code>whoami</code>，而组名等于<code>groups</code>。 </p>

<p>启动hadoop hdfs系统的用户即为超级用户（在这里就是名为hadoop的这个用户），可以进行任意的操作。</p>

<p>在客户端机器上，用gropus命令看一下hbase所在的组，</p>

<pre><code>$ groups
hbase
</code></pre>

<p>说明hbase这个用户所在的组为hbase。</p>

<h3 id="home">2.1 为客户端用户创建home文件夹</h3>

<pre><code>$ hadoop fs -mkdir /user/hbase
$ hadoop fs -chown hbase /user/hbase
$ hadoop fs -chgrp hbase /user/hbase
</code></pre>

<h3 id="hadooptmpdir">2.2 为客户端用户创建hadoop.tmp.dir文件夹</h3>
<p><code>hadoop.tmp.dir</code>既是一个本地目录，也是HDFS上的一个目录，参考<a href="http://stackoverflow.com/questions/2354525/what-should-be-hadoop-tmp-dir">What should be hadoop.tmp.dir?</a>。默认是<code>/tmp/hadoop-${user.name}</code>（参考官方表格，<a href="http://hadoop.apache.org/docs/r1.2.1/core-default.html">core-default</a>），所以我们需要为用户创建这个文件夹，</p>

<pre><code>$ hadoop fs -mkdir /tmp/hadoop-hbase
$ hadoop fs -chown hbase /tmp/hadoop-hbase
$ hadoop fs -chgrp hbase /tmp/hadoop-hbase
</code></pre>

<p>补充说明一下各个常见目录的相关知识，</p>

<ul>
  <li><code>dfs.name.dir</code>和<code>dfs.data.dir</code>都是本地目录，它们是HDFS的基础，所以只可能是本地目录</li>
  <li><code>mapred.local.dir</code>是本地目录，当客户端向集群提交了一个任务后，该job相关的文件（jar包和配置文件）会存放在HDFS上，各个slave从HDFS把这些文件下载到本地，然后开始执行。</li>
  <li><code>mapred.system.dir</code>是一个HDFS目录，存放了一个job的控制信息，被多个slave所共享，所以只能是HDFS目录。</li>
  <li><code>mapred.temp.dir</code>是一个HDFS目录，存放着一个job的临时文件，job结束后会被自动删除。</li>
</ul>

<h3 id="mapreducejobtrackerstagingrootdir">2.3 设置mapreduce.jobtracker.staging.root.dir</h3>
<p>客户端向集群提交任务时，需要把该job需要的文件打包，拷贝到HDFS上。在拷贝之前，得先确定这些资源文件存放在HDFS的什么地方。JobTracker设置有一个工作目录(Staging area, 也称数据中转站)，用来存储与每个job相关的数据。这个目录的前缀由<code>mapreduce.jobtracker.staging.root.dir</code> 参数来指定，默认是<code>${hadoop.tmp.dir}/mapred/staging</code>，每个client user可以提交多个job，在这个目录后就得附加user name的信息。所以这个工作目录(Staging area)默认是<code>${hadoop.tmp.dir}/mapred/staging/denny/.staging/</code>。</p>

<p>一般把前缀设置为<code>/user</code>，这是官方推荐的，见 <a href="http://hadoop.apache.org/docs/r1.2.1/mapred-default.html">mapred-default.xml</a> 里的<code>mapreduce.jobtracker.staging.root.dir</code>处：</p>

<blockquote>
  <p>The root of the staging area for users’ job files In practice, this should be the directory where users’ home directories are located (usually /user)</p>
</blockquote>

<pre><code>#以hadoop用户登录jobtracker机器
$ vim conf/mapred-site.xml
&lt;property&gt;
  &lt;name&gt;mapreduce.jobtracker.staging.root.dir&lt;/name&gt;
  &lt;value&gt;/user&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h3 id="hadoop-1">2.4 重启hadoop集群</h3>
<p>将配置文件scp到所有机器，然后重启集群，</p>

<pre><code>$ bin/stop-all.sh
$ bin/start-all.sh
</code></pre>

<h2 id="section-2">3. 测试一下</h2>
<p>回到客户端机器。</p>

<p>将输入数据拷贝到分布式文件系统中:</p>

<pre><code>$ bin/hadoop fs -put conf input
$ bin/hadoop fs -ls input
</code></pre>

<p>运行 Hadoop 自带的例子:</p>

<pre><code>$ bin/hadoop jar hadoop-examples-*.jar wordcount input output
</code></pre>

<p>查看输出文件:</p>

<pre><code>$ bin/hadoop fs -ls output
$ bin/hadoop fs -cat output/part-r-00000
</code></pre>

<p>如果能看到结果，说明这个例子运行成功。</p>

<h3 id="hadoopbinhadoop">4 （可选）设置别名，名称为hadoop，指向bin/hadoop</h3>
<p>这样就不用每次都cd到Hadoop目录，执行命令了。</p>

<p>在 <code>~/.bashrc</code>中添加如下4行：</p>

<pre><code>unalias hadoop &amp;&gt; /dev/null
alias hadoop="$HOME/local/opt/hadoop-1.2.1/bin/hadoop"
unalias hls &amp;&gt; /dev/null
alias hls="hadoop fs -ls"
</code></pre>

<p>source使之立刻生效，</p>

<pre><code>$ source ~/.bashrc
</code></pre>

<h2 id="section-3">参考资料</h2>

<ol>
  <li><a href="http://blog.csdn.net/j3smile/article/details/7887826">hadoop远程客户端安装配置、多用户权限配置</a></li>
  <li><a href="http://blog.csdn.net/a999wt/article/details/8718707">hadoop如何创建多用户</a></li>
  <li><a href="http://blog.sina.com.cn/s/blog_605f5b4f0101897z.html">关于多用户时hadoop的权限问题</a></li>
  <li><a href="http://langyu.iteye.com/blog/909170">MapReduce: Job提交过程</a></li>
  <li><a href="http://www.hadoopor.com/archiver/tid-481.html">hadoop中的dfs.name.dir,mapred.local.dir,mapred.system.dir和hadoop.tmp.dir说明</a></li>
  <li><a href="http://fenriswolf.me/2012/08/06/hadoop-%E5%8F%83%E6%95%B8%E8%A8%AD%E5%AE%9A-mapred-site-xml/">Hadoop 參數設定 – mapred-site.xml</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在CentOS上安装Hadoop集群]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140202"/>
    <updated>2014-02-02T12:39:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/hadoop-installatioin-on-centos</id>
    <content type="html"><![CDATA[<p>Ubuntu上安装，请参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120103/">在Ubuntu上安装Hadoop</a>。</p>

<p><strong>环境</strong>：CentOS 6.5, OPenJDK 1.7, Hadoop 1.2.1</p>

<p>本文主要参考官网的文档，<a href="http://hadoop.apache.org/docs/r1.2.1/#Getting+Started">Hadoop 1.2.1 Getting Started</a></p>

<h2 id="standalone-mode">1 单机模式(Standalone Mode)</h2>
<p>为了能顺利安装成功，我们先练习在单台机器上安装Hadoop。在单台机器上，可以配置成单机模式(Standalone Mode)和伪分布式模式(Pseudo-Distributed Mode)，参考官方文档<a href="http://hadoop.apache.org/docs/r1.2.1/single_node_setup.html">Single Node Setup</a>。</p>

<h3 id="hadoop-121">1.1 下载Hadoop 1.2.1，解压</h3>
<p>用浏览器下载或wget,</p>

<pre><code>$ wget http://mirror.olnevhost.net/pub/apache/hadoop/common/stable1/hadoop-1.2.1-bin.tar.gz
$ tar -zxf hadoop-1.2.1-bin.tar.gz -C ~/local/opt
$ cd ~/local/opt/hadoop-1.2.1
</code></pre>

<h3 id="confhadoop-envsh-javahome">1.2 编辑 conf/hadoop-env.sh，设置 <code>JAVA_HOME</code></h3>

<pre><code>$ echo $JAVA_HOME
/usr/lib/jvm/java
$ vim conf/hadoop-env.sh
</code></pre>

<p>取消<code>JAVA_HOME</code>那一行的注释，设置正确的JDK位置</p>

<pre><code>export JAVA_HOME=/usr/lib/jvm/java
</code></pre>

<h3 id="job">1.3 运行一个job</h3>
<p>默认情况下，Hadoop就被配置为Standalone模式了，此时Hadoop的所有模块都运行在一个java进程里。</p>

<p>我们运行一个例子测试一下。下面的几行命令，把 <code>conf</code>下的所有xml文件作为输入，在文件中查找指定的正则表达式，把匹配的结果输出到<code>output</code>目录。</p>

<pre><code>$ mkdir input 
$ cp conf/*.xml input 
$ bin/hadoop jar hadoop-examples-*.jar grep input output 'dfs[a-z.]+' 
$ cat output/*
</code></pre>

<p>可以看到正常的结果，说明单机模式运行成功了，下面开始配置伪分布式模式。</p>

<!-- more -->

<h2 id="pseudo-distributed-mode">2 伪分布式模式(Pseudo-Distributed Mode)</h2>
<p>Hadoop能在单台机器上以伪分布式模式运行，即每个Hadoop模块运行在单独的java进程里。</p>

<h3 id="confhadoop-envsh-javahome-1">2.1 编辑 conf/hadoop-env.sh，设置 <code>JAVA_HOME</code></h3>

<pre><code>$ echo $JAVA_HOME
/usr/lib/jvm/java
$ vim conf/hadoop-env.sh
</code></pre>

<h3 id="ssh">2.2 设置无密码SSH登录</h3>
<p>先检查一下是能够无密码登录本机，</p>

<pre><code>ssh localhost
</code></pre>

<p>如果提示输入密码，说明不能，按如下步骤设置。</p>

<pre><code>$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa 
$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys
</code></pre>

<h3 id="section">2.3 配置</h3>

<p>conf/core-site.xml:</p>

<pre><code>&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;fs.default.name&lt;/name&gt;
         &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
     &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>conf/hdfs-site.xml:</p>

<pre><code>&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;dfs.replication&lt;/name&gt;
         &lt;value&gt;1&lt;/value&gt;
     &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>conf/mapred-site.xml:</p>

<pre><code>&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;mapred.job.tracker&lt;/name&gt;
         &lt;value&gt;localhost:9001&lt;/value&gt;
     &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<h3 id="hadoop">2.4 启动Hadoop</h3>

<p>格式化namenode</p>

<pre><code>$ bin/hadoop namenode -format
</code></pre>

<p>启动 Hadoop 后台进程</p>

<pre><code>$ bin/start-all.sh
</code></pre>

<p>Hadoop的log写入到了<code>${HADOOP_HOME}/logs</code>目录下。</p>

<p>现在可以用浏览器打开NameNode和JobTracker的web界面了。</p>

<ul>
  <li>NameNode - <a href="http://localhost:50070/">http://localhost:50070/</a></li>
  <li>JobTracker - <a href="http://localhost:50030/">http://localhost:50030/</a></li>
</ul>

<h3 id="section-1">2.5 运行一个例子</h3>
<p>运行的例子跟单机模式下的例子相同。</p>

<p>将输入数据拷贝到分布式文件系统中:</p>

<pre><code>$ bin/hadoop fs -put conf input
</code></pre>

<p>运行 Hadoop 自带的例子:</p>

<pre><code>$ bin/hadoop jar hadoop-examples-*.jar grep input output 'dfs[a-z.]+'
</code></pre>

<p>查看输出文件:</p>

<pre><code>$ bin/hadoop fs -cat output/*

1	dfs.replication
1	dfs.server.namenode.
1	dfsadmin
</code></pre>

<p>结束后，关闭 Hadoop:</p>

<pre><code>$ bin/stop-all.sh

stopping jobtracker
localhost: stopping tasktracker
stopping namenode
localhost: stopping datanode
localhost: stopping secondarynamenode
</code></pre>

<h2 id="fully-distributed-mode">3 分布式模式(Fully-Distributed Mode)</h2>
<p>主要参考官方文档<a href="http://hadoop.apache.org/docs/r1.2.1/cluster_setup.html">Cluster Setup</a>.</p>

<h3 id="section-2">3.1 准备3台机器</h3>
<p>如果你已经有了三台机器，这一步可以省略。</p>

<p>如果没有，则可以用VMware Workstation 或 VirtualBox创建3台虚拟机。首先用vmware workstation 新建一台CentOS 6.5，装好操作系统，选择 Basic Server，安装JDK，参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120423/">安装和配置CentOS服务器的详细步骤</a>。安装好后然后用<strong>浅拷贝</strong><code>Create a linked clone</code> 克隆出2台，这样有了3台虚拟机。启动3台机器，假设IP分别为<code>192.168.1.131, 192.168.1.132, 192.168.1.133</code>, 131做为NameNode,JobTracker和SecondaryNameNode，身兼3个角色，这3个角色应该放到3台不同的机器上，这里为了简化，用一台机器来做3个角色；132和133为 slave。三台机器上的用户名是<code>hadoop</code>，也可以用其他用户名，但必须三台机器都相同。</p>

<h4 id="section-3">3.1.1 关闭防火墙</h4>
<p>临时关闭防火墙</p>

<pre><code>$ sudo service iptables stop
</code></pre>

<p>下次开机后，防火墙还是会启动。</p>

<p>永久关闭防火墙</p>

<pre><code>$ sudo chkconfig iptables off
</code></pre>

<p>由于这几台虚拟机是开发机，不是生产环境，因此不必考虑安全性，可以永久关闭防火墙，还能给开发阶段带来很多便利。</p>

<h4 id="hostname">3.1.2 修改hostname</h4>
<p>如果集群中的每一台机器事先已经有了hostname，则这一步可以跳过。</p>

<p>这一步看起来貌似不必要，其实是必须的，否则最后运行wordcount等例子时，会出现“Too many fetch-failures”。因为HDFS用hostname而不是IP，来相互之间进行通信（见后面的注意1）。</p>

<p>在CentOS上修改hostname，包含两个步骤(假设将hostname1改为hostname2，参考<a href="http://www.ichiayi.com/wiki/tech/linux_hostname">这里</a>，但不需要第一步)：</p>

<ol>
  <li>将 <code>/etc/sysconfig/network</code> 內的 HOSTNAME 改成 yourhostname</li>
  <li>用<code>hostname</code>命令，临时修改机器名， <code>sudo hostname yourhostname</code></li>
</ol>

<p>用<code>exit</code>命令退出shell，再次登录，命令提示字符串就会变成<code>[username@yourhostname ~]$</code>。</p>

<p>用上述方法，将131改名为master，132改名为slave01，133改名为slave02。</p>

<p><strong>注意</strong>，对于有的Ubuntu/Debia 系统，会把本机的hostname解析成 127.0.1.1，例如：</p>

<pre><code>127.0.0.1       localhost
127.0.1.1       master
</code></pre>

<p>将第二行改为(参考<a href="http://wiki.ubuntu.org.cn/%E5%88%A9%E7%94%A8Cloudera%E5%AE%9E%E7%8E%B0Hadoop">利用Cloudera实现Hadoop</a>)</p>

<pre><code>127.0.0.1       master
</code></pre>

<h4 id="etchosts">3.1.3 修改所有机器的<code>/etc/hosts</code>文件</h4>
<p>在所有机器的<code>/etc/hosts</code>文件中，添加所有hostname对应的IP，一般在一台机器上设置好，然后scp到所有机器。例如</p>

<pre><code>192.168.1.131 master
192.168.1.132 slave01
192.168.1.133 slave02
</code></pre>

<h2 id="master-master">3.2 配置 master 无密码登陆到所有机器（包括master自己登陆自己）</h2>
<p>参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120102/">SSH无密码登录的配置</a></p>

<h2 id="hadoop-1">3.3 把Hadoop压缩包上传到所有机器，并解压</h2>
<p>将 hadoop-1.2.1-bin.tar.gz 上传到所有机器，然后解压。<strong>注意，所有机器的hadoop路径必须一致，因为master会登陆到slave上执行命令，master认为slave的hadoop路径与自己一样。</strong></p>

<p>下面开始配置，配置好了后，把conf 目录scp到所有其他机器。</p>

<h3 id="section-4">3.4 修改6个配置文件</h3>
<p>Hadoop的配置文件比较多，其设计原则可以概括为如下两点：</p>

<ul>
  <li>尽可能模块化。例如core-xxx.xml是针对基础公共库core的，hdfs-xxx.xml是针对分布式文件系统HDFS的，mapred-xxx.xml是针对分布式计算框架MapReduce的。</li>
  <li>动静分离。例如，在Hadoop 1.0.0之前，作业队列权限管理相关的配置被放在mapred-site.xml里，而该文件爱你是不可以动态加载的，每次修改后必须重启Hadoop，但从 1.0.0后，这些配置选项被剥离出来放到独立的配置文件mapred-queue-acls.xml中，该文件可以通过Hadoop命令动态加载。在conf下，core-default.xml, hdfs-default.xml和mapred-default.xml是只读的，core-site.xml, hdfs-site.xml和mapred-site.xml才是用户可以修改的。要想覆盖默认配置，就在xxx-site.xml里修改。</li>
</ul>

<p>以下操作在master上进行。</p>

<h3 id="confhadoop-envsh">3.4.1 conf/hadoop-env.sh</h3>
<p>在 conf/hadoop-env.sh里，设置 <code>JAVA_HOME</code>。如果集群中，每台机器的JDK不一定统一安装在同一个路径，那就要在每个节点的hadoop-env.sh里分别设置<code>JAVA_HOME</code>。</p>

<pre><code>export JAVA_HOME=/usr/lib/jvm/java
</code></pre>

<p>还要设置<code>HADOOP_PID_DIR</code>，这里我们令其为<code>HADOOP_PID_DIR=/home/hadoop/local/var/hadoop/pids</code>，参考<a href="http://www.iteye.com/topic/299219">Hadoop的pid配置</a>。</p>

<pre><code>export HADOOP_PID_DIR=/home/hadoop/local/var/hadoop/pids
</code></pre>

<p>注意，还要<strong>禁用IPv6</strong>，用命令<code>cat /proc/sys/net/ipv6/conf/all/disable_ipv6</code>检查一下系统是否启用了IPv6，如果是0,说明启用了。Hadoop在IPv6的情况下运行不正常，因此需禁用IPv6。</p>

<p>不过我们不用真的禁用IPv6，还有另外一种方法，让java优先选择IPv4即可，在conf/hadoop-env.sh 里添加如下一行，</p>

<pre><code>export HADOOP_OPTS="-server -Djava.net.preferIPv4Stack=true $HADOOP_OPTS"
</code></pre>

<p>参考<a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/#disabling-ipv6">Disabling IPv6</a>，以及Web Crawling and Data Mining with Apache Nutch这本书的第66页。</p>

<h3 id="confmasters">3.4.2 conf/masters</h3>

<pre><code>master
</code></pre>

<h3 id="confslaves">3.4.3 conf/slaves</h3>

<pre><code>slave01
slave02
</code></pre>

<p>这里解释一下，masters文件，存放的其实是SecondaryNameNode。关于masters和slaves两个配置文件，更精确的说明见这个StackOverflow答案，<a href="http://stackoverflow.com/a/19779590/381712">hadoop conf/masters and conf/slaves on jobtracker?</a></p>

<h3 id="confcore-sitexml">3.4.4 conf/core-site.xml</h3>

<pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.default.name&lt;/name&gt;
        &lt;value&gt;hdfs://master:9000&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;fs.checkpoint.dir&lt;/name&gt;
        &lt;value&gt;/home/hadoop/local/var/hadoop/dfs/namesecondary&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>Hadoop会自动创建目录。</p>

<h3 id="confhdfs-sitexml">3.4.5 conf/hdfs-site.xml</h3>

<pre><code>&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;dfs.name.dir&lt;/name&gt;
         &lt;value&gt;/home/hadoop/local/var/hadoop/dfs/name&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
         &lt;name&gt;dfs.data.dir&lt;/name&gt;
         &lt;value&gt;/home/hadoop/local/var/hadoop/dfs/data&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
       &lt;name&gt;dfs.replication&lt;/name&gt;
       &lt;value&gt;2&lt;/value&gt;
     &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>我们只有2台slave，因此<code>dfs.replication</code>设置为2。</p>

<p>Hadoop会自动在master创建 /home/hadoop/local/var/hadoop/dfs/name 目录，在 slaves上创建 /home/hadoop/local/var/hadoop/dfs/data 目录。</p>

<h3 id="confmapred-sitexml">3.4.6 conf/mapred-site.xml</h3>

<pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapred.job.tracker&lt;/name&gt;
        &lt;value&gt;master:9001&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapred.local.dir&lt;/name&gt;
        &lt;value&gt;/home/hadoop/local/var/hadoop/mapred/local&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.jobtracker.staging.root.dir&lt;/name&gt;
        &lt;value&gt;/user&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<h3 id="slaves">3.5 将配置文件拷贝到所有slaves</h3>

<pre><code>$ cd ~/local/opt/hadoop-1.2.1/conf/
$ scp hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml masters slaves hadoop@slave01:~/local/opt/hadoop-1.2.1/conf/
$ scp hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml masters slaves hadoop@slave02:~/local/opt/hadoop-1.2.1/conf/
</code></pre>

<h3 id="hadoop-2">3.6 运行 hadoop</h3>
<p>在master上执行以下命令，启动hadoop</p>

<pre><code>$ cd ~/local/opt/hadoop-1.2.1/
#只需一次，下次启动不再需要格式化，只需 start-all.sh
$ bin/hadoop namenode -format
$ bin/start-all.sh
</code></pre>

<h3 id="section-5">3.7 检查是否启动成功</h3>

<p>在master上执行：</p>

<pre><code>$ jps

2615 NameNode
2767 JobTracker
2874 Jps
</code></pre>

<p>在一台slave上执行：</p>

<pre><code>$ jps

3415 DataNode
3582 TaskTracker
3499 SecondaryNameNode
3619 Jps
</code></pre>

<p>在另一台slave上执行：</p>

<pre><code>$ jps

3741 Jps
3618 DataNode
3702 TaskTracker
</code></pre>

<p>可见进程都启动起来了，说明hadoop运行成功。</p>

<h3 id="wordcount">3.8 运行wordcount例子，进一步测试是否安装成功</h3>
<p>将输入数据拷贝到分布式文件系统中:</p>

<pre><code>$ cd ~/local/opt/hadoop-1.2.1/
$ bin/hadoop fs -put conf input
</code></pre>

<p>运行 Hadoop 自带的例子:</p>

<pre><code>$ bin/hadoop jar hadoop-examples-*.jar wordcount input output
</code></pre>

<p>查看输出文件:</p>

<pre><code>$ bin/hadoop fs -ls output
$ bin/hadoop fs -cat output/part-r-00000
</code></pre>

<p>如果能看到结果，说明这个例子运行成功。</p>

<h3 id="hadoop-3">3.9 停止 hadoop集群</h3>
<p>在master上执行：</p>

<pre><code>$ bin/stop-all.sh
</code></pre>

<h3 id="masterhadoopprefixhadoopprefixbinpath">3.10 （可选）在master上设置环境变量HADOOP_PREFIX，并将HADOOP_PREFIX/bin加入PATH</h3>
<p>这一步是为了将bin目录加入PATH，这样可以在任何位置执行hadoop的各种命令。这步是可选的。</p>

<p>Hadoop不推荐使用<code>HADOOP_HOME</code>，你可以试一下，当设置了<code>HADOOP_HOME</code>后，执行<code>bin/start-all.sh</code>，第一行会打印出一行警告信息，<code>Warning: $HADOOP_HOME is deprecated.</code> 应该用<code>HADOOP_PREFIX</code>代替，见邮件列表里的这封<a href="http://mail-archives.apache.org/mod_mbox/hadoop-common-user/201202.mbox/%3CCB4ECC21.33727%25evans@yahoo-inc.com%3E">邮件</a>。</p>

<p>给所有机器设置环境变量<code>HADOOP_PREFIX</code>，并将<code>$HADOOP_PREFIX/bin</code>加入PATH。</p>

<p>在 <code>~/.bashrc</code>中添加如下4行：</p>

<pre><code>export HADOOP_PREFIX=$HOME/local/opt/hadoop-1.2.1
export PATH=$PATH:$HADOOP_PREFIX/bin
</code></pre>

<p>source使之立刻生效，</p>

<pre><code>$ source ~/.bashrc
</code></pre>

<h2 id="section-6">4. 排除错误</h2>
<p>本文已经尽可能的把步骤详细列出来了，但是我相信大部分人不会一次成功。这时候，查找错误就很重要了。查找错误最重要的手段是查看hadoop的日志，一般在logs目录下。把错误消息复制粘贴到google，搜索一下，慢慢找错误。</p>

<h2 id="section-7">注意</h2>
<ol>
  <li>所有配置文件只能用hostname，不能用IP。两年前我不懂，还为此<a href="http://stackoverflow.com/questions/8702637/hadoop-conf-fs-default-name-cant-be-setted-ipport-format-directly">在stackoverflow上发了帖子</a>。hadoop会反向解析hostname，即使是用了IP，也会使用hostname 来启动TaskTracker。参考<a href="http://stackoverflow.com/questions/15230946/hdfs-lan-ip-address-hostname-resolution">hdfs LAN ip address hostname resolution</a>，<a href="http://www.makenotes.net/?p=337004">hadoop入门经验总结- 杨贵堂的博客</a>，<a href="http://51mst.iteye.com/blog/1152439">hadoop集群配置</a>。</li>
  <li>在第2.5步骤，如果出现 <code>SafeModeException</code> 异常，不用担心，等待几分钟即可。因为hadoop刚刚启动时，会进入安全模式进行自检，这需要花点时间。</li>
  <li>如果在任何一步失败，可以<code>stop-all.sh</code>, 然后<code>hadoop  namenode -format</code>，重试几次，一般可以成功。如果还是不成功，多看看 logs目录下的日志文件，把错误消息复制粘贴到google，搜索答案。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nutch 快速入门(Nutch 2.2.1)]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140201"/>
    <updated>2014-02-01T04:11:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/nutch-tutorial</id>
    <content type="html"><![CDATA[<p>本文主要参考<a href="http://wiki.apache.org/nutch/Nutch2Tutorial">Nutch 2.x Tutorial</a></p>

<p>Nutch 2.x 与 Nutch 1.x 相比，剥离出了存储层，放到了gora中，可以使用多种数据库，例如HBase, Cassandra, MySql来存储数据了。Nutch 1.7 则是把数据直接存储在HDFS上。</p>

<h2 id="hbase">1. 安装并运行HBase</h2>
<p>为了简单起见，使用Standalone模式，参考 <a href="http://hbase.apache.org/book/quickstart.html">HBase Quick start</a></p>

<h3 id="section">1.1 下载，解压</h3>

<pre><code>wget http://archive.apache.org/dist/hbase/hbase-0.90.4/hbase-0.90.4.tar.gz
tar zxf hbase-0.90.4.tar.gz
</code></pre>

<h3 id="confhbase-sitexml">1.2 修改 conf/hbase-site.xml</h3>
<p>内容如下</p>

<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;file:///DIRECTORY/hbase&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
    &lt;value&gt;/DIRECTORY/zookeeper&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p><code>hbase.rootdir</code>目录是用来存放HBase的相关信息的，默认值是<code>/tmp/hbase-${user.name}/hbase</code>； <code>hbase.zookeeper.property.dataDir</code>目录是用来存放zookeeper（HBase内置了zookeeper）的相关信息的，默认值是<code>/tmp/hbase-${user.name}/zookeeper</code>。</p>

<h3 id="section-1">1.3 启动</h3>

<pre><code>$ ./bin/start-hbase.sh
starting Master, logging to logs/hbase-user-master-example.org.out
</code></pre>

<!-- more -->

<h3 id="shell">1.4 试用一下shell</h3>
<p>$ ./bin/hbase shell
HBase Shell; enter ‘help<return>&#8217; for list of supported commands.
Type &#8220;exit<return>&#8221; to leave the HBase Shell
Version 0.90.4, r1150278, Sun Jul 24 15:53:29 PDT 2011</return></return></p>

<p>hbase(main):001:0&gt;</p>

<p>创建一张名字为<code>test</code>的表，只有一个列，名为<code>cf</code>。为了验证创建是否成功，用<code>list</code>命令查看所有的table，并用<code>put</code>命令插入一些值。</p>

<pre><code>hbase(main):003:0&gt; create 'test', 'cf'
0 row(s) in 1.2200 seconds
hbase(main):003:0&gt; list 'test'
..
1 row(s) in 0.0550 seconds
hbase(main):004:0&gt; put 'test', 'row1', 'cf:a', 'value1'
0 row(s) in 0.0560 seconds
hbase(main):005:0&gt; put 'test', 'row2', 'cf:b', 'value2'
0 row(s) in 0.0370 seconds
hbase(main):006:0&gt; put 'test', 'row3', 'cf:c', 'value3'
0 row(s) in 0.0450 seconds
</code></pre>

<p>用<code>scan</code>命令扫描table，验证一下刚才的插入是否成功。</p>

<pre><code>hbase(main):007:0&gt; scan 'test'
ROW        COLUMN+CELL
row1       column=cf:a, timestamp=1288380727188, value=value1
row2       column=cf:b, timestamp=1288380738440, value=value2
row3       column=cf:c, timestamp=1288380747365, value=value3
3 row(s) in 0.0590 seconds
</code></pre>

<p>现在，disable并drop掉你的表，这会把上面的所有操作清零。</p>

<pre><code>hbase(main):012:0&gt; disable 'test'
0 row(s) in 1.0930 seconds
hbase(main):013:0&gt; drop 'test'
0 row(s) in 0.0770 seconds 
</code></pre>

<p>退出shell，</p>

<pre><code>hbase(main):014:0&gt; exit
</code></pre>

<h3 id="section-2">1.5 停止</h3>

<pre><code>$ ./bin/stop-hbase.sh
stopping hbase...............
</code></pre>

<h3 id="section-3">1.6 再次启动</h3>
<p>后面运行Nutch，需要把数据存储到HBase，因此需要启动HBase。</p>

<pre><code>$ ./bin/start-hbase.sh
starting Master, logging to logs/hbase-user-master-example.org.out
</code></pre>

<h2 id="nutch-221">2 编译Nutch 2.2.1</h2>

<h3 id="section-4">2.1 下载，解压</h3>
<pre><code>wget http://www.apache.org/dyn/closer.cgi/nutch/2.2.1/apache-nutch-2.2.1-src.tar.gz
tar zxf apache-nutch-2.2.1-src.tar.gz
</code></pre>

<h3 id="section-5">2.2 修改配置文件</h3>
<p>参考<a href="http://wiki.apache.org/nutch/Nutch2Tutorial">Nutch 2.0 Tutorial</a></p>

<p>修改 <code>conf/nutch-site.xml</code></p>

<pre><code>&lt;property&gt;
  &lt;name&gt;storage.data.store.class&lt;/name&gt;
  &lt;value&gt;org.apache.gora.hbase.store.HBaseStore&lt;/value&gt;
  &lt;description&gt;Default class for storing data&lt;/description&gt;
&lt;/property&gt;
</code></pre>

<p>修改<code>ivy/ivy.xml</code></p>

<pre><code>&lt;!-- Uncomment this to use HBase as Gora backend. --&gt;
&lt;dependency org="org.apache.gora" name="gora-hbase" rev="0.3" conf="*-&gt;default" /&gt;
</code></pre>

<p>修改 <code>conf/gora.properties</code>，确保<code>HBaseStore</code>被设置为默认的存储，</p>

<pre><code>gora.datastore.default=org.apache.gora.hbase.store.HBaseStore
</code></pre>

<h3 id="section-6">2.3 编译</h3>

<pre><code>ant runtime
</code></pre>

<p>刚开始会下载很多jar，需要等待一段时间。</p>

<p>有可能你会得到如下错误：</p>

<pre><code>Trying to override old definition of task javac
  [taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.

ivy-probe-antlib:

ivy-download:
  [taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.
</code></pre>

<p>无所谓，不用管它。</p>

<p>要等一会儿才能编译结束。编译完后，多出来了 build 和 runtime两个文件夹。</p>

<p>第3、4、5、6步与另一篇博客<a href="">Nutch 快速入门(Nutch 1.7)</a>中的第3、4、5、6步骤一模一样。</p>

<h2 id="url">3 添加种子URL</h2>

<pre><code>mkdir ~/urls
vim ～/urls/seed.txt
http://movie.douban.com/subject/5323968/
</code></pre>

<h2 id="url-1">4 设置URL过滤规则</h2>
<p>如果只想抓取某种类型的URL，可以在 <code>conf/regex-urlfilter.txt</code>设置正则表达式，于是，只有匹配这些正则表达式的URL才会被抓取。</p>

<p>例如，我只想抓取豆瓣电影的数据，可以这样设置：</p>

<pre><code>#注释掉这一行
# skip URLs containing certain characters as probable queries, etc.
#-[?*!@=]
# accept anything else
#注释掉这行
#+.
+^http:\/\/movie\.douban\.com\/subject\/[0-9]+\/(\?.+)?$
</code></pre>

<h2 id="agent">5 设置agent名字</h2>

<p>conf/nutch-site.xml:</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;http.agent.name&lt;/name&gt;
  &lt;value&gt;My Nutch Spider&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>这一步是从这本书上看到的，<a href="http://www.packtpub.com/web-crawling-and-data-mining-with-apache-nutch/book">Web Crawling and Data Mining with Apache Nutch</a>，第14页。</p>

<h2 id="solr">6 安装Solr</h2>
<p>由于建索引的时候需要使用Solr，因此我们需要安装并启动一个Solr服务器。</p>

<p>参考<a href="http://wiki.apache.org/nutch/NutchTutorial">Nutch Tutorial</a> 第4、5、6步，以及<a href="http://lucene.apache.org/solr/4_6_1/tutorial.html">Solr Tutorial</a>。</p>

<h3 id="section-7">6.1 下载，解压</h3>

<p>wget http://mirrors.cnnic.cn/apache/lucene/solr/4.6.1/solr-4.6.1.tgz
tar -zxf solr-4.6.1.tgz</p>

<h3 id="solr-1">6.2 运行Solr</h3>

<pre><code>cd example
java -jar start.jar
</code></pre>

<p>验证是否启动成功</p>

<p>用浏览器打开 <a href="http://localhost:8983/solr/admin/">http://localhost:8983/solr/admin/</a>，如果能看到页面，说明启动成功。</p>

<h3 id="nutchsolr">6.3 将Nutch与Solr集成在一起</h3>

<p>将<code>NUTCH_DIR/conf/schema-solr4.xml</code>拷贝到<code>SOLR_DIR/solr/collection1/conf/</code>，重命名为schema.xml，并在<code>&lt;fields&gt;...&lt;/fields&gt;</code>最后添加一行(具体解释见<a href="http://stackoverflow.com/questions/15527380/solr-4-2-what-is-version-field">Solr 4.2 - what is _version_field?</a>)，</p>

<pre><code>&lt;field name="_version_" type="long" indexed="true" stored="true" multiValued="false"/&gt;
</code></pre>

<p>重启Solr，</p>

<pre><code># Ctrl+C to stop Solr
java -jar start.jar
</code></pre>

<p>第7步和第8步也和Nutch 1.7那篇博客中的7、8步很类似。主要区别在于，Nutch 2.x的所有数据，不再以文件和目录的形式存放在硬盘上，而是存放到HBase里。</p>

<h2 id="section-8">7 一步一步使用单个命令抓取网页</h2>
<p>TODO</p>

<h2 id="crawl">8 使用crawl脚本一键抓取</h2>
<p>刚才我们是手工敲入多个命令，一个一个步骤，来完成抓取的，其实Nutch自带了一个脚本，<code>./bin/crawl</code>，把抓取的各个步骤合并成一个命令，看一下它的用法</p>

<pre><code>$ ./bin/crawl 
Missing seedDir : crawl &lt;seedDir&gt; &lt;crawlID&gt; &lt;solrURL&gt; &lt;numberOfRounds&gt;
</code></pre>

<p><strong>注意</strong>，这里是<code>crawlId</code>，不再是<code>crawlDir</code>。</p>

<p>先删除第7节产生的数据，使用HBase shell，用<code>disable</code>删除表。</p>

<h3 id="section-9">8.1 抓取网页</h3>

<pre><code>$ ./bin/crawl ~/urls/ TestCrawl http://localhost:8983/solr/ 2
</code></pre>

<ul>
  <li><code>～/urls</code> 是存放了种子url的目录</li>
  <li>TestCrawl 是crawlId，这会在HBase中创建一张以crawlId为前缀的表，例如TestCrawl_Webpage。</li>
  <li>http://localhost:8983/solr/ , 这是Solr服务器</li>
  <li>2，numberOfRounds，迭代的次数</li>
</ul>

<p>过了一会儿，屏幕上出现了一大堆url，可以看到爬虫正在抓取！</p>

<pre><code>fetching http://music.douban.com/subject/25811077/ (queue crawl delay=5000ms)
fetching http://read.douban.com/ebook/1919781 (queue crawl delay=5000ms)
fetching http://www.douban.com/online/11670861/ (queue crawl delay=5000ms)
fetching http://book.douban.com/tag/绘本 (queue crawl delay=5000ms)
fetching http://movie.douban.com/tag/科幻 (queue crawl delay=5000ms)
49/50 spinwaiting/active, 56 pages, 0 errors, 0.9 1 pages/s, 332 245 kb/s, 131 URLs in 5 queues
fetching http://music.douban.com/subject/25762454/ (queue crawl delay=5000ms)
fetching http://read.douban.com/reader/ebook/1951242/ (queue crawl delay=5000ms)
fetching http://www.douban.com/mobile/read-notes (queue crawl delay=5000ms)
fetching http://book.douban.com/tag/诗歌 (queue crawl delay=5000ms)
50/50 spinwaiting/active, 61 pages, 0 errors, 0.9 1 pages/s, 334 366 kb/s, 127 URLs in 5 queues
</code></pre>

<h3 id="section-10">8.2 查看结果</h3>

<pre><code>./bin/nutch readdb -crawlId TestCrawl -stats
</code></pre>

<p>也可以进HBase shell 查看，</p>

<pre><code>cd ~/hbase-0.90.4
./bin/hbase shell
hbase(main):001:0&gt; scan 'TestCrawl_webpage'
</code></pre>

<p>屏幕开始不断输出内容，可以用Ctrl+C 结束。</p>

<p>在运行scan查看表中内容时，对于列的含义不确定时可以查看<code>conf/gora-hbase-mapping.xml</code>文件，该文件定义了列族及列的含义。</p>

<h2 id="section-11">相关文章</h2>
<p><a href="http://www.yanjiuyanjiu.com/blog/20140121/">Nutch 快速入门(Nutch 1.7)</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[给CentOS安装UEK内核]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140131"/>
    <updated>2014-01-31T22:05:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/install-uek-for-centos</id>
    <content type="html"><![CDATA[<p>最近给CentOS 6.5 安装了docker，不过每次运行都会报警：</p>

<blockquote>
  <p>WARNING: You are running linux kernel version 2.6.32-431.3.1.el6.x86_64, which might be unstable running docker. Please upgrade your kernel to 3.8.0.</p>
</blockquote>

<p>给CentOS 升级内核，有三种途径，一种是yum官方源里有更新的版本，一种途径是自己编译，另一种途径是使用别人编译好了的内核。</p>

<p>CentOS yum源是出了名的更新慢，目前没有 3.8版内核，第二种途径很麻烦，工作量很大，因此本文用第三种。例如UEK，Oracle提供了一个公共的yum源，<a href="http://public-yum.oracle.com/">http://public-yum.oracle.com/</a></p>

<h2 id="yum">添加yum源</h2>

<p>UEK的稳定版还是 2.6 内核的，beta版的内核是3.8的，所以我们使用beta源</p>

<pre><code>sudo wget http://public-yum.oracle.com/beta/public-yum-ol6-beta.repo -P /etc/yum.repos.d
</code></pre>

<p>由于UEK3还没有加入到正式版本中，还目前属于测试阶段，，需要手工将 <code>enabled=0</code>改为 <code>enabled=1</code></p>

<pre><code>sudo vim /etc/yum.repos.d/public-yum-ol6-beta.repo
</code></pre>

<h2 id="gpg-key">添加GPG key</h2>

<pre><code>sudo wget http://public-yum.oracle.com/RPM-GPG-KEY-oracle-ol6 -O /etc/pki/rpm-gpg/RPM-GPG-KEY-oracle
gpg --quiet --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-oracle
</code></pre>

<h2 id="yum-1">更新yum源</h2>

<pre><code>sudo yum update
</code></pre>

<h2 id="kernel">列出所有的kernel</h2>

<pre><code>yum list kernel*
</code></pre>

<!-- more -->

<h2 id="kernel-1">安装 kernel</h2>

<pre><code>sudo yum install kernel-uek
</code></pre>

<h2 id="etcgrubconf">修改/etc/grub.conf</h2>
<p>修改 <code>/etc/grub.conf</code>，将<code>default</code>设置为UEK，一般新安装的内核会在一个，所以设置 <code>default=0</code>。</p>

<h2 id="section">重启</h2>

<pre><code>sudo reboot
uname -r
3.8.13-16.el6uek.x86_64
</code></pre>

<p>内核切换成功了。</p>

<h2 id="docker">测试一下，docker不再报警了</h2>

<pre><code>sudo docker service docker stop
sudo docker -d
</code></pre>

<p><code>Ctrl+C</code> 终止 docker，然后用 <code>sudo service docker start</code> 再次启动docker。</p>

<h2 id="section-1">参考资料</h2>
<p><a href="http://public-yum.oracle.com/">Public Yum Server from Oracle</a></p>

<p><a href="http://blog.liulantao.com/Technology/2013/09/23/kernel-uek-3813-upgrade-notes.html">UEK R3升级手记</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark开发环境的配置]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140130"/>
    <updated>2014-01-30T16:36:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/spark-development-environment</id>
    <content type="html"><![CDATA[<p><strong>软件版本</strong>：Spark 0.9</p>

<p>配置Spark开发环境，其实分为三个层次，一种是针对运维人员，把Spark安装部署到集群；一种是针对普通开发者，引入Spark的jar包，调用Spark提供的接口，编写分布式程序，写好后编译成jar，就可以提交到Spark集群去运行了；第三种是针对Spark开发者，为了给Spark贡献代码，需要git clone Spark的代码，然后导入IDE，为Spark开发代码。</p>

<h2 id="spark">1 部署Spark集群</h2>
<p>这种是运维人员在生产环境下，搭建起一个Spark集群。</p>

<h3 id="spark-1">（可选）创建新用户 Spark</h3>
<p>一般我倾向于把需要启动daemon进程，对外提供服务的程序，即服务器类的程序，安装在单独的用户下面。这样可以做到隔离，运维方面，安全性也提高了。</p>

<p>创建一个新的group,</p>

<pre><code>$ sudo groupadd spark
</code></pre>

<p>创建一个新的用户，并加入group,</p>

<pre><code>$ sudo useradd -g spark spark
</code></pre>

<p>给新用户设置密码，</p>

<pre><code>$ sudo passwd spark
</code></pre>

<p>在每台机器上创建 spark 新用户，并配置好SSH无密码，参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120102/">SSH无密码登录的配置</a></p>

<p>假设有三台机器，hostname分别是 master, worker01, worker02。</p>

<h3 id="spark-">1.1 下载 Spark 预编译好的二进制包</h3>
<p>如果你需要用到HDFS，则要针对Hadoop 1.x 和Hadoop 2.x 选择不同的版本。这里我选择 Hadoop 2.x 版。</p>

<pre><code>spark@master $ wget http://d3kbcqa49mib13.cloudfront.net/spark-0.9.0-incubating-bin-hadoop1.tgz
spark@master $ tar zxf spark-0.9.0-incubating-bin-hadoop1.tgz -C ~/local/opt
</code></pre>

<h3 id="tgzscp">1.2 将tgz压缩包scp到所有机器，解压到相同的路径</h3>

<pre><code>spark@master $ scp spark-0.9.0-incubating-bin-hadoop1.tgz spark@worker01:~
spark@master $ ssh worker01
spark@worker01 $ tar zxf spark-0.9.0-incubating-bin-hadoop1.tgz -C ~/local/opt
spark@worker01 $ exit
spark@master $ scp spark-0.9.0-incubating-bin-hadoop1.tgz spark@worker02:~
spark@master $ ssh worker02
spark@worker02 $ tar zxf spark-0.9.0-incubating-bin-hadoop1.tgz -C ~/local/opt
spark@worker02 $ exit
</code></pre>

<h3 id="section">1.3 修改配置文件</h3>
<p>Spark 0.9 以后，配置文件简单多了，只有一个必须要配置，就是 <code>conf/slaves</code> 这个文件。在这个文件里添加slave的hostname。</p>

<h3 id="slave">1.4 拷贝配置文件到所有slave</h3>

<pre><code>spark@master $ spark@master $ scp ./conf/slaves spark@worker01:~/local/opt/spark-0.9.0-incubating-bin-hadoop1/conf
spark@master $ spark@master $ scp ./conf/slaves spark@worker02:~/local/opt/spark-0.9.0-incubating-bin-hadoop1/conf
</code></pre>

<h3 id="spark-2">1.5 启动Spark集群</h3>

<pre><code>spark@master $ ./sbin/start-all.sh
</code></pre>

<!-- more -->

<p>也可以一台一台启动，先启动 master</p>

<pre><code>spark@master $ ./sbin/start-master.sh
</code></pre>

<p>启动两台 slave，</p>

<pre><code>spark@worker01 $ ./sbin/start-slave.sh 1 spark://master:7077
spark@worker02 $ ./sbin/start-slave.sh 2 spark://master:7077
</code></pre>

<p>其中，<code>1</code>, <code>2</code> 是 worker的编号，可以是任意数字，只要不重复即可，<code>spark://master:7077</code> 是 master 的地址。以后向集群提交作业的时候，也需要这个地址。</p>

<h3 id="section-1">1.6 测试一下，向集群提交一个作业</h3>

<pre><code>spark@master $ ./bin/run-example org.apache.spark.examples.SparkPi spark://master:7077
</code></pre>

<h2 id="section-2">2 配置普通开发环境</h2>
<p>TODO</p>

<h2 id="spark-3">3 配置Spark开发环境</h2>
<p>当你需要修改Spark的代码，或给Spark添加代码，就需要阅读本节了。</p>

<h3 id="git-clone-">3.1 git clone 代码</h3>

<pre><code>git clone git@github.com:apache/incubator-spark.git
</code></pre>

<h3 id="section-3">3.2 编译</h3>
<p>Spark脚本会自动下载对应版本的sbt和scala编译器，因此机器事先不需要安装sbt和scala</p>

<p>按照 github 官方repo首页的文档，输入如下一行命令即可开始编译，</p>

<pre><code>./sbt/sbt assembly
</code></pre>

<h3 id="section-4">3.3 运行一个例子</h3>

<pre><code>./run-example org.apache.spark.examples.SparkPi local
</code></pre>

<p>说明安装成功了。</p>

<h3 id="spark-shell">3.4 试用 spark shell</h3>
<!-- more -->

<p>./spark-shell</p>

<p>会出现<code>scala&gt;</code>提示符号，可见spark脚本自动下载了scala编译器，其实就是一个jar，例如<code>scala-compiler-2.10.3.jar</code>。</p>

<h3 id="scala">3.5 安装scala</h3>
<p>开发Spark的时候，由于Intellij Idea 需要调用外部的sbt和scala，因此机器上还是需要安装scala和sbt。</p>

<p>打开 <code>projects/SparkBuild.scala</code>，搜索<code>scalaVersion</code>，获得spark所使用的scala编译器版本，然后去scala官网<a href="http://www.scala-lang.org/">http://www.scala-lang.org/</a>，下载该版本的scala编译器，并设置<code>SCALA_HOME</code>环境变量，将bin目录加入PATH。例如下载scala-2.10.3.tgz，解压到/opt，设置环境变量如下：</p>

<pre><code>sudo vim /etc/profile
export SCALA_HOME=/opt/scala-2.10.3
export PATH=$PATH:$SCALA_HOME/bin
</code></pre>

<h3 id="sbt">3.6 安装sbt</h3>

<p>打开<code>projects/build.properties</code>，可以看到spark所使用的sbt版本号，去
官网<a href="http://www.scala-sbt.org/">http://www.scala-sbt.org/</a>下载该版本的sbt，双击安装。并设置<code>SBT_HOME</code>环境变量，将bin目录加入PATH。</p>

<h3 id="idea">3.7 下载并安装idea</h3>

<p>Spark核心团队的hashjoin曾在我博客上留言，说他们都使用idea在开发spark，我用过<a href="www.scala-ide.org">Scala IDE</a>和idea，两者各有优劣，总的来说，idea要好用一些，虽然我是老牌eclipse用户，但我还是转向了idea。</p>

<p>去idea官网下载idea的tar.gz包，解压就行。运行idea，安装scala插件。</p>

<h3 id="idea-1">3.8 生成idea项目文件</h3>
<p>在源码根目录，使用如下命令</p>

<pre><code>./sbt/sbt gen-idea
</code></pre>

<p>就生成了idea项目文件。</p>

<h3 id="open-project">3.9 Open Project</h3>
<p>使用 idea，点击<code>File-&gt;Open project</code>，浏览到 <code>incubator-spark</code>文件夹，打开项目，就可以修改Spark代码了。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[我的Ansible playbook]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140129"/>
    <updated>2014-01-29T22:08:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/my-ansible-playbook</id>
    <content type="html"><![CDATA[<p><strong>前提</strong>，安装好Ansible，参考我的上一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20140127">Ansible 快速入门</a></p>

<pre><code>---
- hosts: all
  sudo: True
  remote_user: work
  vars:
    ant_version: 1.9.3
    maven_version: 3.1.1
    scala_version: 2.10.3
    sbt_version: 0.12.4

  tasks:
  ########## for CentOS and RedHat ##########
    - name: Install the libselinux-python package  # because ansible needs it
      yum: name=libselinux-python state=installed
      when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'

    - name: Disable SELinux in conf file
      selinux: state=disabled
      when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'

    - name: update YUM repositories
      shell: 'yum -y update'
      when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'

    - name: Install OpenJDK
      yum: name=java-1.7.0-openjdk-devel state=present
      when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'

    - name: Set JAVA_HOME environment variable
      lineinfile: dest='/etc/profile' regexp='^#?\s*export JAVA_HOME=(.*)$' line='export JAVA_HOME=/usr/lib/jvm/java' state=present
      when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'

    - name: install the 'Development tools' package group
      yum: name="@Development tools" state=present
      when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'

    - name: upgrade all packages
      shell: 'yum -y upgrade'
      when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'


  ########## for Ubuntu and Debian ##########
    - name: Run "apt-get update" to update the source list
      apt: update_cache=yes
      when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'

    - name: Install OpenJDK
      apt: pkg=openjdk-7-jdk state=present
      when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'

    - name: Set JAVA_HOME environment variable
      lineinfile: dest='/etc/profile' regexp='^#?\s*export JAVA_HOME=(.*)$' line='export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-amd64' state=present
      when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'

    - name: Install build-essential
      apt: pkg=build-essential state=present
      when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'

    - name: Update all packages to the latest version
      apt: upgrade=dist
      when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'


  ########## Common opreations for all OS ##########
    # Create local directories
    - file: path=~/local/bin state=directory
      sudo: no
    - file: path=~/local/sbin state=directory
      sudo: no
    - file: path=~/local/src state=directory
      sudo: no
    - file: path=~/local/opt state=directory
      sudo: no
    - file: path=~/local/var state=directory
      sudo: no
    - lineinfile: dest=~/.bashrc regexp='^#?\s*export PATH=(.*)/local/bin(.*)$' line="export PATH=$PATH:$HOME/local/bin" state=present
      sudo: no
    - lineinfile: dest=~/.bashrc regexp='^#?\s*export PATH=(.*)/local/sbin(.*)$' line="export PATH=$PATH:$HOME/local/sbin" state=present
      sudo: no

    - name: Set CLASSPATH and PATH environment variables
      lineinfile: $item
      with_items:
        - dest='/etc/profile' regexp='^#?\s*export CLASSPATH=(.*)$' line='export CLASSPATH=.:$JAVA_HOME/lib/*.jar:$JAVA_HOME/jre/lib/*.jar' state=present
        - dest='/etc/profile' regexp='^#?\s*export PATH=(.*)JAVA_HOME(.*)$' line="export PATH=$PATH:$JAVA_HOME/bin" state=present

    ###### Since Ant in yum and apt is too old, download the .tar.bz2 file and install it
    # Install Apache Ant
    - name: Download Apache Ant
      get_url: url=http://mirror.cc.columbia.edu/pub/software/apache//ant/binaries/apache-ant--bin.tar.bz2 dest=/tmp/apache-ant--bin.tar.bz2
    - name: Untar Ant
      shell: chdir=/tmp creates=/opt/apache-ant- tar -jxf apache-ant--bin.tar.bz2 -C /opt
    - lineinfile: dest=/etc/profile regexp='^#?\s*export ANT_HOME=(.*)$' line='export ANT_HOME=/opt/apache-ant-' state=present
    - lineinfile: dest=/etc/profile regexp='^#?\s*export PATH=(.*)ANT_HOME(.*)$' line="export PATH=$PATH:$ANT_HOME/bin" state=present

    # Install Apache Maven, since there is no Maven package in yum and apt repo
    - name: Download Apache Maven
      get_url: url=http://apache.claz.org/maven/maven-3/3.1.1/binaries/apache-maven--bin.tar.gz dest=/tmp/apache-maven--bin.tar.gz
    - name: Untar Maven
      shell: chdir=/tmp creates=/opt/apache-maven- tar -zxf apache-maven--bin.tar.gz -C /opt
    - lineinfile: dest=/etc/profile regexp='^#?\s*export MAVEN_HOME=(.*)$' line='export MAVEN_HOME=/opt/apache-maven-' state=present
    - lineinfile: dest=/etc/profile regexp='^#?\s*export PATH=(.*)MAVEN_HOME(.*)$' line="export PATH=$PATH:$MAVEN_HOME/bin" state=present

    # Install the scala compiler, because the scala compiler in yum and apt repo is too old
    - name: Download Scala
      get_url: url=http://www.scala-lang.org/files/archive/scala-.tgz dest=/tmp/scala-.tgz
    - name: Untar Scala
      shell: chdir=/tmp creates=/opt/scala- tar -zxf scala-.tgz -C /opt
    - lineinfile: dest=/etc/profile regexp='^#?\s*export SCALA_HOME=(.*)$' line='export SCALA_HOME=/opt/scala-' state=present
    - lineinfile: dest=/etc/profile regexp='^#?\s*export PATH=(.*)SCALA_HOME(.*)$' line="export PATH=$PATH:$SCALA_HOME/bin" state=present

    # Install sbt
    - name: Download sbt
      get_url: url=http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt///sbt.tgz dest=/tmp/sbt-.tgz
    - name: Untar sbt
      shell: chdir=/tmp creates=/opt/sbt- tar -zxf sbt-.tgz -C /opt
    - name: Rename sbt directory
      shell: chdir=/opt creates=/opt/sbt- mv sbt/ sbt-/
    - lineinfile: dest=/etc/profile regexp='^#?\s*export SBT_HOME=(.*)$' line='export SBT_HOME=/opt/sbt-' state=present
    - lineinfile: dest=/etc/profile regexp='^#?\s*export PATH=(.*)SBT_HOME(.*)$' line="export PATH=$PATH:$SBT_HOME/bin" state=present

    # Install Go
    - name: Download Go
      get_url: url=https://go.googlecode.com/files/go1.2.linux-amd64.tar.gz dest=/tmp/go1.2.linux-amd64.tar.gz
    - name: Untar Go
      shell: chdir=/tmp creates=/opt/go tar -zxf go1.2.linux-amd64.tar.gz -C /opt
    - lineinfile: dest=/etc/profile regexp='^#?\s*export GOROOT=(.*)$' line='export GOROOT=/opt/go' state=present
    - lineinfile: dest=/etc/profile regexp='^#?\s*export PATH=(.*)GOROOT(.*)$' line="export PATH=$PATH:$GOROOT/bin" state=present
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Restore Octopress at a new computer]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140128"/>
    <updated>2014-01-28T00:04:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/restore-octopress-at-a-new-computer</id>
    <content type="html"><![CDATA[<p>OS: Ubuntu 12.04 64-bit</p>

<h2 id="install-ruby">1. Install ruby</h2>

<h3 id="install-ruby-via-rvm">1.1 Install ruby via RVM</h3>

<pre><code>$ \curl -sSL https://get.rvm.io | bash -s stable --ruby
</code></pre>

<h3 id="integrating-rvm-with-gnome-terminalhttpsrvmiointegrationgnome-terminal">1.2 <a href="https://rvm.io/integration/gnome-terminal">Integrating RVM with gnome-terminal</a></h3>
<p><code>/etc/profile</code>, <code>~/.bash_profile</code> are for login shell, and <code>~/.bashrc</code> is for interactive shell, and RVM’s path is added to <code>~/.bash_profile</code>, so you need to set the shell as a login shell.</p>

<h3 id="give-it-a-try">1.3 Give it a try</h3>
<p>Exit current shell, and open a new shell,</p>

<pre><code>ruby -v
</code></pre>

<p>You have successfully installed ruby.</p>

<h2 id="install-python">2. Install Python</h2>

<pre><code>$ sudo apt-get install -y python
</code></pre>

<p>Because <a href="http://pygments.org/">Pygments</a> syntax highlighting needs Python.</p>

<h2 id="clone-your-blog-to-the-new-machine">3. Clone your blog to the new machine</h2>
<p>First you need to clone the <code>source</code> branch to the local octopress folder.</p>

<pre><code>$ git clone -b source git@github.com:username/username.github.com.git octopress
</code></pre>

<!-- more -->
<p>Then clone the <code>master</code> branch to the <code>_deploy</code> subfolder.</p>

<pre><code>$ cd octopress
$ git clone git@github.com:username/username.github.com.git _deploy 
</code></pre>

<p>Then run the rake installation to configure everything</p>

<pre><code>$ gem install bundler
$ bundle install
</code></pre>

<p>NOW you’ve setup with a new local copy of your Octopress blog.</p>

<p>You don’t need to run <code>rake setup_github_pages</code> any more.</p>

<h2 id="blogging-at-more-than-one-computer">4. Blogging at more than one computer</h2>

<h3 id="pushing-changes">4.1 Pushing changes</h3>
<p>If you want to blog at more than one computer, you need to make sure that you push everything before switching computers. From the first machine do the following whenever you’ve made changes:</p>

<pre><code>$ rake new_post["hello world"] 
$ rake generate
$ rake deploy
</code></pre>

<p>This will generate your blog, copy the generated files into <code>_deploy/</code>, add them to git, commit and push them up to the master branch, see <a href="http://octopress.org/docs/deploying/github/">Deploying to Github Pages</a>. 
Don’t forget to commit the source for your blog.</p>

<pre><code>$ git add .
$ git commit -am "Some comment here." 
$ git push origin source  # update the remote source branch 
</code></pre>

<h3 id="pull-changes-at-another-computer">4.2 Pull changes at another computer</h3>

<pre><code>$ cd octopress
$ git pull origin source  # update the local source branch
$ cd ./_deploy
$ git pull origin master  # update the local master branch
</code></pre>

<h2 id="reference">Reference</h2>
<p><a href="http://blog.zerosharp.com/clone-your-octopress-to-blog-from-two-places/">Clone Your Octopress to Blog From Two Places</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ansible 快速入门]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140127"/>
    <updated>2014-01-27T12:10:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/ansible-tutorial</id>
    <content type="html"><![CDATA[<p>Ansible 是一个比Puppet, Chef 更轻量的provisioning 工具，不需要启动daemon进程。这点跟跟pssh差不多，但是比pssh更加强大。</p>

<h2 id="section">前提</h2>
<ul>
  <li>所使用的remote_user要能够SSH无密码登录到所有机器，配置方法见<a href="http://www.yanjiuyanjiu.com/blog/20120102/">SSH无密码登录的配置</a></li>
  <li>
    <p>remote_use sudo的时候不需要密码，配置方法如下，</p>

    <pre><code>  sudo chmod +w /etc/sudoers
  sudo vim /etc/sudoers
</code></pre>

    <p>找到 <code>root    ALL=(ALL:ALL) ALL</code>，在下面添加一行</p>

    <pre><code> username    ALL=(ALL:ALL) NOPASSWD:ALL
</code></pre>

    <p>保存退出，然后恢复为只读，</p>

    <pre><code>  sudo chmod +w /etc/sudoers 
</code></pre>
  </li>
</ul>

<p>如果忘记了以上两点，运行任何ansible命令的时候，会卡住不动很久。</p>

<p>如果发现在 “GATHERING FACTS”这里卡住，多半是sudo需要密码，试试加上-K选项，例如<code>ansible-playbook -K playbook.yml</code>，参考<a href="https://groups.google.com/forum/#!topic/ansible-project/FL0mxyxOo4M">Running ansible on local Linux desktop hangs on Gathering Facts</a>。</p>

<p><code>-vvvv</code>表示调试模式，加上后会输出很多中间信息，帮助你调试。</p>

<h2 id="ansible">1. 安装Ansible</h2>
<p>只需要在一台机器上安装，其他机器不需要安装任何东西，这就是ansible比puppet, chef方便的地方。</p>

<pre><code>sudo yum install ansible
</code></pre>

<p>或者</p>

<pre><code>sudo apt-get install ansible
</code></pre>

<p>在<code>/etc/ansible/hosts</code>添加想要操作的机器(这个<code>hosts</code>文件也叫做<a href="http://docs.ansible.com/intro_inventory.html">Inventory</a>)，且这些机器都是能<a href="http://www.yanjiuyanjiu.com/blog/20120102">SSH无密码登录的</a>，然后测试一下：</p>

<pre><code>ansible all -a "/bin/echo hello"
</code></pre>

<p>如果都成功了，说明安装成功。</p>

<p>使用ansible有两种方式：Ad-hoc command 和 playbook。前者用于临时类批量操作，后者用于配置管理，类似与Puppet。</p>

<h2 id="ad-hoc-commands">2. Ad-Hoc Commands</h2>
<p>Ad-hoc命令的形式一般如下：</p>

<pre><code>ansible groupname -m module -a arguments
</code></pre>

<!-- more -->
<p>例如：</p>

<pre><code>ansible all -m yum -a "name=wget state=present"
</code></pre>

<p>参考<a href="http://docs.ansible.com/intro_adhoc.html">Introduction To Ad-Hoc Commands</a></p>

<h2 id="playbook">3. Playbook</h2>

<p>对于稳定的配置，就要使用playbook了。</p>

<p>一个playbook由多个play组成，一个play由多个task组成，参考<a href="http://docs.ansible.com/playbooks_intro.html">Intro to Playbooks</a>。</p>

<p>一个playbook的文件内容通常是如下形式：</p>

<pre><code>---
- hosts: groupname
  remote_user: yourname
  sudo: yes

  tasks:
    - task1
    - task2
</code></pre>

<p>一个play的文件内容通常是如下形式：</p>

<pre><code>- task1
- task2
</code></pre>

<p>例如，批量安装wget和gcc的playbook，可以这么写：</p>

<pre><code>---
- hosts: all
  remote_user: username
  sudo: yes
    
  tasks:
    - yum: name=wget state=present
      when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'
    - yum: name=gcc state=present
      when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'
</code></pre>

<p>这个play包含了2个task。执行一下这个playbook，看看效果:</p>

<pre><code>ansible-playbook wget_playbook.yml
</code></pre>

<p>接下来再举一个例子，我们写两个play，<code>play_debian.yml</code>和<code>play_centos.yml</code>，以及一个playbook，<code>playbook.yml</code>。</p>

<p>play_cenos.yml:</p>

<pre><code>- yum: name=wget state=present
- yum: name=gcc state=present
</code></pre>

<p>play_deban.yml:</p>

<pre><code>- apt: pkg=wget state=present
- apt: pkg=gcc state=present
</code></pre>

<p>playbook.yml:</p>

<pre><code>---
- hosts: all
  remote_user: username
  sudo: yes

  tasks:
    - include: play_centos.yml
      when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'
    - include: play_debian.yml
      when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'
</code></pre>

<p>执行这个playbook:</p>

<pre><code>ansible-playbook playbook.yml
</code></pre>

<h2 id="section-1">4. 进阶</h2>
<p>想要进一步了解ansible，可以学习官网的例子, <a href="https://github.com/ansible/ansible-examples/">ansible examples</a>。</p>

<p>一定要仔细阅读官网给出的最佳实践规范，<a href="http://docs.ansible.com/playbooks_best_practices.html">Best Practices</a>。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[/boot 目录空间不足]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140126"/>
    <updated>2014-01-26T22:07:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/boot-space-insufficient</id>
    <content type="html"><![CDATA[<p>今天在服务器上执行 <code>sudo yum -y update</code>的时候报错：</p>

<blockquote>
  <p>… needs 18MB on the /boot filesystem</p>
</blockquote>

<h2 id="section">1. 列出所有的内核文件</h2>

<pre><code>rpm -q kernel
kernel-2.6.32-431.el6.x86_64
kernel-2.6.32-431.3.1.el6.x86_64
</code></pre>

<p>发现有多个内核，因此可以删除所有不再使用的内核文件，来释放空间。</p>

<h2 id="section-1">2. 查看当前正在使用的内核</h2>

<pre><code>uname -r
2.6.32-431.3.1.el6.x86_64
</code></pre>

<p>注意，如果你是刚刚 <code>yum -y update</code>过，需要重启一下，内核才会更新，不重启的话<code>uname -r</code>还是显示的旧的。</p>

<h2 id="section-2">3. 删除没有使用的内核</h2>

<pre><code>rpm -e 2.6.32-431.el6.x86_64
rpm -e xxx
</code></pre>

<p>将<code>rpm -q kernel</code>显示的内核复制粘贴到<code>xxx</code>位置。</p>

<h2 id="section-3">4. 手动删除其他文件</h2>
<p>把所有未使用的版本全部删除。</p>

<pre><code>sudo rm -rf /lib/modules/2.6.32-431.el6.x86_64
sudo rm -rf /usr/src/kernels/2.6.32-431.el6.x86_64
sudo rm -rf /usr/src/kernels/2.6.32-431.el6.x86_64.debug
sudo rm /boot/*2.6.32-431*
</code></pre>

<h2 id="grub">5. 删除grub里的条目</h2>
<p>上面的步骤做完了后，最后，把grub里未使用的内核删掉，条目序号是从0开始编号的，删除条目后，记得把<code>default</code>设置为正确的序号。</p>

<h2 id="yum-update">6. 再执行 yum update</h2>

<pre><code>sudo yum -y update
</code></pre>

<h2 id="section-4">参考资料</h2>
<p><a href="http://rajaruan.blog.51cto.com/2771737/868293">boot目录空间不足</a></p>

<p><a href="http://www.xiaohuai.com/3301">yum update -y，提示/boot 空间不足的解决方法</a></p>

<p><a href="http://forum.ubuntu.org.cn/viewtopic.php?f=97&amp;t=334647">如何卸载自己编译的内核？【已解决，方法见6L】</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rdesktop 快速入门]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140125"/>
    <updated>2014-01-25T22:05:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/rdesktop-tutorial</id>
    <content type="html"><![CDATA[<p>rdesktop是一款Linux下兼容Windows Remote Desktop Protocal(RDP)协议的客户端，可以用它连接开启了3389的windows机器。输入<code>rdesktop</code>可以看到该命令的所有选项，其中最常用的选项如下：
&gt; -u: user name
&gt; -p: password (- to prompt)
&gt; -f: full-screen mode
&gt; -g: desktop geometry (WxH)
&gt; -x: RDP5 experience (m[odem 28.8], b[roadband], l[an] or hex nr.)</p>

<p>举几个例子，</p>

<pre><code>rdesktop -u feng -p 123456 -xl -f 192.168.1.250
</code></pre>

<p>这条命令表示，-xl表示客户端和win机器在同一个局域网，因此可以画质调节到最好，-f表示全屏，这条命令最好在局域网下使用。</p>

<pre><code>rdesktop -u feng -p 123456 -xm -f 192.168.1.250
</code></pre>

<p>跟上一条命令相比，把-xl换成了-xm，画质调节到最差</p>

<pre><code>rdesktop -u feng -p 123456 -xm -g 1024x768 192.168.1.250
</code></pre>

<p>跟上一条命令相比，将全屏改成了分辨率1024x768</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[集群时间同步--架设内网NTP服务器]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140124"/>
    <updated>2014-01-24T16:40:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/cluster-time-sync-using-ntp</id>
    <content type="html"><![CDATA[<p>环境：CentOS 6.5</p>

<p>对于一个Linux集群，集群内的机器保持时间同步是很重要的，不然会出现很多问题。</p>

<p>本文主要描述如何在集群内架设一台NTP服务器，其他机器都与这台服务器保持时间同步。</p>

<h2 id="ntp">1 安装NTP</h2>
<p>在所有机器上执行，</p>

<pre><code>$ sudo yum install ntp
</code></pre>

<h3 id="section">2 调整时区</h3>
<p>把所有机器的时区调整为上海时区，即”+8000”时区。</p>

<p>先看一下机器的时区是否是对的，</p>

<pre><code>$ date -R
</code></pre>

<p>如果不是”+8000”，则要修改时区，</p>

<pre><code>$ cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
</code></pre>

<h3 id="bios">3 （可选）同步BIOS时间</h3>
<p>Linux系统上面BIOS时间与linux系统时间是分开的，所以调整了时间之后，还需要使用<code>hwclock</code>才能将修改过的时间写入BIOS中。</p>

<p>在<code>/etc/sysconfig/ntpd</code>中添加一行:</p>

<pre><code>SYNC_HWCLOCK=yes
</code></pre>

<h2 id="ntp-1">4 配置NTP服务器</h2>
<p>选择一台能够上网的机器作为NTP服务器，以后这台服务器提供时间同步服务，集群内的其他机器不需要上网去跟公共的NTP服务器同步了。</p>

<!-- more -->

<h3 id="etcntpconf">4.1 修改/etc/ntp.conf</h3>
<p>ntp只有一个配置文件, <code>/etc/ntp.conf</code>.</p>

<p>只需修改一行，找到下面这行，取消注释，</p>

<pre><code>#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap
</code></pre>

<p>变成了</p>

<pre><code>restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap
</code></pre>

<p>这一行的含义是允许所有IP为<code>192.168.1.x</code>的机器与本服务器同步，这样就把这台机器变成了一台NTP服务器，对别的机器提供NTP同步服务。</p>

<h3 id="ntpd">4.2 开机启动ntpd</h3>

<pre><code>$ sudo chkconfig ntpd on
</code></pre>

<h3 id="ntpd-1">4.3 启动ntpd</h3>

<pre><code>$ sudo service ntpd start
</code></pre>

<h3 id="section-1">4.4 验证与状态检查</h3>

<h4 id="ntp-2">4.4.1 查看ntp的端口</h4>

<pre><code>$ netstat -unlnp
</code></pre>

<p>应该看到123端口</p>

<h4 id="ntp-3">4.4.2 查看ntp服务器有无和上层连通</h4>

<pre><code>$ ntpstat

synchronised to NTP server (218.75.4.130) at stratum 3 
   time correct to within 598 ms
   polling server every 64 s
</code></pre>

<h4 id="ntp-4">4.4.3 查看ntp服务器与上层间的联系</h4>

<pre><code>$ ntptrace -n 127.0.0.1

127.0.0.1: stratum 3, offset -0.001095, synch distance 0.532610
116.193.83.174: timed out, nothing received
</code></pre>

<h4 id="ntpntp">4.4.4 查看ntp服务器与上层ntp服务器的状态</h4>

<pre><code>$ ntpq -p
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
+dns.sjtu.edu.cn 202.112.31.197   3 u   47   64    3   28.873   76.001 114.194
+Hshh.org        209.51.161.238   2 u   48   64    3   43.694   75.042 131.058
+202.118.1.130   202.118.1.46     2 u   46   64    3   14.640   75.636 116.999
*dns1.synet.edu. 202.118.1.46     2 u   44   64    3   13.968   74.514 128.913
</code></pre>

<p>其中:</p>

<ul>
  <li>remote - 本机和上层ntp的ip或主机名，“+”表示优先，“*”表示次优先</li>
  <li>refid - 参考上一层ntp主机地址</li>
  <li>st - stratum阶层</li>
  <li>t: 这个…..我也不知道啥意思^_^</li>
  <li>when - 多少秒前曾经同步过时间</li>
  <li>poll - 下次更新在多少秒后</li>
  <li>reach - 已经向上层ntp服务器要求更新的次数</li>
  <li>delay - 网络延迟</li>
  <li>offset - 时间补偿</li>
  <li>jitter - 系统时间与bios时间差</li>
</ul>

<p>如果所有远程服务器的jitter值是4000并且delay和reach的值是0，那么说明时间同步是有问题的，可能的原因是防火墙阻断了与server之间的通讯，检查一下123端口是否正常开放。</p>

<h2 id="section-2">5 配置客户机</h2>
<p>服务器配置好了，接下来就要配置所有的客户端机器，从该服务器同步时间。</p>

<ul>
  <li>方法1，使用<code>ntpdate</code>与上面配置的NTP服务器定时同步（参考资料2），不推荐此方法</li>
  <li>方法2，安装ntpd，指定NTP服务器为上面配置的服务器地址，推荐。</li>
</ul>

<p>下面详细讲述方法2。以下操作适用于所有客户端机器。</p>

<h3 id="ntp-5">5.1 指定NTP服务器</h3>

<p>删除 <code>/etc/ntp.conf</code> 里的所有公网ntp服务器，换成上面配置的服务器，</p>

<pre><code>#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server techwolf-01 iburst
</code></pre>

<p>用hostname或ip都可以。</p>

<h3 id="ntpd-2">5.2 开机启动ntpd</h3>

<pre><code>$ sudo chkconfig ntpd on
</code></pre>

<h3 id="ntpd-3">5.3 启动ntpd</h3>

<pre><code>$ sudo service ntpd start
</code></pre>

<h2 id="section-3">参考资料</h2>

<ol>
  <li><a href="http://www.crsay.com/wiki/wiki.php/server/centos/ntp-set">CentOS配置时间同步NTP</a></li>
  <li><a href="http://www.cnblogs.com/thinksasa/p/3479980.html">CentOS系统时间同步（NTP）</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CentOS 6.5 升级内核到 3.10.28]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140123"/>
    <updated>2014-01-23T00:02:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/centos-6-dot-4-upgrade-kernel</id>
    <content type="html"><![CDATA[<p>本文适用于CentOS 6.4, CentOS 6.5，亲测可行，估计也适用于其他Linux发行版。</p>

<h2 id="section">1. 准备工作</h2>

<h3 id="section-1">1.1 下载源码包</h3>
<p>Linux内核版本有两种：稳定版和开发版 ，Linux内核版本号由3个数字组成：r.x.y</p>

<ul>
  <li>r: 主版本号</li>
  <li>x: 次版本号，偶数表示稳定版本；奇数表示开发中版本。</li>
  <li>y: 修订版本号 ， 表示修改的次数</li>
</ul>

<p>去 <a href="http://www.kernel.org">http://www.kernel.org</a> 首页，可以看到有stable, longterm等版本，longterm是比stable更稳定的版本，会长时间更新，因此我选择 3.10.28，</p>

<pre><code>wget  https://www.kernel.org/pub/linux/kernel/v3.x/linux-3.10.28.tar.xz
</code></pre>

<h3 id="section-2">1.2 解压</h3>

<pre><code>tar -xf linux-3.10.28.tar.xz
</code></pre>

<h3 id="section-3">1.3 更新当前系统</h3>

<pre><code>sudo yum update
sudo yum upgrade
</code></pre>

<h3 id="section-4">1.4 安装编译内核所需要的软件包</h3>

<pre><code>sudo yum groupinstall "Development Tools" # 一口气安装编译时所需的一切工具
sudo yum install ncurses-devel #必须这样才能让 make *config 这个指令正确地执行。
sudo yum install qt-devel #如果你没有 X 环境，这一条可以不用
sudo yum install hmaccalc zlib-devel binutils-devel elfutils-libelf-devel #创建 CentOS-6 内核时需要它们
</code></pre>

<h2 id="section-5">2 配置文件</h2>

<h3 id="section-6">2.1 查看当前系统内核</h3>

<pre><code>uname -r
2.6.32-358.11.1.el6.x86_64
</code></pre>

<h3 id="section-7">2.2 将当前系统的配置文件拷贝到当前目录</h3>

<pre><code>cp /boot/config-2.6.32-358.11.1.el6.x86_64 .config
</code></pre>

<h3 id="section-8">2.3 使用旧内核配置，并自动接受每个新增选项的默认设置</h3>

<pre><code>sh -c 'yes "" | make oldconfig'
</code></pre>

<p><code>make oldconfig</code>会读取当前目录下的<code>.config</code>文件，在<code>.config</code>文件里没有找到的选项则提示用户填写，然后备份<code>.config</code>文件为<code>.config.old</code>，并生成新的<code>.config</code>文件，参考 <a href="http://stackoverflow.com/questions/4178526/what-does-make-oldconfig-do-exactly-linux-kernel-makefile">http://stackoverflow.com/questions/4178526/what-does-make-oldconfig-do-exactly-linux-kernel-makefile</a></p>

<h2 id="section-9">3 编译</h2>

<pre><code>sudo make -j8 bzImage #生成内核文件
sudo make -j8 modules #编译模块
sudo make -j8 modules_install #编译安装模块
</code></pre>

<p>要严格按照这个顺序进行编译，<strong>不能合并成一句</strong>，<code>sudo make -j8 bzImage modules modules_install</code>。</p>

<p><code>-j</code>后面的数字是线程数，用于加快编译速度，一般的经验是，有多少G内存，就填写那个数字，例如有8G内存，则为<code>-j8</code>。</p>

<h2 id="section-10">4 安装</h2>

<pre><code>sudo make install
</code></pre>

<p>如果出现了 <code>ERROR: modinfo: could not find module xxx</code>，数量少的话，可以忽略。</p>

<h2 id="grub">5 修改Grub引导顺序</h2>

<p>安装完成后，需要修改Grub引导顺序，让新安装的内核作为默认内核。</p>

<p>编辑 <code>grub.conf</code>文件，</p>

<pre><code>sudo vim /etc/grub.conf
</code></pre>

<p>数一下刚刚新安装的内核在哪个位置，从0开始，然后设置default为那个数字，一般新安装的内核在第一个位置，所以设置<code>default=0</code>。</p>

<h2 id="section-11">6 重启</h2>

<pre><code>sudo reboot
</code></pre>

<p>重启后，看一下当前内核版本号，</p>

<pre><code>uname -r
3.10.28
</code></pre>

<p>成功啦！！</p>

<h2 id="section-12">7 如果失败，则重新循环</h2>

<p>如果失败，重新开始的话，要清理上次编译的现场 </p>

<pre><code>make mrproper #清理上次编译的现场 
</code></pre>

<p>然后转到第2步，重新开始。</p>

<h2 id="section-13">参考资料</h2>
<ul>
  <li><a href="http://xmodulo.com/2013/07/how-to-upgrade-the-kernel-on-centos.html">How to upgrade the kernel on CentOS</a></li>
  <li><a href="http://winotes.net/centos-64-upgrade-to-kernel-3x.html">CentOS 6.4 升级到 3.x Kernel</a></li>
  <li><a href="http://my.oschina.net/qichang/blog/101542">CentOS Linux 升级内核步骤和方法</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在Centos 6.5上安装docker]]></title>
    <link href="http://www.yanjiuyanjiu.com/blog/20140122"/>
    <updated>2014-01-22T15:25:00+08:00</updated>
    <id>http://www.yanjiuyanjiu.com/blog/install-docker-on-centos65</id>
    <content type="html"><![CDATA[<h2 id="enable-epel-repo-on-centos">1 Enable EPEL Repo on CentOS</h2>

<p>参考 <a href="http://www.centosblog.com/enable-epel-repo-on-centos-5-and-centos-6/">Enable EPEL Repo on CentOS 5 and CentOS 6</a></p>

<pre><code>rpm -Uvh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm
</code></pre>

<h2 id="install-docker">2 Install docker</h2>

<pre><code>yum install docker-io --enablerepo=epel
</code></pre>

<h2 id="docker-daemon-">3 启动 docker daemon 进程</h2>

<pre><code>sudo docker -d &amp;
</code></pre>

<p>这时，有警告，说内核版本过低，</p>

<blockquote>
  <p>WARNING: You are running linux kernel version 2.6.32-431.el6.x86_64, which might be unstable running docker. Please upgrade your kernel to 3.8.0.</p>
</blockquote>

<p>如果你在公司，且公司内部都是通过代理上网，则可以把代理服务器告诉docker，用如下命令(参考<a href="https://github.com/dotcloud/docker/issues/402">这里</a>)：</p>

<pre><code>sudo HTTP_PROXY=http://xxx:port docker -d &amp;
</code></pre>

<h2 id="section">4 升级内核</h2>

<p>见我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20131024">CentOS 6.4 升级内核到 3.11.6</a></p>

<h2 id="ubuntu-">5 下载 ubuntu 镜像</h2>

<pre><code>sudo docker pull ubuntu
</code></pre>

<h2 id="hello-world">6 运行 hello world</h2>

<pre><code>sudo docker run ubuntu /bin/echo hello world
hello world
</code></pre>

<p>安装成功了！！</p>
]]></content>
  </entry>
  
</feed>
