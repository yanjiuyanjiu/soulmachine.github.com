
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>研究研究</title>
  <meta name="author" content="soulmachine">

  
  <meta name="description" content="一个关于机器学习的技术博客">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://www.yanjiuyanjiu.com">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/bootstrap/bootstrap.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/bootstrap/responsive.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/syntax/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">
  <style type="text/css">
    body {
      padding-bottom: 40px;
    }
    h1 {
      margin-bottom: 15px;
    }
    img {
      max-width: 100%;
    }
    .sharing, .meta, .pager {
      margin: 20px 0px 20px 0px;
    }
    .page-footer p {
      text-align: center;
    }
  </style>
  <script src="/javascripts/libs/jquery.js"></script>
  <script src="/javascripts/libs/modernizr-2.0.js"></script>
  <script src="/javascripts/libs/bootstrap.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="研究研究" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!-- <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css"> -->
<!-- <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css"> -->

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-7583537-4']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <nav role="navigation"><div class="navbar navbar-inverse">
  <div class="navbar-inner">
    <div class="container">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>

      <a class="brand" href="/">研究研究</a>

      <div class="nav-collapse">
        <ul class="nav">
  <li><a href="/">Home</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about/">About</a></li>
  <li><a href="/notes/">读书笔记</a></li>
</ul>


        <ul class="nav pull-right" data-subscription="rss">
          <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
          
        </ul>

        
          <form class="pull-right navbar-search" action="http://google.com/search" method="get">
            <fieldset role="search">
              <input type="hidden" name="q" value="site:www.yanjiuyanjiu.com" />
              <input class="search-query" type="text" name="q" results="0" placeholder="Search"/>
            </fieldset>
          </form>
        
      </div>
    </div>
  </div>
</div>
</nav>
  <div class="container">
    <div class="row-fluid">
      <div class="span9">
  
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/20131223">运行mahout的朴素贝叶斯分类器</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-12-23T17:20:00+08:00" pubdate data-updated="true">Dec 23<span>rd</span>, 2013</time>
        
		
         | <a href="/blog/20131223#duoshuo_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="section">1.准备数据</h2>

<h3 id="section-1">1.1 下载数据集，并解压</h3>

<pre><code>wget http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz
tar -xf 20news-bydate.tar.gz
#上传到hdfs
hadoop fs -put 20news-bydate-test .
hadoop fs -put 20news-bydate-train .
</code></pre>

<h3 id="section-2">1.2 转换格式</h3>

<pre><code>#转换为序列文件(sequence files)
mahout seqdirectory -i 20news-bydate-train -o 20news-bydate-train-seq
mahout seqdirectory -i 20news-bydate-test -o 20news-bydate-test-seq
#转换为tf-idf向量
mahout seq2sparse -i 20news-bydate-train-seq -o 20news-bydate-train-vector -lnorm -nv -wt tfidf
mahout seq2sparse -i 20news-bydate-test-seq -o 20news-bydate-test-vector -lnorm -nv -wt tfidf
</code></pre>

<h2 id="section-3">2. 训练朴素贝叶斯模型</h2>

<pre><code>mahout trainnb -i 20news-bydate-train-vectors/tfidf-vectors -el -o model -li labelindex -ow
</code></pre>

<h2 id="section-4">3. 测试朴素贝叶斯模型</h2>

<pre><code>mahout testnb -i 20news-bydate-train-vectors/tfidf-vectors -m model -l labelindex -ow -o test-result
</code></pre>

<h2 id="section-5">4. 查看训练后的结构</h2>

<pre><code>mahout seqdumper -i labelindex 

Input Path: labelindex
Key class: class org.apache.hadoop.io.Text Value Class: class org.apache.hadoop.io.IntWritable
Key: alt.atheism: Value: 0
Key: comp.graphics: Value: 1
Key: comp.os.ms-windows.misc: Value: 2
Key: comp.sys.ibm.pc.hardware: Value: 3
Key: comp.sys.mac.hardware: Value: 4
Key: comp.windows.x: Value: 5
Key: misc.forsale: Value: 6
Key: rec.autos: Value: 7
Key: rec.motorcycles: Value: 8
Key: rec.sport.baseball: Value: 9
Key: rec.sport.hockey: Value: 10
Key: sci.crypt: Value: 11
Key: sci.electronics: Value: 12
Key: sci.med: Value: 13
Key: sci.space: Value: 14
Key: soc.religion.christian: Value: 15
Key: talk.politics.guns: Value: 16
Key: talk.politics.mideast: Value: 17
Key: talk.politics.misc: Value: 18
Key: talk.religion.misc: Value: 19
Count: 20
</code></pre>

</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/20131027">使用docker打造spark集群</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-10-27T20:30:00+08:00" pubdate data-updated="true">Oct 27<span>th</span>, 2013</time>
        
		
         | <a href="/blog/20131027#duoshuo_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>前提条件：</strong>安装好了docker，见我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20131025">Docker安装</a></p>

<p>有两种方式，</p>

<ul>
  <li><a href="https://github.com/apache/incubator-spark">Spark官方repo</a>里，docker文件夹下的脚本。官方的这个脚本封装很薄，尽可能把必要的信息展示出来。</li>
  <li><a href="https://github.com/amplab/docker-scripts">AMPLab开源的这个独立小项目</a>，来打造一个spark集群。这个脚本封装很深，自带了一个DNS服务器，还有hadoop，非常自动化，缺点是很多信息看不到了。</li>
</ul>

<h1 id="section">1. 第1种方式</h1>

<h2 id="git-clone-">git clone 源码</h2>
<p>首先要把官方repo的代码下载下来</p>

<pre><code>git clone git@github.com:apache/incubator-spark.git
</code></pre>

<h2 id="apt">（可选）修改apt源</h2>
<p>在国内，将apt源修改国内源，例如163的源，速度会快很多。将<code>base/Dockerfile</code>里的</p>

<pre><code>RUN echo "deb http://archive.ubuntu.com/ubuntu precise main universe" &gt; /etc/apt/sources.list
</code></pre>

<p>替换为</p>

<pre><code>RUN echo "deb http://mirrors.163.com/ubuntu/ precise main restricted universe multiverse" &gt; /etc/apt/sources.list
RUN echo "deb http://mirrors.163.com/ubuntu/ precise-security main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb http://mirrors.163.com/ubuntu/ precise-updates main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb http://mirrors.163.com/ubuntu/ precise-proposed main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb http://mirrors.163.com/ubuntu/ precise-backports main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise-security main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise-updates main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise-proposed main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
RUN echo "deb-src http://mirrors.163.com/ubuntu/ precise-backports main restricted universe multiverse" &gt;&gt; /etc/apt/sources.list
</code></pre>

<h2 id="build">build镜像</h2>
<p>将<code>build</code>和<code>spark-test/build</code>里的<code>docker</code>命令前，添加<code>sudo</code>，然后执行<code>docker</code>下的<code>build</code></p>

<pre><code>cd docker
./build
</code></pre>

<h2 id="master">启动master</h2>

<pre><code>sudo docker run -v $SPARK_HOME:/opt/spark spark-test-master
</code></pre>

<h2 id="worker">启动worker</h2>
<p>新开一个终端窗口（强烈推荐tmux），启动一个worker</p>

<pre><code>sudo docker run -v $SPARK_HOME:/opt/spark spark-test-worker &lt;master_ip&gt;
</code></pre>

<p>可以在master终端窗口看到worker注册上来了。</p>

<p>可以再开多个终端窗口，启动多个worker。</p>

<h1 id="section-1">2. 第2种方式</h1>

<h2 id="wget">升级wget</h2>
<p>如果发现wget不识别<code>--no-proxy</code>选项，需要升级wget。</p>

<h2 id="section-2">下载镜像</h2>
<p>为了让脚本第一次执行的时候更快，还是手动下载所有的镜像吧，amplab在index.docker.io上有一个官方账号，把这个账号有关spark的repo都pull下来。</p>

<pre><code>sudo docker pull amplab/apache-hadoop-hdfs-precise
sudo docker pull amplab/dnsmasq-precise
sudo docker pull amplab/spark-worker
sudo docker pull amplab/spark-master
sudo docker pull amplab/spark-shell
</code></pre>

<h2 id="git-clone--1">git clone 脚本</h2>

<pre><code>git@github.com:amplab/docker-scripts.git
</code></pre>

<p>这个脚本可以一键启动集群，爽啊哈哈哈！</p>

<h2 id="spark">一键启动spark集群</h2>

<pre><code>sudo ./deploy/deploy.sh -i amplab/spark:0.8.0 -w 3 
</code></pre>

<h2 id="spark-shell">启动 Spark shell</h2>
<p>启动一个交互式shell吧，IP为上一步输出的Master的IP</p>

<pre><code>sudo docker run -i -t -dns 172.17.0.90 amplab/spark-shell:0.8.0
</code></pre>

<h2 id="section-3">运行一个简单的的例子</h2>

<pre><code>scala&gt; val textFile = sc.textFile("hdfs://master:9000/user/hdfs/test.txt")
scala&gt; textFile.count()
scala&gt; textFile.map({line =&gt; line}).collect()
</code></pre>

<h2 id="section-4">关闭集群</h2>

<pre><code>$ sudo ./deploy/kill_all.sh spark
$ sudo ./deploy/kill_all.sh nameserver
</code></pre>

<h2 id="section-5">更多详情请参考项目主页的文档</h2>

<p><a href="https://github.com/amplab/docker-scripts">https://github.com/amplab/docker-scripts</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/20131026">Docker 快速入门</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-10-26T23:41:00+08:00" pubdate data-updated="true">Oct 26<span>th</span>, 2013</time>
        
		
         | <a href="/blog/20131026#duoshuo_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>前提条件：</strong>要安装好 docker，见我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20131025">docker 安装</a></p>

<h2 id="section">交互式命令行入门教程</h2>
<p>首先强烈建议玩一遍官方的一个交互式命令行入门教程，<a href="http://www.docker.io/gettingstarted/">Interactive commandline tutorial</a>。甚至要多玩几遍，加深印象。</p>

<p>初次过完这个教程，感觉docker用起来跟git很类似。</p>

<p>玩完了后，在自己的真实机器上，把上面的命令重新敲一遍，感受一下。</p>

<h2 id="hello-world">Hello World</h2>
<p>参考官方文档<a href="http://docs.docker.io/en/latest/examples/hello_world/#running-examples">Hello World</a></p>

<p>首先下载官方的ubuntu image:</p>

<pre><code>sudo docker pull ubuntu
</code></pre>

<p>然后运行 hello world：</p>

<pre><code>sudo docker run ubuntu /bin/echo hello world
</code></pre>

<h2 id="section-1">三种运行命令的模式</h2>
<p>docker 有三种运行命令的方式，短暂方式，交互方式，daemon方式。</p>

<p><strong>短暂方式</strong>，就是刚刚的那个”hello world”，命令执行完后，container就终止了，不过并没有消失，可以用 <code>sudo docker ps -a</code> 看一下所有的container，第一个就是刚刚执行过的container，可以再次执行一遍：</p>

<pre><code>sudo docker start container_id
</code></pre>

<p>不过这次看不到”hello world”了，只能看到ID，用<code>logs</code>命令才能看得到，</p>

<pre><code>sudo docker logs container_id
</code></pre>

<p>可以看到两个”hello world”，因为这个container运行了两次。</p>

<p><strong>交互方式</strong>，</p>

<pre><code>sudo docker run -i -t image_name /bin/bash
</code></pre>

<p><strong>daemon方式</strong>，即让软件作为长时间服务运行，这就是SAAS啊！</p>

<p>例如，一个无限循环打印的脚本（替换为memcached, apache等，操作方法仍然不变！）：</p>

<pre><code>CONTAINER_ID=$(sudo docker run -d ubuntu /bin/sh -c "while true; do echo hello world; sleep 1; done")
</code></pre>

<p>在container外面查看它的输出</p>

<pre><code>sudo docker logs $CONTAINER_ID
</code></pre>

<p>或者连接上容器实时查看</p>

<pre><code>sudo docker attach $CONTAINER_ID
</code></pre>

<p>终止容器</p>

<pre><code>sudo docker stop $CONTAINER_ID
</code></pre>

<p><code>sudo docker ps</code>看一下，已经没了</p>

<h2 id="docker-ps-">docker ps 命令详解</h2>
<p><code>sudo docker ps</code>，列出当前所有正在运行的container</p>

<p><code>sudo docker ps -l</code>，列出最近一次启动的，且正在运行的container</p>

<p><code>sudo docker ps -a</code>，列出所有的container</p>

<p>其他用法请参考 <code>sudo docker ps -h</code></p>

<p>还有一种方式可以让程序在daemon模式下运行，就是在Dockerfile里设置USER为daemon，见<a href="http://www.docker.io/learn/dockerfile/level2/">Dockerfile tutorial Level2</a>。</p>

<h2 id="http">添加http代理</h2>
<p>在国内，pull或push的时候经常连不上docker.com（原因你懂的，或者在公司内部统一用一个代理上网的时候），可以在docker daemon进程启动的时候加个代理，例如</p>

<pre><code>sudo HTTP_PROXY=proxy_server:port docker -d &amp;
</code></pre>

<p>docker貌似是不识别<code>http_proxy</code>, <code>https_proxy</code>和<code>no_proxy</code>环境变量的，因此要在命令行里指定，参考 <a href="https://github.com/dotcloud/docker/issues/402">Github Issue #402 Using Docker behind a firewall</a>。</p>

<p>如果在命令行里指定了<code>HTTP_PROXY</code>，则要unset掉<code>http_proxy</code>和<code>https_proxy</code>环境变量。原因是：</p>

<ul>
  <li>首先， docker daemon进程是通过http协议与docker.com通信的</li>
  <li>其次，docker的各种命令（例如 <code>run</code>, <code>login</code>等）也是通过http协议与docker daemon进程通信的（发送jasn字符串，daemon进程返回的也是json字符串），有时候docker客户端命令貌似能识别http_proxy变量，这时，客户端发送一个命令，路径是<code>localhost-&gt;http_proxy-&gt;daemon进程</code>，daemon进程返回的数据，路径是 <code>daemon进程-&gt;proxy-&gt;proxy-&gt;localhost</code>，其中，从<code>proxy-&gt;localhost</code>的路径是不通的，因为proxy连接不了内网IP。</li>
</ul>

<p>之所以把这一步放在本文开始，是因为这一步不做的话，后面很多命令会出错，让人摸不着头脑，我在这里就掉进坑了，花了很长时间才搞明白，原来是网络连接不稳定。</p>

<h2 id="dockerfile">熟悉一下 Dockerfile</h2>
<p>完了几遍交互式入门教程后，你会好奇，怎么自己定制一个 image，例如把常用的软件装好后打包 ? 这时候该 Dockfile 登场了。Dockerfile 实质上是一个脚本文件，用于自动化创建image。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/20131026">继续阅读 &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/20131025">Docker安装</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-10-25T23:41:00+08:00" pubdate data-updated="true">Oct 25<span>th</span>, 2013</time>
        
		
         | <a href="/blog/20131025#duoshuo_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="centos-64--docker">1 在 CentOS 6.4 上安装 docker</h2>

<p>docker当前官方只支持Ubuntu，所以在 CentOS 安装Docker比较麻烦(<a href="https://github.com/dotcloud/docker/issues/172">Issue #172</a>)。</p>

<p>docker官方文档说要求Linux kernel至少3.8以上，CentOS 6.4是2.6的内核，于是我哼哧哼哧的<a href="http://www.yanjiuyanjiu.com/blog/20131024">编译安装了最新的kernel 3.11.6</a>，重启后运行docker还是失败，最后找到原因，是因为编译时忘记集成aufs模块了。aufs 需要和 kernel 一起编译，很麻烦。</p>

<p>不过不需要这么麻烦，有强人已经编译好了带aufs模块的内核，见这里<a href="http://nareshv.blogspot.com/2013/08/installing-dockerio-on-centos-64-64-bit.html">Installing docker.io on centos 6.4 (64-bit)</a></p>

<h3 id="selinuxlxc">1.1 取消selinux，因为它会干扰lxc的正常功能</h3>

<pre><code>sudo vim /etc/selinux/config 
SELINUX=disabled
SELINUXTYPE=targeted
</code></pre>

<h3 id="fedora-epel">1.2 安装 Fedora EPEL</h3>

<pre><code>sudo yum install http://ftp.riken.jp/Linux/fedora/epel/6/x86_64/epel-release-6-8.noarch.rpm
</code></pre>

<h3 id="hop5-repo">1.3 添加 hop5 repo地址</h3>

<pre><code>cd /etc/yum.repos.d
sudo wget http://www.hop5.in/yum/el6/hop5.repo
</code></pre>

<h3 id="docker-io">1.4 安装 docker-io</h3>

<pre><code>sudo yum install docker-io
</code></pre>

<p>会自动安装带aufs模块的3.10内核，以及docker-io包。</p>

<h3 id="cgroup--etcfstab--docker">1.5 将 cgroup 文件系统添加到 <code>/etc/fstab</code> , 只有这样docker才能正常工作</h3>

<pre><code>sudo echo "none                    /sys/fs/cgroup          cgroup  defaults        0 0" &gt;&gt; /etc/fstab
</code></pre>

<h3 id="grub">1.6 修改grub引导顺序</h3>

<pre><code>sudo vim /etc/grub.conf
default=0
</code></pre>

<p>设置default为新安装的内核的位置，一般是0</p>

<h3 id="section">1.7 重启</h3>

<pre><code>sudo reboot
</code></pre>

<h3 id="section-1">1.8 检查新内核是否引导成功</h3>

<p>重启后，检查一下新内核是否引导起来了</p>

<pre><code>uname -r
3.10.5-3.el6.x86_64
</code></pre>

<p>说明成功了</p>

<p>看一下 aufs是否存在</p>

<pre><code>grep aufs /proc/filesystems 
nodev   aufs
</code></pre>

<p>说明存在</p>

<h3 id="docker-daemon-">1.9 启动 docker daemon 进程</h3>

<pre><code>sudo docker -d &amp;
</code></pre>

<p>如果你在公司，且公司内部都是通过代理上网，则可以把代理服务器告诉docker，用如下命令(参考<a href="https://github.com/dotcloud/docker/issues/402">这里</a>)：</p>

<pre><code>sudo HTTP_PROXY=http://xxx:port docker -d &amp;
</code></pre>

<h3 id="ubuntu-">1.10 下载 ubuntu 镜像</h3>

<pre><code>sudo docker pull ubuntu
</code></pre>

<h3 id="hello-world">1.11 运行 hello world</h3>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/20131025">继续阅读 &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/20131024">CentOS 6.4 升级内核到 3.11.6</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-10-24T00:02:00+08:00" pubdate data-updated="true">Oct 24<span>th</span>, 2013</time>
        
		
         | <a href="/blog/20131024#duoshuo_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="section">1. 准备工作</h2>

<h3 id="section-1">1.1 下载源码包</h3>
<p>去 <a href="http://www.kernel.org">http://www.kernel.org</a> 首页，下载源码包</p>

<pre><code>wget  https://www.kernel.org/pub/linux/kernel/v3.x/linux-3.11.6.tar.xz
</code></pre>

<h3 id="section-2">1.2 解压</h3>

<pre><code>tar xf linux-3.11.6.tar.xz
</code></pre>

<h3 id="section-3">1.3 更新当前系统</h3>

<pre><code>sudo yum update
sudo yum upgrade
</code></pre>

<h3 id="section-4">1.4 安装必要软件</h3>

<pre><code>sudo yum groupinstall "Development Tools" # 一口气安装编译时所需的一切工具
sudo yum install ncurses-devel #必须这样才能让 make *config 这个指令正确地执行。
sudo yum install qt-devel #如果你没有 X 环境，这一条可以不用
sudo yum install hmaccalc zlib-devel binutils-devel elfutils-libelf-devel #创建 CentOS-6 内核时需要它们
</code></pre>

<h2 id="section-5">2 配置文件</h2>

<h3 id="section-6">2.1 查看当前系统内核</h3>

<pre><code>uname -r
2.6.32-358.11.1.el6.x86_64
</code></pre>

<h3 id="section-7">2.2 将当前系统的配置文件拷贝到当前目录</h3>

<pre><code>cp /boot/config-2.6.32-358.11.1.el6.x86_64 .config
</code></pre>

<h3 id="section-8">2.3 使用旧内核配置，并自动接受每个新增选项的默认设置</h3>

<pre><code>sh -c 'yes "" | make oldconfig'
</code></pre>

<p><code>make oldconfig</code>会读取当前目录下的<code>.config</code>文件，在<code>.config</code>文件里没有找到的选项则提示用户填写，然后备份<code>.config</code>文件为<code>.config.old</code>，并生成新的<code>.config</code>文件，参考 <a href="http://stackoverflow.com/questions/4178526/what-does-make-oldconfig-do-exactly-linux-kernel-makefile">http://stackoverflow.com/questions/4178526/what-does-make-oldconfig-do-exactly-linux-kernel-makefile</a></p>

<h2 id="section-9">3 编译</h2>

<pre><code>sudo make -j200 bzImage #生成内核文件
sudo make -j200 modules #编译模块
sudo make -j200 modules_install #编译安装模块
</code></pre>

<p>要严格按照这个先后顺序进行编译</p>

<p><code>-j</code>后面的数字是线程数，用于加快编译速度，一般的经验是，有多少G内存，就填写那个数字，例如有8G内存，则为<code>-j8</code>。</p>

<h2 id="section-10">4 安装</h2>

<pre><code>sudo make install
</code></pre>

<p>如果出现了 <code>ERROR: modinfo: could not find module xxx</code>，数量少的话，可以忽略。</p>

<h2 id="grub">5 修改Grub引导顺序</h2>

<p>安装完成后，需要修改Grub引导顺序，让新安装的内核作为默认内核。</p>

<p>编辑 <code>grub.conf</code>文件，</p>

<pre><code>sudo vim /etc/grub.conf
</code></pre>

<p>数一下刚刚新安装的内核在哪个位置，从0开始，然后设置default为那个数字，一般新安装的内核在第一个位置，所以设置<code>default=0</code>。</p>

<h2 id="section-11">6 重启</h2>

<p>重启后，看一下当前内核版本号，</p>

<pre><code>uname -r
3.11.6
</code></pre>

<p>成功啦！！</p>

<h2 id="section-12">7 如果失败，则重新循环</h2>

<p>如果失败，重新开始的话，要清理上次编译的现场 </p>

<pre><code>make mrproper #清理上次编译的现场 
</code></pre>

<p>然后转到第2步，重新开始。</p>

<h2 id="section-13">参考资料</h2>
<ul>
  <li><a href="http://xmodulo.com/2013/07/how-to-upgrade-the-kernel-on-centos.html">How to upgrade the kernel on CentOS</a></li>
  <li><a href="http://winotes.net/centos-64-upgrade-to-kernel-3x.html">CentOS 6.4 升级到 3.x Kernel</a></li>
  <li><a href="http://my.oschina.net/qichang/blog/101542">CentOS Linux 升级内核步骤和方法</a></li>
</ul>
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/20131017">安装Spark 0.8 集群(在CentOS上)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-10-17T11:58:00+08:00" pubdate data-updated="true">Oct 17<span>th</span>, 2013</time>
        
		
         | <a href="/blog/20131017#duoshuo_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>环境</strong>:CentOS 6.4, Hadoop 1.1.2, JDK 1.7, Spark 0.8.0, Scala 2.9.3</p>

<p>Spark 0.7.2 的安装请看之前的一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20130617/">安装Spark集群(在CentOS上)</a> 。</p>

<p>Spark的安装很简单，总结起来一句话：下载，解压，然后拷贝到所有机器，完毕，无需任何配置。</p>

<h1 id="jdk-17">1. 安装 JDK 1.7</h1>
<pre><code>yum search openjdk-devel
sudo yum install java-1.7.0-openjdk-devel.x86_64
/usr/sbin/alternatives --config java
/usr/sbin/alternatives --config javac
sudo vim /etc/profile
# add the following lines at the end
export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.19.x86_64
export JRE_HOME=$JAVA_HOME/jre
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
# test
$ java -version
</code></pre>

<p>参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120423/">安装和配置CentOS服务器的详细步骤</a>。</p>

<h1 id="scala-293">2. 安装 Scala 2.9.3</h1>
<p>Spark 0.8.0 依赖 Scala 2.9.3, 我们必须要安装Scala 2.9.3.</p>

<p>下载 <a href="http://www.scala-lang.org/downloads/distrib/files/scala-2.9.3.tgz">scala-2.9.3.tgz</a> 并 保存到home目录.</p>

<pre><code>$ tar -zxf scala-2.9.3.tgz
$ sudo mv scala-2.9.3 /usr/lib
$ sudo vim /etc/profile
# add the following lines at the end
export SCALA_HOME=/usr/lib/scala-2.9.3
export PATH=$PATH:$SCALA_HOME/bin
# save and exit vim
#make the bash profile take effect immediately
source /etc/profile
# test
$ scala -version
</code></pre>

<h1 id="spark">3. 下载预编译好的Spark</h1>
<p>下载预编译好的Spark, <a href="http://spark-project.org/download/spark-0.8.0-incubating-bin-hadoop1.tgz">spark-0.8.0-incubating-bin-hadoop1.tgz</a>. </p>

<p>如果你想从零开始编译，则下载源码包，但是我不建议你这么做，因为有一个Maven仓库，twitter4j.org, 被墙了，导致编译时需要翻墙，非常麻烦。如果你有DIY精神，并能顺利翻墙，则可以试试这种方式。</p>

<h1 id="local">4. Local模式</h1>

<h2 id="section">4.1 解压</h2>

<pre><code>$ tar -zxf spark-0.8.0-incubating-bin-hadoop1.tgz
</code></pre>

<h2 id="sparkhome">4.2 （可选）设置 SPARK_HOME环境变量</h2>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.8.0
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="sparkpi">4.3 现在可以运行SparkPi了</h2>

<pre><code>$ cd $SPARK_HOME
$ ./run-example org.apache.spark.examples.SparkPi local
</code></pre>

<h1 id="cluster">5. Cluster模式</h1>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/20131017">继续阅读 &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/20130829">简洁的Scala</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-08-29T21:38:00+08:00" pubdate data-updated="true">Aug 29<span>th</span>, 2013</time>
        
		
         | <a href="/blog/20130829#duoshuo_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Scala语言是很注重一致性(consistency)的，Scala的简洁性(concision)都是由其一致性带来的。</p>

<p>Scala的看上去很复杂，但是它在概念上是非常一致的。弄清了几个概念后，也就不觉得复杂了，反倒是比Java的简单。</p>

<h1 id="oo--fp">1. OO + FP</h1>

<h2 id="section">1.1 一切都是对象</h2>

<p>更精确地说，应该是“一切值都是对象”。</p>

<ul>
  <li>
    <p>整数, 浮点数等基本类型(primitive type)是对象  </p>

    <pre><code>  123.toByte
  3.14.toInt
</code></pre>

    <p>在Java中，primitive type不是对象，打破了一致性。</p>
  </li>
  <li>
    <p>函数是对象</p>

    <pre><code>  val compare = (x: Int, y: Int) =&gt; x &gt; y
  compare(1, 2)
</code></pre>
  </li>
  <li>
    <p>不再有静态方法(static method)和静态属性(static field)。Java中的静态方法(static method)和静态属性(static field)，有点打破了面向对象，因为它们不属于一个实例，而是属于类。在Scala中，静态方法和静态属性也属于对象，具体来说，属于Scala中的单例object。这样，静态成员和普通成员统一了起来，都附属于某个实例(instance)。</p>

    <pre><code>  object Dog {
    val sound = "wang wang" //static field
  }
</code></pre>
  </li>
</ul>

<h2 id="section-1">1.2 函数是值</h2>

<p>函数是一等公民，跟普通的值没区别</p>

<ul>
  <li>
    <p>可以当作参数传递</p>

    <pre><code>  val  compare = (x: Int , y: Int ) =&gt; x &gt;  y
  list sortWith compare
</code></pre>
  </li>
  <li>
    <p>不管它是实例的方法</p>

    <pre><code>  class AComparator  {
    def  compare(x: Int , y: Int ) = x &gt;  y
  }
  list sortWith ( new  AComparator ).compare
</code></pre>
  </li>
  <li>
    <p>还是匿名子句</p>

    <pre><code>  object  annonymous extends scala.Function2[Int , Int , Boolean] {
    override  def  apply(x: Int , y: Int ) = x &gt;  y
  }
  list sortWith annonymous 
</code></pre>
  </li>
</ul>

<h2 id="section-2">1.3 一切操作都是函数调用</h2>
</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/20130829">继续阅读 &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/20130617">安装Spark集群(在CentOS上)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-17T22:16:00+08:00" pubdate data-updated="true">Jun 17<span>th</span>, 2013</time>
        
		
         | <a href="/blog/20130617#duoshuo_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>环境</strong>:CentOS 6.4, Hadoop 1.1.2, JDK 1.7, Spark 0.7.2, Scala 2.9.3</p>

<p>折腾了几天，终于把Spark 集群安装成功了，其实比hadoop要简单很多，由于网上搜索到的博客大部分都还停留在需要依赖mesos的版本，走了不少弯路。</p>

<h1 id="jdk-17">1. 安装 JDK 1.7</h1>
<pre><code>yum search openjdk-devel
sudo yum install java-1.7.0-openjdk-devel.x86_64
/usr/sbin/alternatives --config java
/usr/sbin/alternatives --config javac
sudo vim /etc/profile
# add the following lines at the end
export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.19.x86_64
export JRE_HOME=$JAVA_HOME/jre
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
# test
$ java -version
</code></pre>

<p>参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120423/">安装和配置CentOS服务器的详细步骤</a>。</p>

<h1 id="scala-293">2. 安装 Scala 2.9.3</h1>
<p>Spark 0.7.2 依赖 Scala 2.9.3, 我们必须要安装Scala 2.9.3.</p>

<p>下载 <a href="http://www.scala-lang.org/downloads/distrib/files/scala-2.9.3.tgz">scala-2.9.3.tgz</a> 并 保存到home目录.</p>

<pre><code>$ tar -zxf scala-2.9.3.tgz
$ sudo mv scala-2.9.3 /usr/lib
$ sudo vim /etc/profile
# add the following lines at the end
export SCALA_HOME=/usr/lib/scala-2.9.3
export PATH=$PATH:$SCALA_HOME/bin
# save and exit vim
#make the bash profile take effect immediately
source /etc/profile
# test
$ scala -version
</code></pre>

<h1 id="spark">3. 下载预编译好的Spark</h1>
<p>下载预编译好的Spark, <a href="http://www.spark-project.org/download-spark-0.7.2-prebuilt-hadoop1">spark-0.7.2-prebuilt-hadoop1.tgz</a>. </p>

<p>如果你想从零开始编译，则下载源码包，但是我不建议你这么做，因为有一个Maven仓库，twitter4j.org, 被墙了，导致编译时需要翻墙，非常麻烦。如果你有DIY精神，并能顺利翻墙，则可以试试这种方式。</p>

<h1 id="section">4. 本地模式</h1>

<h2 id="section-1">4.1 解压</h2>

<pre><code>$ tar -zxf spark-0.7.2-prebuilt-hadoop1.tgz
</code></pre>

<h2 id="sparkexamplesjar-">4.2 设置SPARK_EXAMPLES_JAR 环境变量</h2>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_EXAMPLES_JAR=$HOME/spark-0.7.2/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.2.jar
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<p>这一步其实最关键，很不幸的是，官方文档和网上的博客，都没有提及这一点。我是偶然看到了这两篇帖子，<a href="https://groups.google.com/forum/?fromgroups#!topic/spark-users/nQ6wB2lcFN8">Running SparkPi</a>, <a href="https://groups.google.com/forum/#!msg/spark-users/x5UczgI-Xm8/wzMm3Mb77-oJ">Null pointer exception when running ./run spark.examples.SparkPi local</a>，才补上了这一步，之前死活都无法运行SparkPi。</p>

<h2 id="sparkhomesparkhomebinpath">4.3 （可选）设置 SPARK_HOME环境变量，并将SPARK_HOME/bin加入PATH</h2>
<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.7.2
export PATH=$PATH:$SPARK_HOME/bin
# save and exit vim
#make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="sparkpi">4.4 现在可以运行SparkPi了</h2>

<pre><code>$ cd ~/spark-0.7.2
$ ./run spark.examples.SparkPi local 
</code></pre>

<h1 id="section-2">5. 集群模式</h1>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/20130617">继续阅读 &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/20130614">Installing Spark on CentOS</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-14T19:06:00+08:00" pubdate data-updated="true">Jun 14<span>th</span>, 2013</time>
        
		
         | <a href="/blog/20130614#duoshuo_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>Environment</strong>:CentOS 6.4, Hadoop 1.1.2, JDK 1.7, Spark 0.7.2, Scala 2.9.3</p>

<p>After a few days hacking , I have found that installing a Spark cluster is exteremely easy :)</p>

<h1 id="install-jdk-17">1. Install JDK 1.7</h1>
<pre><code>yum search openjdk-devel
sudo yum install java-1.7.0-openjdk-devel.x86_64
/usr/sbin/alternatives --config java
/usr/sbin/alternatives --config javac
sudo vim /etc/profile
# add the following lines at the end
export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.19.x86_64
export JRE_HOME=$JAVA_HOME/jre
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
# test
$ java -version
</code></pre>

<h1 id="install-scala-293">2. Install Scala 2.9.3</h1>
<p>Spark 0.7.2 depends on Scala 2.9.3, So we must install Scala of version 2.9.3.</p>

<p>Download <a href="http://www.scala-lang.org/downloads/distrib/files/scala-2.9.3.tgz">scala-2.9.3.tgz</a> and save it to home directory.</p>

<pre><code>$ tar -zxf scala-2.9.3.tgz
$ sudo mv scala-2.9.3 /usr/lib
$ sudo vim /etc/profile
# add the following lines at the end
export SCALA_HOME=/usr/lib/scala-2.9.3
export PATH=$PATH:$SCALA_HOME/bin
# save and exit vim
# make the bash profile take effect immediately
source /etc/profile
# test
$ scala -version
</code></pre>

<h1 id="download-prebuilt-packages">3. Download prebuilt packages</h1>
<p>Download prebuilt packages, <a href="http://www.spark-project.org/download-spark-0.7.2-prebuilt-hadoop1">spark-0.7.2-prebuilt-hadoop1.tgz</a>. </p>

<p>If you want to compile it from scratch, download the source package, but I don’t recommend this way, because in Chinese Mainland the GFW has blocked one of maven repositories, twitter4j.org, which makes the compilation an impossible mission unless you can conquer GFW.</p>

<h1 id="local-mode">4. Local Mode</h1>

<h2 id="untar-the-tarball">4.1 Untar the tarball</h2>

<pre><code>$ tar -zxf spark-0.7.2-prebuilt-hadoop1.tgz
</code></pre>

<h2 id="set-the-sparkexamplesjar-environment-variable">4.2 Set the SPARK_EXAMPLES_JAR environment variable</h2>
<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_EXAMPLES_JAR=$HOME/spark-0.7.2/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.2.jar
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<p>This is the most important step that must be done , but unfortunately the official docs and most web blogs haven’t mentioned this. I found this step when I bumped into these posts, <a href="https://groups.google.com/forum/?fromgroups#!topic/spark-users/nQ6wB2lcFN8">Running SparkPi</a>, <a href="https://groups.google.com/forum/#!msg/spark-users/x5UczgI-Xm8/wzMm3Mb77-oJ">Null pointer exception when running ./run spark.examples.SparkPi local</a>.</p>

<h2 id="optionalset-sparkhome-and-add-sparkhomebin-to-path">4.3 (Optional)Set SPARK_HOME and add SPARK_HOME/bin to PATH</h2>

<pre><code>$ vim ~/.bash_profile
# add the following lines at the end
export SPARK_HOME=$HOME/spark-0.7.2
export PATH=$PATH:$SPARK_HOME/bin
# save and exit vim
# make the bash profile take effect immediately
$ source /etc/profile
</code></pre>

<h2 id="now-you-can-run-sparkpi">4.4 Now you can run SparkPi.</h2>

<pre><code>$ cd ~/spark-0.7.2
$ ./run spark.examples.SparkPi local 
</code></pre>

<h1 id="cluster-mode">5. Cluster Mode</h1>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/20130614">继续阅读 &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/20130612">在CentOS上安装Hadoop</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-12T12:39:00+08:00" pubdate data-updated="true">Jun 12<span>th</span>, 2013</time>
        
		
         | <a href="/blog/20130612#duoshuo_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Ubuntu上安装，请参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120103/">在Ubuntu上安装Hadoop</a>。</p>

<p><strong>环境</strong>：CentOS 6.4, JDK 1.7, Hadoop 1.1.2</p>

<h2 id="vmware-workstation-">1. 用vmware workstation 创建三台虚拟机</h2>
<p>首先用vmware workstation 新建一台CentOS 6.4，装好操作系统，选择 Basic Server，安装JDK，参考我的另一篇博客，<a href="http://www.yanjiuyanjiu.com/blog/20120423/">安装和配置CentOS服务器的详细步骤</a>。安装好后然后用浅拷贝<code>Create a linked clone</code> 克隆出两台作为slave，这样有了三台虚拟机。启动三台机器，假设IP分别为<code>192.168.1.131, 192.168.1.132, 192.168.1.133</code>, 131做为master 和 SecondaryNameNode, 身兼两职， 132和133为 slaves。</p>

<h2 id="section">2 关闭防火墙</h2>
<p>临时关闭防火墙</p>

<pre><code>$ sudo service iptables stop
</code></pre>

<p>下次开机后，防火墙还是会启动。</p>

<p>永久关闭防火墙</p>

<pre><code>$ sudo chkconfig iptables off
</code></pre>

<p>由于这几台虚拟机是开发机，不是生产环境，因此不必考虑安全性，可以永久关闭防火墙，还能给开发阶段带来很多便利。</p>

<h2 id="hostname">3. 修改hostname</h2>
<p>这一步看起来貌似不必要，其实是必须的，否则最后运行wordcount等例子时，会出现“Too many fetch-failures”。因为HDFS用hostname而不是IP，来相互之间进行通信（见后面的注意1）。</p>

<p>在CentOS上修改hostname，包含两个步骤(假设将hostname1改为hostname2，参考<a href="http://www.ichiayi.com/wiki/tech/linux_hostname">这里</a>，但不需要第一步)：</p>

<ol>
  <li>将 <code>/etc/sysconfig/network</code> 內的 HOSTNAME 改成 hostname2</li>
  <li>用<code>hostname</code>命令，临时修改机器名， <code>sudo hostname hostname2</code></li>
</ol>

<p>用<code>exit</code>命令退出shell，再次登录，命令提示字符串就会变成<code>[dev@hostname2 ~]$</code>。</p>

<p>用上述方法，将131改名为master，132改名为slave01，133改名为slave02。</p>

<p>在三台机器的/etc/hosts文件中，添加以下三行内容</p>

<pre><code>192.168.1.131 master
192.168.1.132 slave01
192.168.1.133 slave02
</code></pre>

<h2 id="section-1">4. 本地模式和伪分布式模式</h2>

<p>为了能顺利安装成功，我们先练习在单台机器上安装Hadoop。在单台机器上，可以配置成本地模式(local mode)和伪分布式模式(Pseudo-Distributed Mode)，参考官方文档<a href="http://hadoop.apache.org/docs/r1.1.2/single_node_setup.html">Single Node Setup</a>。</p>

<p>将 hadoop-1.1.2-bin.tar.gz 上传到三台机器的 home目录下，然后解压。<strong>注意，三台机器hadoop所在目录必须一致，因为master会登陆到slave上执行命令，master认为slave的hadoop路径与自己一样。</strong></p>

<h3 id="confhadoop-envsh-javahome">4.1 编辑 conf/hadoop-env.sh，设置 JAVA_HOME</h3>

<pre><code>cd hadoop-1.1.2
vim conf/hadoop-env.sh
</code></pre>

<p>注释掉第8行的JAVA_HOME，设置正确的JDK位置</p>

<pre><code>export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.19.x86_64
</code></pre>

<h3 id="section-2">4.2 测试本地模式是否正常</h3>
<p>默认情况下，Hadoop就被配置为本地模式，现在就可以开始测试一下。</p>

<pre><code>$ mkdir input 
$ cp conf/*.xml input 
$ bin/hadoop jar hadoop-examples-*.jar grep input output 'dfs[a-z.]+' 
$ cat output/*
</code></pre>

<p>可以看到正常的结果，说明本地模式运行成功了，下面开始配置伪分布式模式。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/20130612">继续阅读 &rarr;</a>
    </footer>
  


    </article>
  
  <ul class="pager">
    
    <li class="previous"><a href="/blog/page/2/">&larr; Older</a></li>
    
    <li><a href="/blog/archives">Blog Archives</a></li>
    
  </ul>
</div>
<aside class="sidebar-nav span3">
  
    <section>
  <h2>公告</h2>
  <p>独学而无友，则孤陋而寡闻，我每周在清华举办机器学习读书会，欢迎大家前来交流切磋。</p>
  <p>详情请见<a href="http://q.weibo.com/1644133">读书会微博群</a></p>
</section>
<section>
  <h2>分类目录</h2>
    <ul id="category-list"><li><a href='/blog/categories/algorithm/'>algorithm (1)</a></li><li><a href='/blog/categories/docker/'>docker (2)</a></li><li><a href='/blog/categories/hadoop/'>hadoop (2)</a></li><li><a href='/blog/categories/language/'>language (3)</a></li><li><a href='/blog/categories/machine-learning/'>machine-learning (5)</a></li><li><a href='/blog/categories/spark/'>spark (5)</a></li><li><a href='/blog/categories/tools/'>tools (10)</a></li></ul>
</section>
<section>
  <h2>友情链接</h2>
  <ul>
    <li>
      <a href="http://yewen.us/" title="大学同学，ACM高手，曾在百度，现在人人网">笨狗随手留下</a>
    </li>
	<li>
      <a href="http://blog.liancheng.info/" title="网易，百度，技术高手">连城</a>
    </li>
    <li>
      <a href="http://www.rational.so/" title="系统方向的清华博士">阎栋</a>
    </li>
	<li>
      <a href="http://www.parallellabs.com/" title="冠诚，IBM研究院研究员">并行实验室</a>
    </li>
	<li>
      <a href="http://shenfeng.me/" title="http-kit, clojure">沈峰</a>
    </li>
	<li>
      <a href="http://www.lihaipeng.info/" title="百度商业产品高级研发工程师">李海鹏</a>
    </li>
	<li>
      <a href="http://www.chioka.in/" title="Google工程师">Eric</a>
    </li>
    <li>
      <a href="http://blog.csdn.net/lgnlgn" title="好朋友，曾在赶集网，现在老家的猫扑">梁兄的技术博客</a>
    </li>
    <li>
      <a href="http://www.doesbetter.com/" title="机器学习，北邮">王孝舒的博客</a>
    </li>
    <li>
      <a href="http://www.foreverlee.net/" title="机器学习，中科院计算所">ForeverLee</a>
    </li>
  </ul>
</section>




<section>
  <h2>最新文章</h2>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/20131223">运行mahout的朴素贝叶斯分类器</a>
      </li>
    
      <li class="post">
        <a href="/blog/20131027">使用docker打造spark集群</a>
      </li>
    
      <li class="post">
        <a href="/blog/20131026">docker 快速入门</a>
      </li>
    
      <li class="post">
        <a href="/blog/20131025">docker安装</a>
      </li>
    
      <li class="post">
        <a href="/blog/20131024">CentOS 6.4 升级内核到 3.11.6</a>
      </li>
    
  </ul>
</section><section>
<h2>最新评论</h2>
<ul class="ds-recent-comments" data-num-items="5" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="0" data-excerpt-length="32"></ul>

<!--多说js加载开始，一个页面只需要加载一次 -->
<script type="text/javascript">
  var duoshuoQuery = {short_name:"yanjiuyanjiu"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = 'http://static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script>
<!--多说js加载结束，一个页面只需要加载一次 -->

</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo" class="page-footer"><!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
<p>
  Copyright &copy; 2014 - soulmachine -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  











</body>
</html>
